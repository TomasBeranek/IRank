\chapter{Introduction}
\textit{Static analysis} is a widely used technique for finding errors during \textit{software development}. \textit{Static analyzers} can also be deployed on code that is not yet finished, making it possible to detect errors in the early stages of the development, even before tests can be run. However, static analyzers often suffer from a high number of \textit{false positives} (i.e., \textit{false alarms}). If the percentage of false positives is too high, these tools are almost unusable in practice. Therefore, a lot of effort is devoted to the automatic detection of false positives.

This thesis focuses on the Meta Infer static analyzer. It is a highly \textit{scalable}, \textit{interprocedural}, \textit{open-source} tool for analyzing C/C++/C\#/Obj-C, and Java source files. Infer can detect \textit{null pointer dereferences}, \textit{dead stores}, \textit{uninitialized values}, \textit{deadlocks}, \textit{data races}, \textit{variable overflows}, and many other types of errors. Compared to other static analyzers, it is characterized by its ease of use -- its input consists of the compilation commands that compile the analyzed source files. Although Infer has been successfully used in practice by a number of companies (including Meta), it does have its disadvantages, and the main one is the high number of false positives. From experiments conducted in the author's bachelor's thesis, it was found that up to 90~\% of the reports are false positives. This number increases to more than 95~\% if errors of the dead store type, which are relatively common and harmless by themselves, are not considered.

The main contribution of the thesis is the design and implementation of a \textit{report ranking} system for the Meta Infer tool. The developed system can rank reports by the probability of being a \textit{true positive} (i.e., a \textit{real error}), thereby addressing the problem of a large number of false positives and making Infer a more practical tool because the current percentage of false positives is too high.

The report ranking system is based on \textit{graph neural networks} (GNNs), which have become increasingly popular for code-related tasks in recent years because many \textit{code properties} can be naturally expressed using graphs. A \textit{dataset} is necessary to train GNN models. This thesis utilizes the D2A dataset~\cite{D2A-zheng2021d2a}, which contains \textit{labeled} (true positive vs false positive) Infer reports from 6 open-source projects. D2A includes samples in a textual form, which must be converted into a graph form. For this reason, a training pipeline was created that generates Graph D2A -- D2A transformed into a graph form. The samples in Graph D2A cannot be directly used for training \textit{GNN models}; \textit{feature engineering} must first be applied to them. Feature engineering optimizes the graphs and transforms them into the format proposed in this thesis -- \textit{Extended Code Property Graphs} (ECPGs), which enrich existing \textit{Code Property Graphs} (CPGs) commonly used for \textit{vulnerability} detection in source code using GNNs.
In particular, we enrich them by \textit{Call Graphs}, \textit{data types}, and other information.

The developed GNN models were trained using ECPGs from the training sets of 3 D2A projects, namely httpd, libtiff, and nginx. The models were evaluated on the test sets of the same projects. The experimental results show that using the models we obtain comparable, and in some cases even superior results than the existing \textit{state-of-the-art} solutions, which are developed by strong industrial teams from IBM~\cite{D2A-zheng2021d2a, pujar2024analyzing}. These results demonstrate that the created models are a suitable open-source alternative to the compared existing solutions, all of which are -- to the best of our knowledge -- \textit{closed source}.

The models were also tested using \textit{cross-analysis} -- a model is tested on a different project than it was trained on. The models proved insufficient for this challenge, highlighting the difficulty of cross-analysis in this area of research, as none of the existing compared solutions function in cross-analysis either.

The last contribution of this thesis is the inference pipeline, which can run Infer analysis, generate an ECPG for each report, and finally sort the reports using the created GNN models, for any C (and subset C++) software. This pipeline is based on the author's bachelor's thesis, which dealt with automating Meta Infer analysis. This pipeline, originally designed for cross-analysis, can also be used for inference on projects with sufficient history, on which the GNN models were trained.

\paragraph{Structure of the thesis}
The rest of the thesis is structured as follows. Chapter~\ref{preliminaries} explains the basic concepts of static analysis, Meta Infer, graph neural networks, graph representations used, and finally describes the tools used -- LLVM Slicer, LLVM2CPG, and Joern. Chapter~\ref{chapter-d2a} describes the D2A dataset, its creation principle, comparison with other datasets, and the reasons for choosing D2A. Chapter~\ref{design} describes the design of the training pipeline, inference pipeline, and the proposed architecture of the GNN models. The implementation of the models and both pipelines is described in Chapter~\ref{implementation}. The results of experiments and comparison with existing models are in Chapter~\ref{experiments}. Finally, the conclusion is presented in Chapter~\ref{conclusion}. The thesis also includes Appendix~\ref{appendix-media} with the content of the attached media and of the additional resources available in the Zenodo trusted repository, Appendix~\ref{appendix-use-manual} with installation instructions and user manual, and Appendix~\ref{appendix-additional-data} with additional figures and tables.

\paragraph{Acknowledgement} This thesis is a collaboration with Red Hat. It is also supported by the H2020 ECSEL Valu3s, GACR AIDE 23-06506S, and IGA FIT-S-23-8151 projects.


\chapter{Preliminaries}
\label{preliminaries}
This chapter introduces the basic concepts, principles, and tools on which this thesis builds. Specifically, Section~\ref{static-analysis} briefly describes \textit{static analysis}, its applications, advantages, and limitations. Section~\ref{infer} describes the Meta Infer static analyzer, its use, types of \textit{detectable errors}, and its advantages and disadvantages. Section~\ref{gnn} describes the general principle of \textit{graph neural networks}, their advantages for \textit{source code analysis}, and especially their input format. Section~\ref{cpg} introduces the different \textit{source code representations} used as input to graph neural networks and focuses on the most commonly used type -- \textit{code property graphs}. Section~\ref{slicer} presents the LLVM-Slicer for \textit{slicing} LLVM bitcode. Section~\ref{llvm2cpg} describes the LLVM2CPG tool for constructing code property graphs from LLVM bitcode. Finally, Section~\ref{joern} presents the Joern platform used for various static analysis tasks.


\section{Static Analysis}
\label{static-analysis}

Static analysis~\cite{bc, static-analysis-EMANUELSSON20085, testovani-herout} can be understood as a way of \textit{reasoning} about the \textit{run-time properties} of computer programs without the need to run them (at least not under their original semantics) or provide their inputs. Using static analysis, it is possible to investigate program properties such as \textit{time} or \textit{memory complexity}, look for errors such as \textit{null pointer dereferences}, \textit{accesses beyond array boundaries}, improper handling of resources, etc. It is also possible to check for \textit{synchronization errors} such as \textit{deadlocks}, \textit{data races}, \textit{atomicity violations}, etc. Finally, static analysis can be used to ensure compliance with language standards, e.g., MISRA-C/MISRA-C++\footnote{\textbf{MISRA}'s website: \url{https://www.misra.org.uk/}.} or compliance with practices for writing readable code, e.g., Google Java Style\footnote{\textbf{Google Java Style Guide}: \url{https://google.github.io/styleguide/javaguide.html}.}.

The opposite of static analysis is \textit{dynamic analysis}, which requires running the program to be analyzed and thus a need to provide inputs. Since both approaches have their advantages and disadvantages, it is not advisable to use only one, but rather to use both simultaneously to complement each other. The advantages of static analysis are~\cite{bc, static-analysis-pros-cons}:
\begin{itemize}
    \item Static analysis implicitly considers all possible paths in the code (even the rarely executing ones),
    \item can report the exact location of the error and thus speed up the fix,
    \item does not require executable, sometimes even compilable source code, so errors can be detected early in the development,
    \item can be run fully automatically, after some initial setup.
\end{itemize}

However, static analysis also has its disadvantages~\cite{testovani-herout, static-analysis-pros-cons}:
\begin{itemize}
    \item The initial setup can be tedious for some tools as it may require, e.g., creating \textit{models} of certain functions, access to the compilation commands, or manually defining the required style guide.
    \item Running heavier-weight static analysis can be time and memory consuming.
    \item Static analyzers can report \textit{false positives} (i.e., false errors) or \textit{false negatives} (i.e., missed real errors).
\end{itemize}

The \textit{Rice's theorem} implies~\cite{static-analysis-spa} that all non-trivial properties of program behavior are \textit{undecidable}. From this, it follows that in order to derive such properties automatically, it is necessary to introduce some degree of \textit{approximations}. This approximation is the cause of false positives and false negatives. However, if a suitable approximation is used, it is possible to use static analysis to prove some properties (as opposed to dynamic analysis) -- typically the absence of errors. An example of this behavior is the use of Frama-C to create an RTE-free\footnote{\textbf{Run Time Error (RTE)}.} X.509 parser~\cite{RTEFreeParser}. However, most tools try to create approximations that balance the number of false positives and false negatives to make the tools practical to use.


\section{Meta Infer}
\label{infer}
Meta Infer~\cite{infer-web} (formerly Facebook Infer) is an open-source\footnote{\textbf{Meta Infer}'s repository: \url{https://github.com/facebook/infer/}.} \textit{framework} for writing \textit{intraprocedural} and \textit{interprocedural} static analyses~\cite{harmim-dip, marcin-bc, marek-bc}. Although it is a framework, Infer already includes a number of default and non-default (i.e., they must be explicitly enabled) analyses. Individual analyses are plugged into Infer in the form of \textit{plugins}. Different plugins use different principles to detect different types of errors, e.g. InferBO, which uses the \textit{symbolic interval} technique~\cite{InferBO} to detect incorrect array indexing, or the Bi-abduction plugin, which uses \textit{bi-abduction}~\cite{SeparationLogic} -- a form of \textit{inference} for \textit{separation logic} that models computer memory -- to detect errors associated with incorrect memory manipulation. Among other issues, Infer can detect null pointer dereferences, \textit{dead stores}, \textit{uninitialized values}, deadlocks, data races, \textit{variable overflows}, and many other types of errors. Table~\ref{table:infer-plugins} lists all the plugins that Infer provides, along with information about the language support and whether the plugin is enabled by default. More detailed information about each plugin and the types of errors reported by Infer can be found in~\cite{AllIssues}.

\begin{table}
\centering
\caption{Language support information for all non-experimental Infer plugins, along with whether the plugins are enabled by default.}
\vskip6pt
\label{table:infer-plugins}
\begin{tabular}{ !{\vrule width 1pt}l!{\vrule width 1pt}c|c|c|c|c!{\vrule width 1pt}c!{\vrule width 1pt} } 
 \noalign{\hrule height 1pt}
 \hspace{1.65cm}\textbf{Plugin} & \textbf{C} & \textbf{C++} & \textbf{Objective C} & \textbf{Java} & \textbf{C\#} & \textbf{Default} \\ 
 \noalign{\hrule height 1pt}
 Annotation Reachability & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark &  \\ 
 \hline
 Bi-abduction & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\ 
 \hline
 InferBO & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark &  \\ 
 \hline
 Cost & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark &  \\ 
 \hline
 Eradicate &  &  &  & \checkmark & \checkmark &  \\ 
 \hline
 Impurity & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark &  \\ 
 \hline
 Inefficient keySet Iterator &  &  &  & \checkmark & \checkmark & \checkmark \\ 
 \hline
 Litho "Required Props" &  &  &  & \checkmark & \checkmark &  \\ 
 \hline
 Liveness & \checkmark & \checkmark & \checkmark &  &  & \checkmark \\ 
 \hline
 Loop Hoisting & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark &  \\ 
 \hline
 Pulse & \checkmark & \checkmark & \checkmark & \checkmark &  &  \\ 
 \hline
 Purity & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark &  \\ 
 \hline
 Quandary & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark &  \\ 
 \hline
 RacerD &  & \checkmark &  & \checkmark & \checkmark & \checkmark \\ 
 \hline
 .NET Resource Leak &  &  &  &  & \checkmark & \checkmark \\ 
 \hline
 SIOF &  & \checkmark &  &  &  & \checkmark \\ 
 \hline
 Self in Block &  & \checkmark & \checkmark &  &  & \checkmark \\ 
 \hline
 Starvation & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\ 
 \hline
 Uninit & \checkmark & \checkmark & \checkmark &  &  & \checkmark \\ 
 \noalign{\hrule height 1pt}
\end{tabular}
\end{table}

Infer plugins are not \textit{sound}, which, in the context of finding errors, means that they may have false negatives. Instead, Infer aims for maximal practical use -- scaling to millions of lines of code thanks to \textit{modular analysis}. It is also very simple to use~\cite{infer-usage} compared to other analyzers. Infer takes as an input \textit{compilation commands} that allow the Infer's internal clang compiler to transform source files into the SIL\footnote{\textbf{Smallfoot Intermediate Language (SIL)}.} internal representation~\cite{SIL2, SIL}. This transformation (\textit{capture}) of the source code takes place in the \textit{capture phase}. To facilitate the capture of compilation commands, Infer supports a~variety of \textit{build systems} such as ant, cmake, Gradle, Make, Maven, and others. However, experiments conducted in previous work by the author~\cite{bc} show that this support is incomplete and often fails to capture compilation commands. Therefore, as part of the same work, a compiler wrapper was created that can reliably capture compilation commands and pass them to Infer.

The capture phase is followed by an \textit{analysis phase} in which the required plugins are run over the SIL. The output of Infer after the analysis phase is a list of found errors. Experiments on \textit{real-world} programs in previous work~\cite{bc} also show that Infer has a very high number of false positives. Specific numbers suggest approximately 4.5 false positives for every real error. However, this score is very optimistic since it includes dead store errors, which are harmless and can be detected by common compilers and are present in real-world programs in very large numbers, especially in the C language when using conditional compilation. Without dead stores, the number increases to approximately 9 false positives for every real error. In general, such a high number of false positives in static analyzers results in developers' distrust of these tools and consequent ignoring of analysis results~\cite{InferInFB, DisadvantagesOfStaticAnalysis, DisadvantagesOfStaticAnalysis2}. Therefore, efforts are made to reduce false positives.


\section{Graph Neural Networks}
\label{gnn}

There are a number of approaches for detecting errors in programs using \textit{machine learning}~\cite{taxonomy-hanif2021rise}. The approaches can be divided into \textit{convolutional neural networks} (CNNs)~\cite{CNN1-MIX-duan2019vulsniper}, \textit{recurrent neural networks} (RNNs)~\cite{RNN5-li2021vuldeelocator, RNN2-li2021sysevr, RNN3-li2018vuldeepecker, RNN6-lin2017poster, RNN7-saccente2019project, RNN4-zou2019mu}, and graph neural networks (GNNs)~\cite{GNN1-cao2021bgnn4vd, GNN3-cheng2021deepwukong, GNN6-ganz2021explaining, GNN5-vsikic2022graph, GNN4-IBM-suneja2020learning, GNN2-zhou2019devign}, depending on the \textit{architecture} of the model used. These approaches are often combined with each other~\cite{GNN8-RNN-fang2022jstrong, CNN1-RNN-li2020automated, GNN7-RNN-rabheru2020hybrid, RNN1-CNN-russell2018automated}.

CNNs achieve very good results, e.g., in \textit{image classification}. This is aided by \textit{convolutional layers} that can appropriately capture \textit{spatial information} from an image. However, this principle is not so effective for source code~\cite{gnn-usage}. In order to use the source code as input to a CNN, it must first be transformed into a graph and then into a matrix (e.g. an \textit{adjacency matrix}). Due to the fact that the nodes in a graph do not have a fixed order, the same graph can be expressed as an adjacency matrix (which has a fixed order of nodes) in multiple ways. This property is very undesirable because a single result is wanted for the same graph. It also makes it impossible to use the \textit{local spatial properties} of convolutional layers. Another problem for CNN, is the arbitrary size of the graph since the adjacency matrix have a fixed size.

Another frequently used approach is to represent code as a \textit{sequence}, especially for recurrent neural networks. This approach is based on the idea that the source code can be treated as a \textit{natural language}. While these approaches achieve very good results~\cite{NLP-APPROACH1-buratti2020exploring, NLP-APPROACH2-hanif2022vulberta, pujar2024analyzing}, the properties of source code can be better represented using graphs. Appropriately designed graphs can more explicitly model properties between parts of the code that would otherwise the model had to learn during training. The idea that a graph is a better representation of source code than aa adjacency matrix or a sequence is supported by the experiments in~\cite{GNN4-IBM-suneja2020learning}, especially on \textit{synthetic datasets} (on \textit{datasets} with real-world examples, all approaches seem to perform poorly). Arbitrary input sizes can also pose problems for RNNs, as they may have a limited input sequence length~\cite{pujar2024analyzing}.

GNNs are designed to work on arbitrarily large graphs. For this reason, and the previously mentioned reasons, GNNs were chosen for this thesis. Therefore, a brief description of a~general graph neural network based on \textit{message passing} follows. The description uses a~slightly modified notation from~\cite{GGNN-li2015gated}.

Consider an \textit{oriented graph} structure $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ where $\mathcal{V}$ is the \textit{set} of \textit{nodes} and $\mathcal{E}$ is the set of \textit{oriented edges} $\mathit{e} = ( \mathit{v}, \mathit{v}' ) \in \mathcal{V} \bigtimes \mathcal{V}$. The source node of an edge $\mathit{e} = ( \mathit{v}, \mathit{v}' )$ is $\mathit{v}$ and the target node is $\mathit{v}'$. The \textit{embedding vector} of a node $\mathit{v}$ is denoted by $\textbf{h}_\mathit{v} \in \mathbb{R}^\mathit{D}$ where $\mathit{D}$~is the \textit{dimension} of the vector. Each node has a \textit{label} which is denoted by $\mathit{l}_\mathit{v} \in \{1, ... , \mathit{L}_\mathcal{V}\}$, and each edge has a label which is denoted by $\mathit{l}_\mathit{e} \in \{1, ... , \mathit{L}_\mathcal{E}\}$. Further, auxiliary sets of nodes are defined. The set $\textsc{In}(\mathit{v}) = \{\mathit{v}' | (\mathit{v}', \mathit{v}) \in \mathcal{E}\}$ contains the \textit{predecessors} of a node $\mathit{v}$. The set $\textsc{Out}(\mathit{v}) = \{\mathit{v}' | (\mathit{v}, \mathit{v}') \in \mathcal{E}\}$ contains the \textit{descendants} of a node $\mathit{v}$. \textit{Bi-directional propagation} then proceeds by updating each node until \textit{convergence} (or for a fixed number of steps) using the following formula:

\begin{equation*}
	\textbf{h}_\mathit{v}^{(\mathit{t})} = \sum_{\mathit{v}' \in \textsc{In}(\mathit{v})} \mathit{f}(\mathit{l}_\mathit{v}, \mathit{l}_{(\mathit{v}', \mathit{v})}, \mathit{l}_{\mathit{v}'}, \textbf{h}_{\mathit{v}'}^{(\mathit{t}-1)}) + \sum_{\mathit{v}' \in \textsc{Out}(\mathit{v})} \mathit{f}(\mathit{l}_\mathit{v}, \mathit{l}_{(\mathit{v}, \mathit{v}')}, \mathit{l}_{\mathit{v}'}, \textbf{h}_{\mathit{v}'}^{(\mathit{t}-1)})
\end{equation*}

Here, the function $\mathit{f}$ can be a \textit{linear function} or a \textit{neural network}. For each node $\mathit{v}$, the output of this network is defined as $\mathit{o}_\mathit{v} = \mathit{g}(\textbf{h}_\mathit{v}^{(\mathit{T})}, \mathit{l}_\mathit{v})$ where $\mathit{g}$ is an arbitrary \textit{differentiable} function and $\mathit{T}$ is the final iteration. In case where \textit{graph-level classification}/\textit{regression} is needed, it is possible to artificially add a so-called "\textit{super node}" to the original graph, which will be connected to all nodes. This will allow graph-level classification/regression to be treated in the same way as \textit{node-level classification}/\textit{regression}.

The above description of how the information is propagated in GNNs shows that the graphs used as inputs, must form a single WCC\footnote{A \textbf{Weakly Connected Component (WCC)} is a set of nodes where there is a path between any two nodes, without respecting the direction of the edges.} in the case of bi-directional GNNs. And for \textit{directional GNNs} the edges must also be properly oriented (more information in Section~\ref{feature-engineering}). If the graph does not meet these properties, it is not possible to pass information between WCCs within \textit{GNN updates} (this needs not be a problem for some types of tasks, but it is crucial for the system designed in this thesis). If the function $\mathit{f}$ is \textit{differentiable}, then all components are differentiable, and after $\mathit{T}$ iterations, it is possible to compute gradients of the parameters (typically located within the function $\mathit{f}$) and train the GNN layers using \textit{gradient descent}.


\section{Source Code as a Graph}
\label{cpg}

There are many types of graphs that are commonly used as source code representations, e.g., \textit{abstract syntax trees} (AST), \textit{control flow graphs} (CFG), \textit{program dependency graphs} (PDG), and others. One type of such commonly use graphs is the \textit{code property graph} (CPG), which is composed of all three previously mentioned graphs and used in its pure form, e.g., in~\cite{USE-CPG-liu2020retrieval, GNN4-IBM-suneja2020learning}. Modified versions of it are often used as well, e.g., \textit{simplified CPGs} (SCPG) for \textit{function-level vulnerability detection}\footnote{In some articles, the terms ''error'' and ''vulnerability'' are used interchangeably, but not every error is a vulnerability.} in C/C++~\cite{SCPG-wu2021vulnerability}, CPGs with added edges that reflect the original order of \textit{tokens} (i.e., individual source code elements)~\cite{GNN2-zhou2019devign}, or \textit{code composite graphs} (CCG) again for vulnerability detection in C/C++~\cite{GNN1-cao2021bgnn4vd}. Furthermore, PDGs alone are used, e.g., for finding \textit{malicious code} in JavaScript~\cite{GNN8-RNN-fang2022jstrong}, XFGs (\textit{subgraphs} of PDGs) for detecting vulnerabilities in C/C++ code~\cite{GNN3-cheng2021deepwukong}. Or CFGs together with token sequences for detecting vulnerabilities in PHP~\cite{GNN7-RNN-rabheru2020hybrid}.

According to~\cite{CPG-yamaguchi2014modeling}, the reason for the creation of CPGs is the inability of each subgraph type to detect certain types of errors independently during \textit{traversal}. For example, ASTs are not suitable for detecting \textit{divisions by zero}. However, by combining ASTs and PDGs, this is possible, but one still cannot detect, e.g., integer overflows. This can only be detected by combining ASTs, CFGs and PDGs. This combination of graphs results in a representation that is able to capture both syntactic and semantic properties of the code and preserve most types of errors in it. Exceptions are, e.g., \textit{race conditions}, which need more external information. A~complete table of detectable errors and required graph types is given in~\cite{CPG-yamaguchi2014modeling}. The following graph definitions are based on the original definitions from the paper introducing CPGs~\cite{CPG-yamaguchi2014modeling}, with only minor changes in notation to resemble the GNN definition given earlier. It should be noted that the following definitions employ an abuse of notation, as it was used in the original paper.

To define a CPG, it is first necessary to define a \textit{property graph}~\cite{CPG-yamaguchi2014modeling}, which is a~commonly used graph type in \textit{graph databases} such as Neo4j. A property graph is an oriented \textit{multigraph}  $\mathcal{G} = (\mathcal{V}, \mathcal{E}, \lambda, \mu)$, where $\mathcal{V}$ denotes the set of nodes, $\mathcal{E}$ denotes the set of edges $\mathit{e} = (\mathit{v}, \mathit{v}')$, where $\mathit{v}, \mathit{v}' \in \mathcal{V}$. $\lambda$ denotes the \textit{edge labeling function} $\lambda : \mathcal{E} \rightarrow \Sigma$, with $\Sigma = {1, ..., L_\mathcal{E}}$ being the edge labels. Finally, $\mu$ denotes the function $\mu: (\mathcal{V} \cup \mathcal{E}) \bigtimes K \rightarrow S$ that assigns attributes to nodes and edges where $K$ is the set of \textit{attribute} names and $S$ is the set of attribute values.

An AST~\cite{CPG-yamaguchi2014modeling} is an \textit{ordered tree} whose inner nodes represent \textit{operators} and outer nodes (\textit{leaves}) represent \textit{operands}. The oriented edges then show the parenting relation. The AST captures the syntactic nature of the code. Consider the code in Listing~\ref{listing:cpg}. The AST constructed for this code is shown in Figure~\ref{figure:AST}. 

To create a CPG definition, the subgraph types must be converted to the same format -- in particular, to the previously defined property graphs. An AST as a property graph is a structure $\mathcal{G}_{\textsc{Ast}} = (\mathcal{V}_{\textsc{Ast}}, \mathcal{E}_{\textsc{Ast}}, \lambda_{\textsc{Ast}}, \mu_{\textsc{Ast}})$, where $\mathcal{V}_{\textsc{Ast}}$ is the set of AST nodes and $\mathcal{E}_{\textsc{Ast}}$ is the set of AST edges. The function $\lambda_{\textsc{Ast}}$ is defined as $\lambda_{\textsc{Ast}}(\mathit{v}) = \textrm'\textsc{Ast}\textrm'$ and is applied to each node $\mathit{v} \in \mathcal{V}_{\textsc{Ast}}$. The function~$\mu_{\textsc{Ast}}:~\mathcal{V_{\textsc{Ast}}}~\bigtimes~K_{\textsc{Ast}}~\rightarrow~S_{\textsc{Ast}}$ is applied to each node and attribute. The attribute names are $K_{\textsc{Ast}}~=~\{\textrm'code\textrm',~\textrm'order\textrm'\}$ and the attribute values are $S_{\textsc{Ast}}~=~S_{code}~\cup~S_{order}$, where $S_{code}$ are types of nodes in an AST, e.g., \textit{variable}, \textit{constant}, mathematical operators, etc., and $S_{order}$ assigns values that order a node among its siblings in the AST to preserve the ordering from the original tree.

\begin{lstlisting}[
    style=c++, label={listing:cpg}, float=t,
    caption={A code sample. The code was taken from~\cite{CPG-yamaguchi2014modeling}.}
]
void foo()
{
    int x = source();
    if (x < MAX)
    {
        int y = 2 * x;
        sink(y);
    }
}
\end{lstlisting}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.75\textwidth]{figures/AST.pdf}
	\caption{The abstract syntax tree for the code in Listing~\ref{listing:cpg}. This figure was taken from~\cite{CPG-yamaguchi2014modeling}.}
	\label{figure:AST}
\end{figure}

A~CFG~\cite{CPG-yamaguchi2014modeling} is an oriented graph describing the possible paths of \textit{program control} and the conditions for their \textit{execution}. The nodes of the graph represent \textit{statements} and \textit{predicates}, while the edges represent \textit{control passing}. Each command node has an outgoing edge labeled~$\varepsilon$, which denotes an \textit{unconditional} passing of control. While a predicate node must have two outgoing edges $true$ and $false$ for different evaluations of a given predicate. Consider the code in Listing~\ref{listing:cpg}. The~CFG constructed for this code is shown in Figure~\ref{figure:CFG-PDG}. The CFG as a property graph is the structure $\mathcal{G}_{\textsc{Cfg}} = ( \mathcal{V}_{\textsc{Cfg}}, \mathcal{E}_{\textsc{Cfg}}, \lambda_{\textsc{Cfg}}, \cdot )$ where $\mathcal{V}_{\textsc{Cfg}}$ is the set of nodes corresponding to the nodes from the AST as follows:

\begin{equation*}
\mathcal{V}_{\textsc{Cfg}} = \{\mathit{v} \in \mathcal{V}_{\textsc{Ast}} \: | \: \mu_{\textsc{Ast}}(v, \textrm'code\textrm') \in \{\textrm'\textsc{Stmt}\textrm', \textrm'\textsc{Pred}\textrm'\} \}
\end{equation*}

The edge labeling function is defined as $\lambda_{\textsc{Cfg}} : \mathcal{E}_{\textsc{Cfg}} \rightarrow \Sigma_{\textsc{Cfg}}$ where the values in the set $\Sigma_{\textsc{Cfg}}~=~\{\textrm'true\textrm',~\textrm'false\textrm',~\textrm'\varepsilon\textrm'\}$ correspond to the meaning of edges in the CFG.

A~PDG~\cite{CPG-yamaguchi2014modeling} is again an oriented graph whose nodes are statements and predicates. There are two types of edges in a PDG, namely \textit{data dependency edges}, which model the influence of a variable on the value of another variable, and \textit{control dependency edges}, which model the influence of predicates on the values of variables. Consider the code in Listing~\ref{listing:cpg}. A~PDG constructed for this code is shown in Figure~\ref{figure:CFG-PDG}. The PDG as a property graph is a structure $\mathcal{G}_{\textsc{Pdg}} = ( \mathcal{V}_{\textsc{Cfg}}, \mathcal{E}_{\textsc{Pdg}}, \lambda_{\textsc{Pdg}}, \mu_{\textsc{Pdg}} )$ where the nodes are the same as in the CFG. The edge labeling function is defined as $\lambda_{\textsc{Pdg}} : \mathcal{E}_{\textsc{Pdg}} \rightarrow \Sigma_{\textsc{Pdg}}$, where the edge labels $\Sigma_{\textsc{Pdg}} = \{\textrm'data\textrm', \textrm'control\textrm'\}$ correspond to the meaning of edges in the PDG. The function assigning attribute values has the form of $\mu_{\textsc{Pdg}}: \mathcal{E_{\textsc{Pdg}}} \bigtimes K_{\textsc{Pdg}} \rightarrow S_{\textsc{Pdg}}$, where $K_{\textsc{Pdg}}~=~\{\textrm'symbol\textrm',~\textrm'condition\textrm'\}$ and $S_{\textsc{Pdg}} = S_{\textsc{Var}} \cup \{\textrm'true\textrm', \textrm'false\textrm'\} $. The set $S_{\textsc{Var}}$ represents the set of names of all variables that occur as the output node of the data dependency edges. The function $\mu_{\textsc{Pdg}}$ then works by assigning the value of the attribute $\textrm'symbol\textrm'$ to the $\textrm'data\textrm'$ edges as the name of the variable represented by the source node of the edge, and $\textrm'control\textrm'$ edges are assigned the attribute value $\textrm'condition\textrm'$ depending on whether they are in the $true$ or $false$ branch. 

\begin{figure*}[t]
  \centering
  \centering
  \includegraphics[width=0.36\linewidth]{figures/CFG.pdf}\hfill
  \includegraphics[width=0.48\linewidth]{figures/PDG.pdf}
  \caption{The control flow graph (on the left) and the program dependence graph (on the right) for the code in Listing~\ref{listing:cpg}. These figures were taken from~\cite{CPG-yamaguchi2014modeling}.}
  \label{figure:CFG-PDG}
\end{figure*}

The CPG is then defined using the previous definitions of AST, CFG, and PDG as: 

\begin{equation*}
    \mathcal{G} = ( \mathcal{V}_{\textsc{Ast}}, \mathcal{E}_{\textsc{Ast}} \cup \mathcal{E}_{\textsc{Cfg}} \cup \mathcal{E}_{\textsc{Pdg}}, \lambda, \mu )
\end{equation*}
where the definition of the function $\lambda$ is as follows:

  \begin{equation*}
      \lambda(e) = 
      \begin{cases} 
      \lambda_{\textsc{Ast}} (e) \;\; \textit{if} \;\; e \in \mathcal{E}_{\textsc{Ast}} \\
      \lambda_{\textsc{Cfg}} (e) \;\; \textit{if} \;\; e \in \mathcal{E}_{\textsc{Cfg}} \\
      \lambda_{\textsc{Pdg}} (e) \;\; \textit{if} \;\; e \in \mathcal{E}_{\textsc{Pdg}} 
   \end{cases}
  \end{equation*}

and the definition of the $\mu$ function is:
  
  \begin{equation*}
      \mu(x, p) = 
      \begin{cases} 
      \mu_{\textsc{Ast}} (x, p) \;\; \textit{if} \;\; (x, p) \in \mathcal{V}_{\textsc{Ast}} \bigtimes K_{\textsc{Ast}} \\
      \mu_{\textsc{Pdg}} (x, p) \;\; \textit{if} \;\; (x, p) \in \mathcal{E}_{\textsc{Pdg}} \bigtimes K_{\textsc{Pdg}} \\
   \end{cases}
  \end{equation*}

\iffalse
\begin{multicols}{2}
  \begin{equation*}
      \lambda(e) = 
      \begin{cases} 
      \lambda_{\textsc{Ast}} (e) \;\; \textit{if} \;\; e \in \mathcal{E}_{\textsc{Ast}} \\
      \lambda_{\textsc{Cfg}} (e) \;\; \textit{if} \;\; e \in \mathcal{E}_{\textsc{Cfg}} \\
      \lambda_{\textsc{Pdg}} (e) \;\; \textit{if} \;\; e \in \mathcal{E}_{\textsc{Pdg}} 
   \end{cases}
  \end{equation*}\break
  \begin{equation*}
      \mu(x, p) = 
      \begin{cases} 
      \mu_{\textsc{Ast}} (x, p) \;\; \textit{if} \;\; (x, p) \in \mathcal{V}_{\textsc{Ast}} \bigtimes K_{\textsc{Ast}} \\
      \mu_{\textsc{Pdg}} (x, p) \;\; \textit{if} \;\; (x, p) \in \mathcal{E}_{\textsc{Pdg}} \bigtimes K_{\textsc{Pdg}} \\
   \end{cases}
  \end{equation*}
\end{multicols}
\fi

A~CPG for the code in Listing~\ref{listing:cpg} is shown in Figure~\ref{figure:CPG} where the irrelevant \texttt{FUNC}, \texttt{IF}, and \texttt{STMT} nodes were omitted for demonstration purposes. And also an entry point and an exit point were added.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.98\textwidth]{figures/CPG.pdf}
	\caption{The code property graph for the code in Listing~\ref{listing:cpg}. This figure was taken from~\cite{CPG-yamaguchi2014modeling}.}
	\label{figure:CPG}
\end{figure}

\section{LLVM-Slicer}
\label{slicer}
As described in more detail in Section~\ref{graph-construction-phase}, the LLVM-slicer is used for program slicing in this thesis. LLVM-slicer is an open-source\footnote{\textbf{LLVM-Slicer}' repository: \url{https://github.com/mchalupa/dg}.} tool which uses the DG library~\cite{DG-2-chalupa2020dg, DG-chalupa2020dg}. The DG library implements various interprocedural static analyses -- namely, \textit{pointer analysis}, \textit{data dependence analysis}, \textit{control dependence analysis}, and \textit{value relationship analysis}. These analyses are implemented in DG as independent of the input language. However, the front-end currently supports LLVM bitcode only~\cite{LLVM-bitcode}. LLVM bitcode is a \textit{storage format} for LLVM IR\footnote{\textbf{LLVM Intermediate Representation (LLVM IR)}.}~\cite{LLVM-IR}, which is an \textit{assembly language} used as a low-level representation of code during the various stages of LLVM compilation.

The main use of the DG library is the aforementioned LLVM-slicer, which uses the DG analyses for \textit{program slicing} -- removing pieces of code that have no effect on \textit{user-defined} areas in the code. Results of experiments with LLVM-Slicer on benchmarks from the Software Verification Competition can be found in~\cite{DG-chalupa2020dg}. Although LLVM bitcode is language-independent and can be generated from, e.g., C, C++, or Rust, LLVM-slicer does not support certain constructs in LLVM bitcode that handle \textit{exceptions}. This means that it is not able to handle a C++ program that uses exceptions. If the C++ code is exception-free, it should be able to slice it. The input to the LLVM-slicer is a \textbf{single LLVM bitcode} file and slicing criteria. The output is the sliced LLVM bitcode.

Slicing criteria are specified, for example, using the option \texttt{-sc}. This option allows for a~relatively extensive specification of slicing criteria~\cite{llvm-slicer-readme}. However, in this thesis, the basic format \texttt{-sc file\#function\#line\#obj} is used only. The fields \texttt{file}, \texttt{function}, \texttt{line}, or \texttt{obj} can be empty. The meanings of \texttt{file}, \texttt{function}, and \texttt{line} are straightforward -- they refer to locations in the code. The \texttt{obj} field maps to a function call or a variable use at the location (the code must be compiled with debugging information, see Section~\ref{bitcode-generation}).

Furthermore, it is necessary to define an \textit{entry point function} that must be present in the input bitcode. The default entry point is the \texttt{main} function. However, it can be overridden using the option \texttt{--entry=function}. The entry function acts as the starting point for the analysis -- anything above this function in the call tree is removed.


\section{LLVM2CPG}
\label{llvm2cpg}
In this thesis, the open-source\footnote{\textbf{LLVM2CPG}'s repository: \url{https://github.com/ShiftLeftSecurity/llvm2cpg}.} LLVM2CPG tool is used for generating CPGs from LLVM bitcode, as detailed in Section~\ref{graph-construction-phase}. The CPGs were originally created for high-level languages such as C, which creates some problems when creating CPGs from low-level LLVM IR~\cite{llvm2cpg-webpage}. One problem is mapping LLVM IR instructions to classical high-level operations in order to display the CPG in the same format as, e.g., for the C source code. Some operations can be mapped directly because they have the same \textit{semantics}, others can be modeled using functions, and some cannot be mapped at all and need to be bypassed by another mechanism. The CPG output format can be further processed by Ocular\footnote{\textbf{Ocular}'s documentation: \url{https://docs.shiftleft.io/ocular/quickstart}.} (proprietary), Plume\footnote{\textbf{Plume}'s documentation: \url{https://plume-oss.github.io/plume-docs/}.} (open-source) or Joern (open-source, see Section~\ref{joern}).


\section{Joern}
\label{joern}
Joern is used in this thesis to enrich CPGs with additional information, as detailed in Section~\ref{graph-construction-phase}. Joern~\cite{joern-website} is a powerful open-source\footnote{\textbf{Joern}'s repository: \url{https://github.com/joernio/joern}.} platform providing various tools from the area of static analysis. Using Joern, it is possible to write custom static analyses or \textit{queries} over source files. Joern supports various \textit{programming languages}, such as C, C++, JavaScript, Kotlin, Python, or Java. It is also possible to construct different graph representations of the code (ASTs, CFGs, CDGs, DDGs, PDGs or CPGs), which can be exported in different formats, e.g., DOT~\cite{DOT} or csv for the Neo4j graph database~\cite{neo4j-webpage}. It is also possible to load already constructed CPGs in different formats, e.g., in the output format of the LLVM2CPG tool. Joern can be used as a \textit{command line} tool, through an \textit{interactive environment}, or as an \textit{integration library}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{D2A Dataset}
\label{chapter-d2a}
This chapter introduces the D2A dataset, which is used in this work to train a system that reduces false positives of the Meta Infer static analyzer. Specifically, the chapter discusses the creation of D2A, the structure of individual samples, comparisons with other existing datasets, and presents statistics regarding the distribution of Meta Infer's error types. This chapter draws primarily from~\cite{D2A-zheng2021d2a, D2A-webpage}.

D2A is a dataset developed by IBM, containing errors found by the Meta Infer static analyzer and information about their validity (true positive/false positive). D2A was first introduced in~\cite{D2A-zheng2021d2a} and is freely available for download at~\cite{D2A-webpage}. The dataset is generated automatically based on \textit{differential static analysis}, and the source files for the D2A generation pipeline are open-source\footnote{\textbf{D2A pipeline}'s repository: \url{https://github.com/IBM/D2A}.}. The dataset fits into the area of static analysis and is primarily intended for creating models aimed at eliminating false reports produced by static analyzers. Initial results from models such as Catboost, LightGBM, Random Forest, Extra-Trees, or the voting model can already be found in the article introducing D2A~\cite{D2A-zheng2021d2a}. The team behind D2A also later published the work~\cite{pujar2024analyzing}, where they improve the existing models and add the C-BERT model, which is \textit{Bidirectional Encoder Representations from Transformers}~\cite{devlin2018bert}, but trained on C code and \textit{fine-tuned} on the D2A dataset for the purpose of classifying reports.

Several reasons led to the selection of the D2A dataset for this thesis:
\begin{enumerate}
    \item It is created from real-world open-source projects.
    \item Meta Infer was used for differential static analysis (thus, samples contain trace, location, error type, etc., which is necessary for extracting slicing information, more information in Section~\ref{graph-construction-phase}).
    \item Being an automatically generated dataset, it is sufficiently large.
    \item The author of this thesis has previously collaborated with the creators of the D2A dataset.
\end{enumerate}

\begin{figure}[t]
	\centering
	\includegraphics[width=1\textwidth]{figures/d2a-pipeline.png}
	\caption{A~schematic of the D2A generation pipeline. This figure was taken from~\cite{D2A-zheng2021d2a}.}
	\label{figure:d2a-pipeline}
\end{figure}

\subsubsection{Dataset Creation Principle}
The D2A dataset was automatically generated using differential static analysis on open-source projects with extensive git histories. The schematic of the pipeline for generating D2A is shown in Figure~\ref{figure:d2a-pipeline}. The fundamental concept of this pipeline is that the git history includes commits that fix real errors. Therefore, the entire pipeline starts with the identification of these potential fixing commits. These commits are identified using the Commit Message Analyzer, which, through similarity-based methods and key phrase search in commit messages, can select commits that are highly likely fixing errors. For each such commit, Meta Infer (see Section~\ref{infer}) is run on the version of the code before and after the commit. Errors that are found in the before version and are missing in the after version are considered true positives, indicating they have been fixed. For an error to be counted as a true positive, it must also satisfy the following conditions:
\begin{enumerate}
    \item The error must not appear in later versions.
    \item The commit must have modified some part of the bug trace\footnote{A \textbf{bug trace} is information attached to some outputs of Meta Infer. It includes sections of the code that influenced the particular error.} related to the error.
\end{enumerate}

All other errors are considered false positives -- this is, of course, an approximation because otherwise it would imply that the project in its latest version contains no errors, which is highly unlikely. The D2A dataset also includes another type of sample called \textit{after-fix} samples, which are labeled as false positives. Each after-fix sample is generated as a~counterpart to a true positive sample on the after version, where the corresponding true positive have been fixed -- the after-fix samples contain the fixed code. After-fix samples have the property of creating a~balanced dataset along with the true positives and also form pairs that can help models learn to differentiate between true positives and false positives. This is because the pairs provide the models access to the same code with and without the error. However, these samples naturally do not have Meta Infer outputs and are not used in this thesis.

As previously mentioned, each sample includes the output from Meta Infer, the code of the functions related to the error, and additional metadata such as the ID, label, commit hash, and compiler arguments for all files affected by the error (this is possible because Meta Infer needs to compile the code as discussed in Section~\ref{infer}). The complete list of sample attributes is too extensive to be included here, but it is documented in~\cite{d2a-sample-description}. Attributes and their formats necessary for the further explanation will be described in later chapters.

\subsubsection{Comparison with Other Existing Datasets}
There are numerous datasets designed for training models that identify errors in C/C++ code as can be seen in the table comparing existing datasets with D2A in~\cite{D2A-zheng2021d2a}. These datasets are typically categorized into synthetic and real-world types. Synthetic datasets offer the advantage of 100~\% label accuracy and the ability to automatically generate samples, making them sufficiently large. However, synthetic samples are typically simpler and differ from real code, which may lead to poor \textit{generalization} when applied to real-world software. Real-world datasets can be further divided into manually and automatically created. Manually created datasets are highly accurate but are typically too small. Automatically created datasets, on the other hand, suffer from lower accuracy but are large enough. The D2A dataset employs a hybrid approach, automatically generating samples from real-world projects while striving to identify bug-fixing commits that were manually corrected. As a~result, the dataset achieves an accuracy where the true positive class has accuracy of 41~\% and false positive class has accuracy of 81~\%. These accuracies were determined through manual validation of 41 samples labeled as true positive and 16 samples labeled as false positive.

\subsubsection{Dataset Distribution}
The D2A dataset includes 6 open-source projects--openssl\footnote{\textbf{openssl}'s repository: \url{https://github.com/openssl/openssl}.}, libav\footnote{\textbf{libav}'s repository: \url{https://github.com/libav/libav}.}, nginx\footnote{\textbf{nginx}'s repository: \url{https://github.com/nginx/nginx}.}, libtiff\footnote{\textbf{libtiff}'s repository: \url{https://gitlab.com/libtiff/libtiff}.}, httpd\footnote{\textbf{httpd}'s repository: \url{https://github.com/apache/httpd}.}, and FFmpeg\footnote{\textbf{FFmpeg}'s repository: \url{https://github.com/FFmpeg/FFmpeg}.}. While it is theoretically possible to expand it to include any software with a sufficient history of commits, generating it is computationally demanding as it requires running Meta Infer twice for each targeted commit on the entire project. The D2A dataset contains a total of 1,314,276 samples and is provided with a split into \textit{training}, \textit{validation}, and \textit{testing datasets} to match the results of the models from~\cite{D2A-zheng2021d2a, pujar2024analyzing}. Each sample is labeled either true positive (1) or false positive (0) and categorized by error type as determined by Meta Infer outputs, such as \texttt{NULL\_DEREFERENCE}, \texttt{UNINITIALIZED\_VALUE}, etc. Tables~\ref{tab:d2a-bug-types1} and~\ref{tab:d2a-bug-types2} show the counts of samples according to the label, project, and error type. The tables also highlight the types of errors supported by the system for reducing false positives in this thesis, with more details available in Section~\ref{bitcode-generation}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Design of a System for Reducing False Positives in Meta Infer}
\label{design}
This chapter describes the design of training and inference pipelines for transforming the D2A dataset into its graphical form, referred to as Graph D2A. Specifically, Section~\ref{training-pipeline} describes the training pipeline, which transforms the D2A dataset into Graph D2A. Section~\ref{bitcode-generation} focuses on the bitcode generation phase, aiming to produce LLVM bitcode from the D2A dataset samples. Section~\ref{graph-construction-phase} explains the creation of extended code property graphs from the generated LLVM bitcode. Section~\ref{graph-d2a} provides a detailed description of the Graph D2A format. Section~\ref{feature-engineering} discusses the feature engineering process, which converts graphs from Graph D2A into an optimized input format for graph neural networks. Section~\ref{gnn-model} outlines how to train graph neural networks using these optimized graphs.

Section~\ref{inference-pipeline} addresses the design of the inference pipeline, a modification of the training pipeline designed to automatically extract graphs and apply the graph neural network models to any real-world C (and a subset of C++) software. Specifically, Section~\ref{capture-phase} describes the capture phase, aimed at running Meta Infer analysis and extracting LLVM bitcode from the build of real-world software. Finally, Section~\ref{inference-phase} discusses the inference phase, which deploys the trained models on the created graphs and ranks a list of errors detected by Infer based on the likelihood of being true positives.

We recall that he goal of this thesis is to create a system to reduce false positives from the static analyzer Meta Infer, described in Section~\ref{infer}. Due to reasons mentioned in Section~\ref{gnn}, graph neural networks (GNNs) were chosen for this task. The goal of the trained models is to rank the errors found by Infer based on their likelihood of being true positives. The D2A dataset was selected for reasons detailed in Chapter~\ref{chapter-d2a}. Although D2A includes the source code of functions mentioned in Infer's bug traces, this information is stored as text (more specifically as JSON) and not as graphs. To enable the training of GNNs on D2A, it first needs to be transformed into an appropriate graph format. According to Section~\ref{cpg}, a suitable and frequently used representation are the code property graphs (CPGs) and its modified versions. The application of GNNs to source code requires a~preliminary mechanism for graph construction. However, the existing graph construction methods have several limitations, which led to the development of our training and inference pipelines. The three main disadvantages of the current solutions are:

\begin{enumerate}
    \item Insufficient graph representations, such as constructing only ASTs~\cite{GNN5-vsikic2022graph}, XFGs~\cite{GNN3-cheng2021deepwukong}, or CFGs~\cite{GNN7-RNN-rabheru2020hybrid}\footnote{This work, however, employs a hybrid approach using both GNN and RNN.}.
    \item Not considering conditional compilation~\cite{GNN1-cao2021bgnn4vd, JOERN-CPG-guan2020code, GNN4-IBM-suneja2020learning, JOERN-CPG-xiaomeng2018cpgva, GNN2-zhou2019devign}.
    \item The inability to automatically construct graphs for any software~\cite{GNN1-cao2021bgnn4vd, JOERN-CPG-guan2020code, GNN4-IBM-suneja2020learning, JOERN-CPG-xiaomeng2018cpgva, GNN2-zhou2019devign}.
\end{enumerate}

Points 2) and 3) are closely linked. The previous works, namely,~\cite{GNN1-cao2021bgnn4vd, JOERN-CPG-guan2020code, GNN4-IBM-suneja2020learning, JOERN-CPG-xiaomeng2018cpgva, GNN2-zhou2019devign}, all use the Joern tool (see Section~\ref{joern}) to construct CPGs (and its various modifications), that is why they are mentioned in both 2) and 3). Although Joern is a very useful tool, its disadvantage is that it analyses the source files directly and is not able to connect to the build process itself. This makes it unable to identify which source files to process and which not to. While Joern can \textit{recursively} find and process source files in a given directory~\cite{joern-doc}, it does indeed process everything it finds in those directories. This becomes a problem if the software includes different versions of the source code, e.g. for different \textit{operating systems} (Windows or Linux), which are selected only during compilation. Joern will thus not be able to correctly construct a CPG without knowing which file to use in a given context. Therefore, Joern cannot be fully automatically deployed on arbitrary software. 

There is a similar problem with conditional compilation where Joern does not know which part of the code to use, or what values the macros have, since they can be (and very often are) defined during compilation. For this reason, Joern considers all macros as undefined by default, and therefore irretrievably loses code fragments that did not satisfy the conditions within \texttt{\#ifdef} or \texttt{\#ifndef} during preprocessing. These problems do not seem to manifest themselves in artificial datasets, and for concrete real-world software, these problems must be solved manually if using pure Joern.

Points 2) and 3) are also closely related to Infer -- since its inputs are compilation commands, and the source code is compiled using them before the analysis (see Section~\ref{infer}) -- Infer analyzes the preprocessed code. This means that the Infer's analysis is platform-dependent -- it can find different errors under various compilation conditions. Therefore, it makes sense to construct graphs from the code as seen by Infer.

The use case of the proposed pipelines differs subtly from previous studies. In particular, we need to construct graphs based on the code in alignment with the Infer report that needs to be sorted. This requires the capability to slice the code according to the information extracted from the report. Program slicing is also employed in some earlier studies. However, in this regard, the most comparable studies, specifically~\cite{D2A-zheng2021d2a, pujar2024analyzing}, do not use program slicing.

The proposed pipelines are intended to create extended CPGs (further discussed in Sections~\ref{graph-d2a} and~\ref{feature-engineering}) from software written in C and a subset of C++. The limitation for C++ arises from the use of LLVM-Slicer, with specific reasons elaborated in Section~\ref{slicer}.


\section{Training Pipeline}
\label{training-pipeline}

\begin{figure}[t]
	\centering
	\includegraphics[width=1\textwidth]{figures/training-pipeline.pdf}
	\caption{The figure shows a schematic of the training pipeline that transforms the D2A dataset into Graph D2A, and then trains models on it. Dashed boxes represent the intermediate products and data generated by the pipeline. A~blue outline highlights the important outputs of the pipeline, and a green outline indicates the tool developed in this thesis (in addition to assembling and controlling the entire pipeline). The training pipeline includes phases such as bitcode generation, graph construction, and feature engineering, detailed in Sections~\ref{bitcode-generation},~\ref{graph-construction-phase}, and~\ref{feature-engineering}, respectively. Icons were taken from~\cite{icon-git, icon-ibm, icon-tfrecords, icon-model-training, icon-model}.}
	\label{figure:training-pipeline}
\end{figure}

The goal of the training pipeline is to transform the D2A dataset into its graph version -- Graph D2A, upon which a GNN model will be trained. Figure~\ref{figure:training-pipeline} shows that the training pipeline consists of three stages -- bitcode generation, graph construction, and feature engineering, each detailed in Sections~\ref{bitcode-generation},~\ref{graph-construction-phase}, and~\ref{feature-engineering}, respectively. The input to the entire pipeline is the D2A dataset along with the project repositories from which D2A was generated. The outputs of the pipeline:
\begin{enumerate}
    \item For each project: a \textbf{Graph D2A dataset} -- the D2A dataset transformed into raw extended code property graphs (ECPGs) in the CSV format (see Sectio~\ref{graph-d2a}), which can be used for training GNNs (not only for ranking static analysis reports).
    \item For each project: a \textbf{Graph D2A dataset with feature engineering} (see Section~\ref{feature-engineering}) prepared in the commonly used TFRecords format (again, see Section~\ref{feature-engineering}) for GNN training.
    \item Same for all projects: the \textbf{TFGNN schema} describing the format of the Graph D2A with feature engineering (see Section~\ref{feature-engineering}).
    \item Might be same for all projects (see Chapter~\ref{hyperparameter-tuning}): the \textbf{GNN model} for ranking Infer reports.
\end{enumerate}

Both the training and inference pipelines internally use a conversion to LLVM IR. Since many languages can be compiled into LLVM IR (see Section~\ref{slicer}), the Graph D2A can, to some extent, be considered language-independent; consequently, the models trained on it can also be considered as such. However, it is still important to remember that the original language was C. There are several advantages to generating graphs from LLVM IR:
\begin{itemize}
    \item The output graphs have a simpler structure (as LLVM IR is a much simpler language compared to, for example, C or C++).
    \item The existing tools like LLVM-Slicer and LLVM2CPG can be utilized.
    \item The output graphs are language-independent.
\end{itemize}
However, there are also disadvantages:
\begin{itemize}
    \item The output graphs are larger in terms of the number of nodes and edges.
    \item It is not possible to transform the dataset directly; instead, a recompilation of individual D2A samples is necessary.
\end{itemize}


\subsection{Bitcode Generation}
\label{bitcode-generation}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/bitcode-generation.pdf}
	\caption{The figure shows a schematic of the bitcode generation phase,  which generates LLVM bitcode for each D2A sample whose error type is supported. Dashed boxes represent the intermediate products and generated data. A~green outline highlights the tools developed in this thesis. Icons were taken from~\cite{icon-git, icon-ibm, icon-llvm}.}
	\label{figure:bitcode-generation}
\end{figure}

LLVM bitcode is a binary representation of LLVM IR and can be freely converted between the two~\cite{LLVM-bitcode}. For conversion from LLVM IR to LLVM bitcode, the tool \texttt{llvm-as} (llvm assembler) is used, and for the reverse conversion, \texttt{llvm-dis} (llvm disassembler) is employed. However, these tools are not strictly necessary because, as shown in Figure~\ref{figure:bitcode-generation}, the bitcode generator directly produces LLVM bitcode, and all other parts of the pipeline (that work with LLVM IR) operate directly on LLVM bitcode as well.

The objective of the bitcode generator is to produce a set of LLVM bitcode files for each sample from the D2A dataset. The number of LLVM bitcode files for each sample is equal to the number of source files for that sample in D2A. The names of the source files for each sample can be extracted from the \texttt{compiler\_args} attribute in D2A. Each sub-attribute in \texttt{compiler\_args} follows the format~\cite{d2a-sample-description}:

\begin{lstlisting}[language=bash, xleftmargin=2em]
'file.c' : '-compiler\_arg1 -compiler\_arg2 ...'
\end{lstlisting}

Taking only the keys will produce a set of files (for a given sample) that need LLVM bitcode generation.

Before generating LLVM bitcode, it is essential first to filter the dataset and remove samples that will not be transformed. Tables~\ref{tab:d2a-bug-types1} and~\ref{tab:d2a-bug-types2} show that some error types have very few true positives. A~small number of positive samples can make it difficult for models to train as they may not have sufficient information to learn the underlying patterns of true positives for those error types. Consequently, all error types with fewer than \textbf{200 true positives} across the entire dataset will be filtered out. \texttt{DEAD\_STORE} errors are always true positives, as established in previous work by the author~\cite{bc} and also confirmed by the D2A authors' experiments~\cite{D2A-zheng2021d2a} who do not include \texttt{DEAD\_STORE} errors in the manual verification of D2A.

An exception is made for the error types \texttt{BUFFER\_OVERRUN\_L1} and \texttt{INTEGER\_OVERFLOW\_L1}, which are included despite having only 28 and 22 true positives, respectively. The reason is that \texttt{BUFFER\_OVERRUN\_L1} is the same as, for example, \texttt{IBUFFER\_OVERRUN\_L5} -- the only difference being that Infer is more certain of the truthfulness of \texttt{L1} than \texttt{L5} (more information in~\cite{bc}), similarly for \texttt{INTEGER\_OVERFLOW\_L1}. Thus, both errors share the same underlying pattern, and the model should be capable of learning it. In the end, only errors with fewer than 30 true positives are removed, and even out of those, not all are removed, hence the overall data loss is minimal. However, this filtering implies a limitation that the model can only be applied to supported bug types which are highlighted in Tables~\ref{tab:d2a-bug-types1} and~\ref{tab:d2a-bug-types2}.

Generating LLVM bitcode can be accomplished during the compilation using the \texttt{clang} compiler (all projects in D2A are written in C language) of the specified source file by inserting the following options~\cite{clang-doc} (which were recommended by the old version of Joern documentation\footnote{Joern's old documentation (unavailable): \url{https://docs.joern.io/llvm2cpg/getting-bitcode}.}, which unfortunately is no longer available) into the compilation command:

\begin{enumerate}
    \item \texttt{-emit-llvm} -- ensures that LLVM bitcode is used for \textit{object files}.
    \item \texttt{-g} -- adds debug information which allows \textit{backward mapping} of LLVM bitcode to the original source code, enabling the use of program slicing based on location information~\cite{llvm-slicer-readme}.
    \item \texttt{-grecord-command-line} -- inserts more debug information into the LLVM bitcode.
    \item \texttt{-fno-inline-functions} -- disables the use of \textit{inline functions}.
    \item \texttt{-fno-builtin} -- prevents the compiler from inserting \textit{built-in functions}.
\end{enumerate}

The compilation command must indeed be specifically for compiling (it must include \texttt{-c}), and not for linking, preprocessing, etc. Additionally, the \texttt{-o} option along with its value must be removed, so that the compilation command generates a \texttt{.bc} (LLVM bitcode) file instead of the original file. The \texttt{compiler\_args} attribute contains only options -- typically just \texttt{-I} (include directories) and \texttt{-D} (definitions of macros and their values). Since neither \texttt{-c}, \texttt{-o}, nor the specific compiler used are mentioned among the options, it is unnecessary to remove \texttt{-o} or check if it is indeed a compilation command. Instead of the originally used compiler (which cannot be identified from D2A alone), \texttt{clang} will be used. Given that Infer also internally uses \texttt{clang} (see Section~\ref{infer}), it ensures that both compilations -- for analysis and for bitcode generation -- are identical (different compilers might apply different optimizations and have different default behavior). The resulting compilation command for the file \texttt{file.c}, generating \texttt{file.bc}, would look like this:

\begin{lstlisting}[language=bash, xleftmargin=2em]
clang -emit-llvm -g -grecord-command-line -fno-inline-functions \
-fno-builtin {D2A_compiler_args} -c file.c
\end{lstlisting}

Thanks to this compilation process and the information from \texttt{compiler\_args}, the definition and application of macros are successfully achieved. This addresses the previously unconsidered problem of conditional compilation, which was mentioned at the beginning of this chapter.

At this stage, the use of the inference pipeline, described in more detail in Section~\ref{inference-pipeline}, might seem applicable. It is capable of generating LLVM bitcode for any C/C++ project. However, D2A consists of 6 projects, and within a single project, the samples are not made from the same version, but from thousands of different versions of the given project. The inference pipeline would thus need to be executed separately for each of these versions, which is computationally infeasible. The generated LLVM bitcode for each sample would be vast, and most of it would later be removed during program slicing. Instead, information from D2A and the git repositories of projects from D2A is used to compile only the necessary files on specific project versions.

For simplicity, consider the transformation of a single project within D2A. For each sample, it is necessary to restore the project repository to the version (commit) in which the error appears, which can be obtained from the D2A attribute \texttt{commit}. Then, the names of the files that need to be transformed into LLVM bitcode are extracted from D2A. These files are then compiled to generate LLVM bitcode. However, for this process to be fully automated, successful compilation of at least the required files across all required commits must be ensured. Proper configuration data, all dependencies, generated data (e.g., C headers), etc., are needed for successful compilation. All these elements change with software development, and automating LLVM bitcode generation requires manual adjustment to the specific project (more in Section~\ref{implementation-bitcode-generator}).

Once a set of LLVM bitcode files is generated for each sample, these files need to be merged into a single one. This requirement stems from the requirements of the LLVM-Slicer tool (see Section~\ref{slicer}). The tool \texttt{llvm-link}~\cite{llvm-link-doc} is used for this purpose, which, despite its name, is not involved in the typical linking process of compilers. \texttt{llvm-link} merely combines multiple LLVM bitcode files into a single one while preserving the LLVM bitcode format. The tool \texttt{llvm-link} was chosen based on recommendations in the documentation of the LLVM-Slicer tool~\cite{llvm-slicer-readme}. The output of the bitcode generation phase is, for each sample in the dataset, a single LLVM bitcode file containing the transformed source code of all files relevant to that sample. Additionally, the D2A dataset is filtered to include only supported error types.


\subsection{Graph Construction}
\label{graph-construction-phase}
The input to the graph construction phase, as shown in Figure~\ref{figure:graph-construction}, consists of the filtered D2A dataset and an LLVM bitcode file for each sample. The output for each sample is a~CPG, extended with additional information (the format of the output data is described in Section~\ref{graph-d2a}). The output graphs are stored in the CSV format for the Neo4j database. Additionally, a script in the Cypher language is included with each sample to load the respective graph into the database~\cite{joern-doc}. Although Neo4j is commonly used for storing and querying graph data, it is not used in this thesis.

\begin{figure}[t]
	\centering
	\includegraphics[width=1\textwidth]{figures/graph-construction.png}
	\caption{The figure shows a schematic of the graph construction phase, which creates a~raw ECPG for each input D2A sample. Dashed lines represent the intermediate products and generated data. A~green outline highlights the tools developed in this thesis. Icons were taken from~\cite{icon-ibm, icon-funnel, icon-shiftleft, icon-graph, joern-website}.}
	\label{figure:graph-construction}
\end{figure}

\subsubsection{Program Slicing}
First, for each sample, it is necessary to extract information required for program slicing performed by the LLVM-Slicer (see Section~\ref{slicer}) from the D2A dataset. This information includes:
\begin{itemize}
    \item The \textbf{entry point function} -- the function in which the program slicing should start (the top-most function in the bug trace).
    \item The \textbf{file} -- the name of the file where the error is located.
    \item The \textbf{function} -- the name of the function in which the error is located.
    \item The \textbf{line} -- the line number on which the error is located.
    \item The \textbf{variable} (optional) -- the variable related to the error (relevant only for certain types of errors).
\end{itemize}

These details form the so-called slicing criteria. The challenge with extracting slicing criteria is that each type of error has a different format, and these details cannot be uniformly obtained from all samples. The name of the entry point function is an exception and can be retrieved for all samples from the \texttt{procedure} attribute (both from D2A and Infer analysis output). The entry function is the highest-level function in the call graph\footnote{A \textbf{Call Graph} is an oriented graph in which nodes represent functions and edges represent calls between these functions. It can be considered a substructure of the control flow graph to some extent.} among all the functions mentioned in the Infer bug trace. Practically, this means that anything above this function is not important for the manifestation of the error. If the entry function itself is called with the correct parameters and in the right context, it must lead to the reported error (assuming it is a true positive). This is crucial information for program slicing, as it means that everything above the entry function can be discarded (which is exactly what LLVM-Slicer does), because it is not useful for the future classification of true positives/false positives.

In total, 14 types of errors are supported, as shown in Tables~\ref{tab:d2a-bug-types1} and~\ref{tab:d2a-bug-types2}. However, some error types are similar enough that slicing criteria can be extracted in the same way, resulting in the following groups:
\begin{enumerate}
    \item \texttt{NULLPTR\_DEREFERENCE} -- this group contains only the identically named type.
    \item \texttt{INTEGER\_OVERFLOW} -- includes all \texttt{INTEGER\_OVERFLOW\_}\textbf{X} where $X \in \{\texttt{L1}, \texttt{L2}, \texttt{L5}, \texttt{U5}\}$.
    \item \texttt{INFERBO\_ALLOC\_MAY\_BE\_BIG} -- contains only the identically named type.
    \item \texttt{UNINITIALIZED\_VALUE} -- again contains only the identically named type.
    \item \texttt{BUFFER\_OVERRUN} -- includes all \texttt{BUFFER\_OVERRUN\_}\textbf{X} where 
    \newline
    $X~\in~\{\texttt{L1}, \texttt{L2}, \texttt{L3}, \texttt{L4}, \texttt{L5}, \texttt{U5}\}$.
    \item \texttt{NULL\_DEREFERENCE} -- the last group contains only the identically named type.
\end{enumerate}

\paragraph{The \texttt{NULLPTR\_DEREFERENCE} Group}
For errors of type \texttt{NULLPTR\_DEREFERENCE}, three different formats of the Infer output were found in the D2A, distinguishable by the \texttt{qualifier} attribute~\cite{d2a-sample-description}, which contains a~brief description of the error and shares the same name as in the JSON output of Infer:
\begin{enumerate}
    \item \texttt{'call to `put\_bits()` eventually accesses memory that is the null 
    \newline
    pointer on line 543 indirectly during the call to `init\_put\_bits()`.'}
    \newline
    -- e.g., sample with \texttt{id: ffmpeg\_8e48b53d696b53cef2814548e4d0693387e875ea\_1}.
    \item \texttt{'accessing memory that is the null pointer on line 3191 indirectly 
    \newline
    during the call to `av\_malloc()`.'} 
    \newline
    -- e.g., sample with \texttt{id: ffmpeg\_6a30264054cc320fe610c072c71d008f7e3c3efb\_1}.
    \item \texttt{'accessing memory that is the null pointer on line 315.'}
    \newline
    -- e.g., sample with \texttt{id: ffmpeg\_9c908a4c99e0498dd26bd1de84ff085ac8e73e4a\_1}.
\end{enumerate}

For Cases 2) and 3), the \texttt{file}, \texttt{function}, and \texttt{line} information in the \texttt{bug\_info} attribute are correct~\cite{d2a-sample-description}. However, for Case 1), the same information only contains the location of the function call where the dereference occurs -- in this case, \texttt{put\_bits()}. The correct error location needs to be obtained from the last step of the \texttt{trace} attribute (bug trace), as shown in Listing~\ref{listing:nullptr-dereference-trace} which shows this last step for Case 1). For Cases 2) and 3), the locations in the \texttt{bug\_info} and the last \texttt{trace} step coincide, thus uniformly extracting information from the last bug trace step is feasible for all three formats. Additionally, all three formats share the same last step with \texttt{description} -- \texttt{invalid access occurs here}. The only difference is that in \texttt{trace}, \texttt{function} is named as \texttt{func\_name} and \texttt{line} must be extracted from \texttt{loc}, as seen in Listing~\ref{listing:nullptr-dereference-trace}.

\begin{lstlisting}[
    language=json, 
    label={listing:nullptr-dereference-trace}, 
    float=t,
    caption={The last step of the \texttt{trace}~\cite{d2a-sample-description} for a \texttt{NULLPTR\_DEREFERENCE} error taken from the sample with \texttt{id: ffmpeg\_8e48b53d696b53cef2814548e4d0693387e875ea\_1} on the FFmpeg project. The listing demonstrates the format of storing a bug trace in D2A.}
]
 "trace": [
     // ... other trace steps ...
     {
         "idx": 16,
         "level": 2,
         "description": "invalid access occurs here",
         "func_removed": null,
         "file_removed": null,
         "file": "libavcodec/put_bits.h",
         "loc": "179:9",
         "func_name": "put_bits",
         "func_key": "libavcodec/put_bits.h@139:1-189:2",
         "is_func_definition": true,
         "url": "https://github.com/FFmpeg/FFmpeg/blob/5962f6b0da037da30 fcc848331afa6a081a4eb09/libavcodec/put_bits.h/#L179"
     }
    ]
\end{lstlisting}

For \texttt{NULLPTR\_DEREFERENCE} errors, it is indeed sensible to consider extracting the variable name because null dereferences typically occur on a variable. However, they can theoretically occur on a constant like \texttt{(NULL)} or a general expression, such as \texttt{(p-p)}. When the null dereference occurs on a variable, it would be best to extract the name of this variable along with the error location. A~more precise slicing criterion would allow for more accurate program slicing, thereby removing more unnecessary information from future graphs. Unfortunately, the variable name does not appear in any of the described formats of \texttt{NULLPTR\_DEREFERENCE} in the Infer output (and thus not in D2A either). The only clue available is the column, which can be extracted from the \texttt{loc} attribute (see Listing~\ref{listing:nullptr-dereference-trace}). Knowing the line and column where the dereference occurs might seem to simplify the extraction of the variable name. However, it is necessary to distinguish when the dereference is on a variable, a constant, an expression, or a macro. Distinguishing a variable can be done straightforwardly using the C language naming rules for variables -- a name can only follow the pattern \texttt{[a-zA-Z\_][a-zA-Z0-9\_]*}, and anything else cannot be a variable and thus should not be extracted. However, macros can be named the same as variables (and typically are). Therefore, distinguishing between a variable and a macro is non-trivial and would require at least preprocessing by a compiler and subsequent adjustment of the error position, as macro expansion can change both the line and column. For now, the extraction of the variable name for \texttt{NULLPTR\_DEREFERENCE} will be left for future improvements.

\paragraph{The \texttt{INTEGER\_OVERFLOW} Group}
For the \texttt{INTEGER\_OVERFLOW} error types, two formats were identified, which can again be distinguished using the \texttt{qualifier} attribute:
\begin{enumerate}
    \item \texttt{([0, 8] - [0, 8]):unsigned32.} 
    \newline
    -- e.g., sample with \texttt{id: ffmpeg\_1542087b54ddf682fb6177f999c6f9f79bd5613f\_1}.
    \item \texttt{([0, 1] - 1):unsigned32 \textbf{by call to} `avfilter\_unref\_buffer`.} 
    \newline
    -- e.g., sample with \texttt{id: ffmpeg\_ca5973f0bfac4560342605f8a52efc88b4f4dbd3\_1}.
\end{enumerate}
For Case 1), location information can be obtained directly from \texttt{bug\_info}. For Case 2), a~similar problem arises as with \texttt{NULLPTR\_DEREFERENCE} Case 1) -- \texttt{bug\_info} contains only the location of the function call, in this case, \texttt{avfilter\_unref\_buffer}, where the integer overflow/underflow occurs. It is necessary to extract information from the \texttt{trace} attribute. To cause an overflow/underflow, two operands are needed (with the exception of operators like \texttt{++}, which in terms of value change is equivalent to \texttt{+1}), and each operand can either be a variable or an expression. Although LLVM-Slicer is capable of slicing based on multiple criteria (in this case, both operands), to ensure the resulting graph is complete, it is necessary to include the operation itself. Thus, slicing must be based on the entire line, as slicing by expression is not yet supported (to the best of the author's knowledge).

\paragraph{The \texttt{INFERBO\_ALLOC\_MAY\_BE\_BIG} Group}
The \texttt{INFERBO\_ALLOC\_MAY\_BE\_BIG} group contains only a single error type of the same name, and only one format was identified:
\begin{enumerate}
    \item \texttt{Length: [0, 2147483631] \textbf{by call to} `av\_dup\_packet`.}
    \newline
    -- e.g., sample with \texttt{id: ffmpeg\_c36d9fb10c31c6835d01232fddff6932a3ce347f\_1}.
\end{enumerate}

Similar to \texttt{NULLPTR\_DEREFERENCE} Case 1), it is necessary to extract the location from the last step of the \texttt{trace} because the \texttt{bug\_info} points to a function call in which the error occurs, in this case, \texttt{av\_dup\_packet}. The correct location points to a function call that allocates memory, such as \texttt{malloc}, \texttt{realloc}, etc. If the call appears as \texttt{realloc(ptr, size)}, ideally, it is preferable to slice directly by \texttt{size} because its value is of interest. However, the call can often appear as \texttt{realloc(ptr, str\_len + 1)}, and in this case, the value of the entire expression is needed. Of course, it is possible to extract the name of the variable only in certain cases, but there are still the previously mentioned issues with macros and also with detecting the correct argument. From the output of Infer, it is not possible to determine which argument it concerns. With functions like \texttt{malloc} or \texttt{realloc}, the argument is known from their definitions, but it is necessary to consider cases where, during Infer analysis, custom models are created, and theoretically, any function could be considered an allocation function. For these reasons, for this type of error, slicing is only done by the line. Moreover, allocation functions typically are not very large, so including their code in the graphs does not represent too much unnecessary data.

\paragraph{The \texttt{UNINITIALIZED\_VALUE} Group}
The \texttt{UNINITIALIZED\_VALUE} group also contains only a single error type of the same name. Two formats have been identified, which can again be distinguished using the \texttt{qualifier} attribute:
\begin{enumerate}
    \item \texttt{The value read from ret was never initialized.} 
    \newline
    -- e.g., sample with \texttt{id: ffmpeg\_ed80423e6bcfe18cca832b74dcc877427f8cf346\_1}.
    \item \texttt{The value read from pix[\_] was never initialized.}
    \newline
    -- e.g., sample with \texttt{id: ffmpeg\_1f62bae77d6ced3b79deaa8ce5ba3381fd4a541d\_1}.
\end{enumerate}
Neither format includes additional information in the \texttt{trace}, so the information is solely from \texttt{bug\_info}. The location is correct for both formats. Case 1) concerns uninitialized variables, where it makes sense to slice also by the variable because the specific variable itself is of interest. Moreover, the variable's name can be easily obtained from the \texttt{qualifier}. Case 2) concerns uninitialized arrays (or items in an array), where the situation is more complex because it is not possible to obtain the access index into the array from the Infer output. It could be extracted from the code, but problems such as slicing by expressions and macros arise. If slicing is done only by the line, then information about the index would be included in the output. However, experience from checking outputs from Infer (especially in~\cite{bc}) suggests that if an array item is uninitialized, it is because the entire array was not initialized. For these reasons, even for Case 2), the variable name is extracted -- in this case, it is always an array, and the slicing is done with respect to the array.

\paragraph{The \texttt{BUFFER\_OVERRUN} Group}
For the \texttt{BUFFER\_OVERRUN} error types, two formats have been identified, which can again be distinguished using the \texttt{qualifier} attribute:
\begin{enumerate}
    \item \texttt{Offset: [0, 15] Size: 4.}
    \newline
    -- e.g., sample with \texttt{id: ffmpeg\_61d490455ade68a02dfdcfdb172ba3ded2fe0f9d\_1}.
    \item \texttt{Offset: [1, 4] Size: 4 by call to `filter\_mb\_mbaff\_edgecv`.} 
    \newline
    -- e.g., sample with \texttt{id: ffmpeg\_0f5e5ecc888af015015f2ce1211a066350fbe377\_1}.
\end{enumerate}
For Case 1), the information in \texttt{bug\_info} is correct. For Case 2), the location needs to be taken from the last step of the \texttt{trace} again. Neither format in the Infer output specifies the name of the array or the index name (if it involves a variable rather than an expression). For these types of errors, both the name of the array and the index are necessarily required. To obtain both names, a more complex extraction method from the source code would again be necessary. Hence, for this type of error, slicing is currently done by the line.

\paragraph{The \texttt{NULL\_DEREFERENCE} Group}
The \texttt{NULL\_DEREFERENCE} group contains only a single error type of the identical name. \texttt{NULL\_DEREFERENCE} and \texttt{NULLPTR\_DEREFERENCE} are semantically identical, with the difference lying in the Infer plugin that produced them -- Bi-abduction (\texttt{NULL\_DEREFERENCE}) and Pulse (\texttt{NULLPTR\_DEREFERENCE}). Since they originate from different plugins (which may have issues with different language constructs leading to varying patterns of true and false positives) and also have different formats, it makes sense to list them separately. Two formats have been found, distinguishable by the \texttt{qualifier} attribute:

\begin{enumerate}
    \item \texttt{pointer `filter` last assigned on line 3191 could be null and is
    \newline
    dereferenced at line 3194, column 9.}
    \newline
    -- e.g., sample with \texttt{id: ffmpeg\_15ae526d6763d8e21833feb78680ee3571080017\_1}.
    \item \texttt{pointer `null` is dereferenced by call to `ff\_sdp\_write\_media()` at
    \newline
    line 2538, column 5.}
    \newline
    -- e.g., sample with \texttt{id: ffmpeg\_a94ada4250ff1d9e6101c910fe71dde6c3b5e485\_1}.
\end{enumerate}

Ideally, for this type of error, it would be desirable to slice by the variable name (if the incorrect dereference occurs on a variable). For Case 1), both the location information and the variable name can be obtained directly from \texttt{bug\_info}, as the variable name is mentioned in the \texttt{qualifier}. For Case 2), \texttt{bug\_info} contains only the location of the function call within which the incorrect dereference occurs. If the \texttt{qualifier} in Case 2) contains a variable name (instead of \texttt{null}), it is not the variable on which the dereference occurs but a variable whose value is passed to the called function. Unfortunately, in the \texttt{trace}, it is not possible to determine which step represents the incorrect dereference because most steps lack a \texttt{description}. In some cases, the last step in \texttt{trace} represents the incorrect dereference, but in some others, it does not. Unfortunately, it is also not possible to distinguish between these types. For these reasons, for Case 2), slicing is done only by the line of the called function. This ensures that the error is included in the graph, even if it is deeper in the call graph. However, this introduces a significant ammount of unnecessary information.

\paragraph{The \texttt{adjusted\_bug\_loc} Attribute}
An important note is that any sample, regardless of the type of error, may contain the \texttt{adjusted\_bug\_loc} attribute in D2A. The attribute adjusts the location of the error if the \texttt{bug\_info} -- extracted directly from Infer's output -- does not precisely pinpoint the error's location. The \texttt{adjusted\_bug\_loc} was derived using the same principles previously described for each group of error types. This raises the question of why not directly use \texttt{adjusted\_bug\_loc}. In the training pipeline, it is indeed possible to use these data, but in the inference pipeline, this information is no longer available because it only operates with Infer's output on real-world software, not with D2A. Therefore, a similar method of extracting precise error locations from the Infer report will eventually need to be designed and implemented for real-world applications. Utilizing it also in the training pipeline has additionally allowed verification of whether the author of this thesis and the authors of D2A agree on the method for extracting the exact location of errors -- this has been verified across all samples of supported error types.

\paragraph{The Application of Program Slicing}
The extracted slicing criteria, along with LLVM bitcode, form the input for LLVM-Slicer (see Section~\ref{slicer}). The output is a sliced LLVM bitcode according to the input slicing criteria. The purpose of program slicing is to remove parts of the graphs that do not influence the occurrence of the error. Consequently, this effectively reduces the size of the resulting graph and eliminates unnecessary information, which should facilitate and speed up the learning process for GNN models. LLVM-Slicer was chosen based on a recommendation by Ing. Viktor Malík. Upon verification, it was found to meet all the requirements, particularly in terms of input and output formats. An alternative, such as the tool \texttt{llvm-slicing}\footnote{\textbf{llvm-slicing}'s repository: \url{https://github.com/zhangyz/llvm-slicing}.}, is no longer maintained.

Program slicing at this stage of the pipeline also allows for specifying slicing criteria in relation to the original code, which is more appropriate than specifying them later in the CPG and slicing using tools like Joern~\cite{joern-doc}. Theoretically, it should be possible to identify slicing criteria in the output CPG graph thanks to debug information (see Section~\ref{bitcode-generation}) attached to individual nodes, which can map certain LLVM constructions back to the original code. However, this information may be lost during CPG construction, as discussed in Section~\ref{llvm2cpg}. Consequently, it becomes challenging to accurately map CPG nodes back to the original code, risking the incorrect construction of the node set intended for slicing criteria.

\subsubsection{Generation of Extended Code Property Graphs from LLVM Bitcode}
Sliced LLVM bitcode serves as the input to the LLVM2CPG tool (see Section~\ref{llvm2cpg}), which generates CPGs. These CPGs are then processed by the Joern tool (see Section~\ref{joern}), whose task is to:

\begin{enumerate}
    \item convert CPGs from binary to the CSV format,
    \item create Extended CPGs (ECPGs) by adding additional layers such as information about types, files, functions, and more (see Section~\ref{graph-d2a}).
\end{enumerate}

Joern is utilized solely as a CLI\footnote{\textbf{Command Line Interface (CLI)}.} tool in this thesis. It takes a binary CPG and a script with a list of commands to execute, primarily load and save operations, as Joern automatically constructs additional layers upon loading. The resulting ECPGs are saved again in a binary format. The final step involves converting the binary format into the easily usable CSV format using a Joern sub-tool -- \texttt{joern-export}~\cite{joern-doc}, which creates a directory with CSV files containing:
\begin{itemize}
    \item CSV header file,
    \item CSV data file (without header),
    \item Cypher script for importing into the Neo4j database.
\end{itemize}

The CSV header file is kept for each sample, even though it might seem unnecessary. The reason lies in Joern's non-deterministic behavior regarding the columns generated on different machines. For instance, the header for \texttt{METHOD} nodes (see Section~\ref{graph-d2a}) in httpd (true positives) and openssl (true positives) -- openssl has two additional columns, which, however, contain no useful information.

The Joern tools (and Joern Export) were chosen due to their frequent use in the field of GNNs~\cite{GNN1-cao2021bgnn4vd, JOERN-CPG-guan2020code, GNN4-IBM-suneja2020learning, JOERN-CPG-xiaomeng2018cpgva, GNN2-zhou2019devign}. LLVM2CPG is specifically recommended on the LLVM Project website for generating CPGs in combination with Joern~\cite{llvm2cpg-webpage}.

\subsection{Graph D2A}
\label{graph-d2a}
The D2A dataset, where each sample is transformed into an ECPG generated using the Joern tool, will henceforth be referred to as \textbf{Graph D2A}. Graph D2A is one of the main contributions of this thesis and, in combination with the original D2A, enables other researchers to create their own graph representations (based on CPGs) for GNNs and apply their own feature engineering. As mentioned earlier, ECPGs contain additional information compared to CPGs described in Section~\ref{cpg}. Additionally, the CSV format facilitates further preprocessing of the dataset before inputting into GNNs. In this thesis, individual samples of Graph D2A will be referred to as raw ECPGs, precisely because they are in the CSV format and because the node/edge attributes are not yet processed or modified -- they often lack, have an inappropriate format, and the format is not uniform (\texttt{int}, \texttt{float}, \texttt{string}, etc.). Processing raw ECPGs into a format suitable for training GNNs will be addressed in Section~\ref{feature-engineering}.

The complete output format of raw ECPGs is described in the automatically generated documentation of the Joern tool~\cite{joern-cpg-doc} (version 1.1), from which the following information is also taken. Like CPGs, raw ECPGs are directed, node-labeled, edge-labeled, multigraphs. The set of nodes that share the same label will be referred to as a \textit{node set}, for future compatibility. All nodes within a single node set have the same set of attributes (although some values may be missing). Similarly, edges that share the same label will be referred to as an \textit{edge set}. No edge sets (except \texttt{REACHING\_DEF}) in raw ECPGs have attributes, but as mentioned in the following sections, it may make sense to move certain attributes from nodes to edges. Raw ECPGs consist of \textit{layers}, where each layer can add additional node/edge sets or their attributes. Individual layers may be language-dependent, as Joern can generate ECPG graphs from any language for which a frontend is written. Currently, this includes languages such as \textbf{LLVM IR}, C/C++/C\#, Java, JavaScript, Python, Kotlin, PHP, Go, Ruby, Swift, and more~\cite{joern-cpg-doc, joern-doc}. The Joern layers that are \textbf{completely} missing for LLVM IR are the \textbf{Comment Layer}, \textbf{Finding Layer}, \textbf{TagsAndLocation Layer}, \textbf{Configuration Layer}, \textbf{Binding Layer}, and \textbf{Annotation Layer}. These missing layers will not be further described.

Since this thesis utilizes only LLVM IR (specifically LLVM bitcode) as Joern input, the following explanation includes only layers generated for LLVM IR. Although the Joern documentation is highly detailed, it does not describe all the attributes of the node sets. A~complete list of these attributes is provided in Table~\ref{tab:attributes1}. The description of each attribute is discussed in detail in Section~\ref{feature-engineering}, where the removal of irrelevant attributes is addressed. Node sets are hierarchically organized -- if node set \texttt{X} inherits from node set \texttt{Y}, it implies that \texttt{X} contains the same attributes as \texttt{Y} and adds some unique ones (typically). A~visualization of the hierarchy of node sets (which are generated for LLVM bitcode) and base class node sets can be seen in Figure~\ref{figure:node-set-hierarchy}.

\begin{figure}[t]
	\centering
	\includegraphics[width=1\textwidth]{figures/node-set-hierarchy.pdf}
	\caption{The figure shows the hierarchy of node sets in green (generated for LLVM IR) and the base class node sets in blue.}
	\label{figure:node-set-hierarchy}
\end{figure}

\textbf{The MetaData Layer} contains only one node set -- \texttt{META\_DATA}. In each graph, there is precisely one such node with \texttt{ID: 1}, containing information about how the graph was generated -- e.g., input language, version, etc.

\textbf{The FileSystem Layer} includes information about the files from which the graph was generated. It specifically adds the node set \texttt{FILE}, where each node represents an input source file. This layer also introduces an edge set \texttt{SOURCE\_FILE}, which connects nodes from other node sets to \texttt{FILE} nodes based on their source file.

\textbf{The Namespace Layer} introduces the \texttt{NAMESPACE} node set, which resembles \texttt{FILE} and describes the namespace as known from programming. This layer also introduces the \texttt{NAMESPACE\_BLOCK} node set, which groups code under a common namespace, defined using specific statements like \texttt{namespace{ }} in C++ or \texttt{package} in Java.

\textbf{The Method Layer} includes declarations of methods, functions, and procedures (collectively referred to as 'functions' hereafter). This layer also includes their inputs and outputs but \textbf{does not contain their code}. Included in this layer are node sets:
\begin{itemize}
    \item \texttt{METHOD} -- information about a specific function.
    \item \texttt{METHOD\_PARAMETER\_IN} -- represents the input parameters of a specific \texttt{METHOD} node.
    \item \texttt{METHOD\_PARAMETER\_OUT} -- represents the output parameters corresponding to the inputs of a specific \texttt{METHOD} node.
    \item \texttt{METHOD\_RETURN} -- represents the return parameter of a specific \texttt{METHOD} node.
\end{itemize}

\textbf{The Type Layer} contains information about type declarations, type relationships, type instantiation, type hierarchies, parameterized types, and aliases. This layer introduces the following node sets:
\begin{itemize}
    \item \texttt{MEMBER} -- member of a structured type.
    \item \texttt{TYPE} -- instance of a type.
    \item \texttt{TYPE\_ARGUMENT} -- argument used during parameterized type instantiation (e.g., Java \texttt{Generics}, C++ \texttt{templates}).
    \item \texttt{TYPE\_DECL} -- type declaration.
    \item \texttt{TYPE\_PARAMETER} -- formal parameter of parameterizable types.
\end{itemize}
Additionally, the layer provides the following edge sets:
\begin{itemize}
    \item \texttt{ALIAS\_OF} -- alias relationship between a type declaration and a type.
    \item \texttt{BINDS\_TO} -- links type arguments to type parameters during type instantiation.
    \item \texttt{INHERITS\_FROM} -- inheritance relationship between type declarations and types.
\end{itemize}

\textbf{The Ast Layer} is the core of ECPGs, providing ASTs for all input code. AST nodes are linked into trees via the \texttt{AST} edge set, and sibling positions in the tree are specified using the \texttt{ORDER} attribute. The layer offers the following node sets:
\begin{itemize}
    \item \texttt{AST\_NODE} -- template providing basic attributes of AST nodes.
    \item \texttt{BLOCK} -- compound statement grouping multiple statements.
    \item \texttt{CALL} -- function call.
    \item \texttt{CALL\_REPR} -- template for the \texttt{CALL} node set.
    \item \texttt{CONTROL\_STRUCTURE} -- control structure statements and jumps.
    \item \texttt{EXPRESSION} -- template for any code fragment that can be evaluated.
    \item \texttt{FIELD\_IDENTIFIER} -- identifier of an element in an array.
    \item \texttt{IDENTIFIER} -- identifier of a variable.
    \item \texttt{JUMP\_LABEL} -- jump label.
    \item \texttt{JUMP\_TARGET} -- any code location marked as a jump target.
    \item \texttt{LITERAL} -- constant.
    \item \texttt{LOCAL} -- local variable.
    \item \texttt{METHOD\_REF} -- function reference when passed as a parameter.
    \item \texttt{MODIFIER} -- language-specific modifiers like \texttt{static},\texttt{private}, \texttt{public}, etc.
    \item \texttt{RETURN} -- \texttt{return} statement.
    \item \texttt{TYPE\_REF} -- reference to a type/class.
    \item \texttt{UNKNOWN} -- other code fragments not classifiable into any of the above node sets.
\end{itemize}

\textbf{The CallGraph Layer} describes the relationships between function calls. This layer provides only the following edge sets:
\begin{itemize}
    \item \texttt{ARGUMENT} -- links \texttt{CALL} nodes to their arguments and \texttt{RETURN} nodes to the expressions they return.
    \item \texttt{CALL} -- links \texttt{CALL} nodes to \texttt{METHOD} nodes.
    \item \texttt{RECEIVER} -- links \texttt{CALL} nodes to the objects on which the method was invoked.
\end{itemize}

\textbf{The Cfg Layer} provides CFGs for all functions. This layer provides the \texttt{CFG\_NODE} node set, which is also an \texttt{AST\_NODE}. Therefore, all \texttt{CFG\_NODE} are \texttt{AST\_NODE}, but not all \texttt{AST\_NODE} are \texttt{CFG\_NODE}. Additionally, the layer adds the \texttt{CFG} edge set, which links \texttt{CFG\_NODE} nodes in the direction of control flow (without distinguishing between true and false paths).

\textbf{The Dominators Layer} provides \textit{dominator} and \textit{post-dominator trees}~\cite{princeton-dominators} for all functions. These trees are closely related to the CFG Layer, as they identify sets of inescapable nodes in CFGs. The layer provides the following edge sets:
\begin{itemize}
    \item \texttt{DOMINATE} -- an edge indicating that the source node \textit{dominates} the destination node.
    \item \texttt{POST\_DOMINATE} -- an edge indicating that the source node \textit{post-dominates} the destination node.
\end{itemize}

\textbf{The Pdg Layer} provides PDGs for all functions. As defined in Section~\ref{cpg}, a PDG should provide data dependency and control dependency edges. The Pdg Layer provides the \texttt{CDG} edge set, which provides control dependency edges (without distinguishing between true and false paths), and the \texttt{REACHING\_DEF} edge set, which indicates that a variable (source node) reaches a specific point (target node) unchanged -- an extension of data dependency edges.

\textbf{The Shortcuts Layer} provides a more explicit representation of certain properties using the following edge sets:
\begin{itemize}
    \item \texttt{CONTAINS} -- links nodes to the function (\texttt{METHOD} node) that contains them.
    \item \texttt{EVAL\_TYPE} -- links a node to its data type (\texttt{TYPE} node).
    \item \texttt{PARAMETER\_LINK} -- connects \texttt{METHOD\_PARAMETER\_OUT} nodes to their corresponding \texttt{METHOD\_PARAMETER\_IN} nodes.
\end{itemize}

\textbf{The Base Layer} provides the \texttt{DECLARATION} node set, which is merely a template for all declarations. Additionally, it provides the \texttt{REF} edge set, which indicates that an \texttt{IDENTIFIER} (source node) belongs to a specific node (target node), e.g., an identifier belongs to a local variable (\texttt{LOCAL} node).

An important note is that neither \texttt{CFG} nor \texttt{CDG} edges contain any information, which differs from the definition in Section~\ref{cpg}. This can cause issues, especially when correctly modeling program branching. However, the required information is present in the graph, and branching can be modeled, as discussed in Section~\ref{feature-engineering}.


\subsection{Feature Engineering}
\label{feature-engineering}
Graph D2A provides raw ECPGs in the CSV format. However, these graphs cannot be directly used to train GNNs. They first need to be transformed into a format suitable for model training, which is ensured by the feature engineering phase in Figure~\ref{figure:feature-enginering}. The graph format is determined by the library chosen for implementing GNNs. In this thesis, TFGNN (TensorFlow GNN) is used, which is an open-source\footnote{\textbf{TFGNN}'s repository: \url{https://github.com/tensorflow/gnn}.} extension of TensorFlow -- one of the most widely used machine learning libraries. A~relatively simple and commonly used dataset format within TF (not only for graph data) is TFRecord, which is designed to store sequences of binary data~\cite{tfrecord} -- graphs, in this case. The input to the feature engineering phase is thus Graph D2A, and the output is a dataset in the TFRecord format. This transformation also includes feature engineering, which consists of the following steps:

\begin{enumerate}
    \item \textbf{Feature Selection} -- removing edge/node sets and their attributes that are not important for ranking false positives.
    \item \textbf{Graph Optimization} -- reducing the graph size while preserving crucial information.
    \item \textbf{Node/Attribute Transformation} -- some nodes/attributes need to be converted to another format or decomposed into multiple components.
    \item \textbf{Feature Normalization} -- it is beneficial to normalize features for more stable training.
\end{enumerate}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\textwidth]{figures/feature-engineering-phase.pdf}
	\caption{The figure shows a schematic of the feature engineering phase -- the final phase of the training pipeline. Dashed boxes represent the intermediate products and generated data. A~blue outline highlights the important outputs of the pipeline, and a green outline indicates the tools developed in this thesis. Icons were taken from~\cite{icon-feature, icon-tfrecords, icon-model-training, icon-model}.}
	\label{figure:feature-enginering}
\end{figure}

In feature engineering, the aim is to refine raw ECPGs to a state where models can be as small as possible while learning and generalizing effectively. The ultimate goal in reducing false positives is to train models that work for \textit{cross-analysis} -- training on one project while performing \textit{inference} on a different, unseen project. None of the models compared in this thesis, specifically those from~\cite{D2A-zheng2021d2a, pujar2024analyzing}, function within cross-analysis. Thus, in feature engineering, adjustments will be made that should help achieve cross-analysis for GNNs. The aim is to remove information from raw ECPGs that could lead the GNN model to \textit{overfit} to individual samples or entire projects. These adjustments include, for instance, removing specific variable names or suppressing the original programming style (already managed by converting to LLVM IR). These modifications are described in detail later in this section.


\subsubsection{Feature Selection}
Feature selection is one of the most crucial parts of the training pipeline because it allows for the greatest reduction in graph sizes. Simultaneously, incorrect feature selection prevents the GNN models from efficiently extracting distinguishing patterns. This significant importance was the primary motivation for Graph D2A (raw EPCGs) to also be one of the outputs of this thesis. If feature selection was poorly executed, the resulting dataset would be unusable, providing only incomplete data for the task. Thanks to Graph D2A, experimenting with different feature selections in the future is possible.

It is important to choose node/edge sets and their attributes that best and most accurately describe the code from the perspective of potential errors. These optimized graphs should contain only the information needed to distinguish between true positives and false positives. Additionally, the aim is for models to generalize well to unseen projects; for instance, the model should not remember variable names and thus recognize specific projects/samples based on them. This section describes which node/edge sets and their attributes were chosen and why (also why some were not chosen). The following description includes only edge/node sets that were generated for LLVM IR (at least for a single sample). Node sets that did not occur (even if some additional information from their layer did, see Section~\ref{graph-d2a}) are \texttt{TYPE\_ARGUMENT}, \texttt{TYPE\_PARAMETER}, \texttt{CONTROL\_STRUCTURE}, \texttt{JUMP\_LABEL}, \texttt{JUMP\_TARGET}, \texttt{MODIFIER}, and \texttt{TYPE\_REF}. Template node sets (see Figure~\ref{figure:node-set-hierarchy}) are also not generated. The edges \texttt{RECEIVER}, \texttt{CONDITION}, \texttt{INHERITS\_FROM}, and \texttt{BINDS\_TO} are also not present in Graph D2A. Attributes not mentioned for a given node set, and that the node set does have (see Table~\ref{tab:attributes1}), contain only a single (or, for instance, two but useless) value across the entire dataset and thus do not provide any useful information. The following descriptions are based on the Joern documentation~\cite{joern-cpg-doc} and the examination of Graph D2A samples.

\paragraph{The \texttt{META\_DATA} Node Set}
The node set \texttt{META\_DATA} will not be used at all because all its attributes contain only a single value for all samples. The only exception might be \texttt{LANGUAGE}, which holds information about the language from which the ECPG was generated. This is useful when the system has multiple input languages, which theoretically could be true for the proposed system (thanks to converting the input language to LLVM IR). However, since each input language would first be compiled to LLVM IR, \texttt{LANGUAGE} would always have the value \texttt{'LLVM'}.

\paragraph{The \texttt{FILE} Node Set}
The node set \texttt{FILE} also contains only attributes with the same values across samples. The reason it does not contain information about files is LLVM Link -- the input file to Joern with LLVM bitcode is always just one, so the node set \texttt{FILE} contains only a single node. Thus, the node set will be completely removed. Simultaneously, the edge set \texttt{SOURCE\_FILE} will also be removed because it depends on the \texttt{FILE} node set.

\paragraph{The \texttt{NAMESPACE} Node Set}
The node set \texttt{NAMESPACE} will also be completely removed. LLVM IR does not have namespaces like other high-level languages. At best, the concept of namespaces can be discussed in relation to individual files. However, the same problem arises as with the \texttt{FILE} node set -- all code is in a single file and thus in a single namespace. The attribute \texttt{NAME} contains only the values \texttt{<global>} and \texttt{llvm-link\_global}. Analogously for the node set \texttt{NAMESPACE\_BLOCK}.

\paragraph{The \texttt{METHOD} Node Set}
The node set \texttt{METHOD} will be used, specifically its attributes:
\begin{itemize}
    \item \texttt{IS\_EXTERNAL} -- has values \texttt{true} and \texttt{false}, indicating whether the function's code is available (and therefore a ECPG) or not (dynamic library call).
    \item \texttt{ORDER} -- the value is always \texttt{0}, but it is retained for later node set merging (see below in this section).
    \item \texttt{FULL\_NAME} -- the full name of the function (e.g., \texttt{malloc}). Ideally, this information should be removed to prevent the model from remembering functions from individual D2A projects, reducing the likelihood of successful cross-analysis. However, it is necessary to distinguish functions for which no code is available. Functions from standard libraries are crucial to remember because, for instance, \texttt{malloc} or \texttt{free} are essential for detecting memory leak errors. Using \texttt{FULL\_NAME} only for \texttt{IS\_EXTERNAL} functions is logical because they often belong to standard libraries, avoiding project-specific function names. However, project-specific functions can also be dynamically linked, and standard functions can be linked statically. For now, \texttt{FULL\_NAME} will be used for all functions, and the usage limited to some functions only is left for future work.
\end{itemize}

The following attributes contain some information but will not be used:
\begin{itemize}
    \item \texttt{AST\_PARENT\_FULL\_NAME} -- for \texttt{METHOD} nodes, this is the name of the \texttt{NAMESPACE\_BLOCK}, which does not contain any useful information, as mentioned before.
    \item \texttt{FILENAME} -- analogously to \texttt{AST\_PARENT\_FULL\_NAME}.
    \item \texttt{LINE\_NUMBER} -- information about which line the definition is on (may be empty), this information is not crucial for error detection.
    \item \texttt{NAME} -- for \texttt{METHOD} nodes, it contains, like \texttt{FULL\_NAME}, the function name.
    \item \texttt{SIGNATURE} -- contains the function signature, which is potentially useful information. However, since the data types of individual nodes will be included later (including the arguments and return values of all functions) as separate nodes, the signature will be implicitly present in the graph structure, and the \texttt{SIGNATURE} attribute would be redundant.
\end{itemize}

\paragraph{The \texttt{METHOD\_PARAMETER\_IN} Node Set}
For the node set \texttt{METHOD\_PARAMETER\_IN}, only the \texttt{ORDER} attribute will be used (the order among siblings in the AST), which indicates the index/order of the parameter within the function declaration. The other attributes will not be used, namely:
\begin{itemize}
    \item \texttt{CODE} -- contains the parameter name, which is better removed to improve generalization between projects.
    \item \texttt{INDEX} -- always has the same value as \texttt{ORDER} and expresses the same information, so it is unnecessary redundancy.
    \item \texttt{NAME} -- contains the same value as \texttt{CODE} --  will be removed.
    \item \texttt{TYPE\_FULL\_NAME} -- information about the data type, but since data types will be modeled as separate nodes, this information is redundant.
    \item \texttt{IS\_VARIADIC} -- information on whether the parameter is variadic (denoted as \texttt{'...'} in C, e.g., in the \texttt{printf} function). This information will be discarded for future node set merging purposes, but it is still useful information.
\end{itemize}

\paragraph{The \texttt{METHOD\_PARAMETER\_OUT} Node Set}
Node set \texttt{METHOD\_PARAMETER\_OUT} will be completely removed because, for statically typed languages (like LLVM IR), it contains the same information as \texttt{METHOD\_PARAMETER\_IN} and would unnecessarily add redundant data. The edge set \texttt{PARAMETER\_LINK} is also removed because it connects \texttt{METHOD\_PARAMETER\_IN} and \texttt{METHOD\_PARAMETER\_OUT}.

\paragraph{The \texttt{METHOD\_RETURN} Node Set}
In the \texttt{METHOD\_RETURN} node set, only the \texttt{ORDER} attribute will be used, mainly due to the later merging of node sets. The attributes \texttt{CODE}, \texttt{DYNAMIC\_TYPE\_HINT\_FULL\_NAME} (which can be empty), and \texttt{TYPE\_FULL\_NAME} typically contain the same information about the data type, which will be discarded for the aforementioned reasons.

\paragraph{The \texttt{MEMBER} Node Set}
For the \texttt{MEMBER} node set, only the \texttt{ORDER} attribute will be used, indicating the order within the defined structure. Other attributes will not be used:
\begin{itemize}
    \item \texttt{CODE} -- contains the name of the component. The name of the component is not important for distinguishing true positives and false positives; only its type and order matter.
    \item \texttt{NAME} -- contains the same information as \texttt{CODE}.
    \item \texttt{TYPE\_FULL\_NAME} -- types will later be expressed via nodes.
\end{itemize}

\paragraph{The \texttt{TYPE} Node Set}
The \texttt{TYPE} node set contains the attributes \texttt{FULL\_NAME}, \texttt{NAME}, and \texttt{TYPE\_DECL\_FULL\_NAME}, which contain the same information -- the full name of the data type. Therefore, only \texttt{FULL\_NAME} will be retained (although any of them could be used). Based on the name, the data type can be distinguished into multiple categories, such as integer, float, pointer, function signature, etc. More information can be found below in this section. Modeling data types using external nodes greatly simplifies other nodes that (where appropriate) carry their own type information in their attributes, which can then be removed. Overall, graphs will be smaller (in terms of data quantity in attributes) and simpler.

\paragraph{The \texttt{TYPE\_DECL} Node Set}
The \texttt{TYPE\_DECL} node set does not add any new information for LLVM IR compared to the \texttt{TYPE} node set. The node set could be useful for languages with parameterizable types or classes like C++ or Java. Therefore, this node set is completely removed. However, it must be removed carefully because it connects important parts of the graphs -- \texttt{TYPE} nodes of structured types with their \texttt{MEMBER} nodes. More information on this is provided later in this section.

\paragraph{The \texttt{BLOCK} Node Set}
The \texttt{BLOCK} node set does not contain any useful information in its attributes. Its usefulness lies in how it connects other nodes in the AST. For instance, each function has its own \texttt{BLOCK} node that contains all top-level statements. \texttt{BLOCK} nodes are useful, for example, for determining variable scope and also as \textit{latent nodes}\footnote{\textbf{Latent node} -- a node in the graph that does not contain any information itself and serves purely as a connection between other nodes.} for passing information within GNN~\cite{tfgnn-data-prep}. Again, for future node set merging, the \texttt{ORDER} attribute is retained, as well as the \texttt{ARGUMENT\_INDEX} attribute. However, neither carries any useful information.

\paragraph{The \texttt{CALL} Node Set}
In the \texttt{CALL} node set, only the attributes \texttt{ORDER} and \texttt{ARGUMENT\_INDEX} will be retained. If the parent of the \texttt{CALL} node is another \texttt{CALL} node, then \texttt{ARGUMENT\_INDEX} indicates the position among the function call arguments. If the parent is a \texttt{BLOCK} node, \texttt{ARGUMENT\_INDEX} indicates the position among the commands contained in that \texttt{BLOCK} node. However, this information is not particularly important and will be removed later, but detection of this case can only be done by examining the graph, as described later in this section.

\paragraph{The \texttt{FIELD\_IDENTIFIER} Node Set}
LLVM IR supports the array data type~\cite{LLVM-IR}, so the \texttt{FIELD\_IDENTIFIER} node set must be included. Again, the \texttt{ORDER} and \texttt{ARGUMENT\_INDEX} attributes are retained due to the later merging of node sets, although they contain the same value across the dataset. Both values are always \texttt{2}, because access to an array is modeled in ECPGs as a call to the \texttt{getElementPtr} operator, where the \texttt{FIELD\_IDENTIFIER} is always the second argument. The \texttt{CANONICAL\_NAME} attribute, which contains the name of the field, will not be included for reasons similar to those for the \texttt{FULL\_NAME} attribute in the \texttt{METHOD} node set -- the model should not remember samples/projects based on specific names.

\paragraph{The \texttt{IDENTIFIER} Node Set}
In the \texttt{IDENTIFIER} node set, only \texttt{ARGUMENT\_INDEX} and \texttt{ORDER} are retained, which now contain valid values. Other attributes are not included:
\begin{itemize}
    \item \texttt{CODE} -- contains the name of the variable, which is not used for the same reasons as \texttt{CANONICAL\_NAME} in \texttt{FIELD\_IDENTIFIER}.
    \item \texttt{COLUMN\_NUMBER} -- potentially useful information, especially for refining pooling in the GNN head (see Section~\ref{gnn-model}), but this is left for future improvements.
    \item \texttt{LINE\_NUMBER} -- same reason as \texttt{COLUMN\_NUMBER}.
    \item \texttt{NAME} -- contains the same information as \texttt{CODE}.
    \item \texttt{TYPE\_FULL\_NAME} -- types are handled using the \texttt{TYPE} node set.
\end{itemize}
This may raise the question of how to identify \texttt{IDENTIFIER} nodes that refer to the same variable in the graph when their names are discarded. The answer lies in the \texttt{LOCAL} nodes and \texttt{REF} edges that connect nodes representing the same variable. This information is thus represented by the graph structure and not by the node attributes.

\paragraph{The \texttt{LITERAL} Node Set}
In the \texttt{LITERAL} node set, the \texttt{ORDER} and \texttt{ARGUMENT\_INDEX} attributes are retained. Additionally, the \texttt{CODE} attribute, which contains the literal value, is also retained. This value can be an integer, floating point, string, array, structure, or any supported data type in LLVM IR~\cite{LLVM-IR}. Storing this value will require the creation of a~special node set capable of accommodating these different formats (see later in this section). The \texttt{COLUMN\_NUMBER}, \texttt{LINE\_NUMBER}, and \texttt{TYPE\_FULL\_NAME} attributes are not used for the reasons mentioned earlier.

\paragraph{The \texttt{LOCAL} Node Set}
In the \texttt{LOCAL} node set, only the \texttt{ORDER} attribute is used. Other attributes, such as \texttt{CODE}, \texttt{NAME}, and \texttt{TYPE\_FULL\_NAME}, are not used for the previously mentioned reasons. The usefulness of this node set lies mainly in connecting the \texttt{IDENTIFIER} nodes that represent the same variable. The problem is that \texttt{LOCAL} nodes only exist for local variables, not for global ones. LLVM IR can have global variables, but unfortunately, LLVM2CPG and Joern cannot properly encode them in the graph. Consider the read and write operations in Listing~\ref{listing:llvm-global-var}. It is clear which variable is being read/written to -- the \texttt{x}~and \texttt{y} variables. However, if ECPG is generated using LLVM2CPG and Joern, each access to a global variable is preceded by obtaining its address, and then the data is written/read to/from that address -- the variable identifier is not used. Obtaining the address is modeled as a call (a \texttt{CALL} node) to the \texttt{addressOf} operator, which has a single operand that should contain the global variable identifier in this case. But as shown in Listing~\ref{listing:ecpg-global-var}, the operand is of type \texttt{LITERAL} with value \texttt{0} and type \texttt{i32}. Therefore, global variables cannot be distinguished from each other. One possible way would be to use the debug info \texttt{!dbg !40} in the original LLVM IR (see Listing~\ref{listing:llvm-global-var}), which points to the exact location (line and column) in the original C code, and thus the name of the global variable could be extracted from there. However, this encounters the previously mentioned problem -- variables versus macros. Extracting global variables is thus left for future improvements.


\begin{lstlisting}[
    language=none, 
    label={listing:llvm-global-var}, 
    float=t,
    caption={An example of a code in LLVM IR that demonstrates reading and writing to global variables.}
]
 store i32 %5, i32* @x, align 4, !dbg !40    // write x
 %10 = load i32, i32* @x, align 4, !dbg !27  // load x
 store i32 %6, i32* @y, align 4, !dbg !42    // write y
 %7 = load i32, i32* @y, align 4, !dbg !24   // load y 
\end{lstlisting}

\begin{lstlisting}[
    language=none, 
    label={listing:ecpg-global-var}, 
    float=t,
    caption={The simplified ECPG in the CSV format for the LLVM IR code from Listing~\ref{listing:llvm-global-var}, demonstrating reading and writing to global variables.}
]
 // node set CALL
 ID,LABEL,CODE,COLUMN_NUMBER,LINE_NUMBER,METHOD_FULL_NAME,TYPE_FULL_NAME
 35,CALL,&0,16,10,<operator>.addressOf,i32*
 46,CALL,&0,16,12,<operator>.addressOf,i32*
 88,CALL,&0,5,7,<operator>.addressOf,i32*
 96,CALL,&0,5,8,<operator>.addressOf,i32*
 
 // edge set AST
 START_ID,END_ID,TYPE
 35,34,AST
 46,45,AST
 88,87,AST
 96,95,AST
 
 // node set LITERAL
 ID,LABEL,CODE,COLUMN_NUMBER,LINE_NUMBER,TYPE_FULL_NAME
 34,LITERAL,0,16,10,i32
 45,LITERAL,0,16,12,i32
 87,LITERAL,0,5,7,i32
 95,LITERAL,0,5,8,i32
\end{lstlisting}

\paragraph{The \texttt{METHOD\_REF} Node Set}
In the \texttt{METHOD\_REF} node set, only \texttt{ARGUMENT\_INDEX} and \texttt{ORDER} will be retained. The \texttt{COLUMN\_NUMBER} and \texttt{LINE\_NUMBER} attributes will not be retained for the reasons mentioned earlier. The \texttt{CODE} and \texttt{METHOD\_FULL\_NAME} attributes both contain the name of the method that the node represents. However, since \texttt{METHOD\_REF} is connected to the \texttt{METHOD} node via \texttt{REF} edges, these attributes can be discarded.

\paragraph{The \texttt{RETURN} Node Set}
In the \texttt{RETURN} node set, \texttt{ORDER} and \texttt{ARGUMENT\_INDEX} are used, which contain useful values. The location information \texttt{COLUMN\_NUMBER} and \texttt{LINE\_NUMBER} are discarded again.

\paragraph{The \texttt{UNKNOWN} Node Set}
In the \texttt{UNKNOWN} node set, only \texttt{ORDER} and \texttt{ARGUMENT\_INDEX} will be retained, both containing valid values. The \texttt{CODE} attribute may contain clues about what the \texttt{UNKNOWN} node holds -- typically the name of a data type or a signature. The \texttt{UNKNOWN} node itself does not provide any useful information, but it is typically deeply embedded in the graph and connects surrounding nodes. Because there are significantly fewer \texttt{UNKNOWN} nodes compared to other nodes, it will be retained.

\paragraph{Edge Sets}
The edge set \texttt{ALIAS\_OF} is discarded because aliases are resolved and the original type names are used during compilation and generating LLVM bitcode. Information about aliases is present in the graph through debugging information (see Section~\ref{bitcode-generation}) in the form of \texttt{TYPE} nodes. However, these nodes are later removed as part of graph optimizations (see below in this section).

The edge sets \texttt{AST}, \texttt{CFG}, and \texttt{CDG} are, of course, retained because they form the core of the CPG.

The edge set \texttt{ARGUMENT} is retained because it connects \texttt{CALL} nodes to their arguments.

The \texttt{CALL} edge set connects \texttt{CALL} nodes to their corresponding \texttt{METHOD} nodes, thereby adding a call graph to the CPG. The AST itself is created for each function, but the trees are not interconnected, preventing message propagation during GNN computation. The \texttt{CFG} edges, along with \texttt{CALL} edges, connect these individual ASTs at semantically appropriate places.

The edge sets \texttt{DOMINATE} and \texttt{POST\_DOMINATE} form dominator and post-dominator trees~\cite{princeton-dominators}, which provide useful information but essentially express certain simple properties of the CFG more explicitly. Additionally, there are too many of these edges, so they will not be used. However, it would be beneficial to experiment with them in future work.

The \texttt{REACHING\_DEF} edge set will not be used because there are too many of these edges in each graph. However, this is another very useful edge set that is worth experimenting with in future work.

The \texttt{CONTAINS} edge set will not be used because it also represents a relatively large number of additional edges. Furthermore, the information about which method a node belongs to can easily be obtained from the AST.

The \texttt{EVAL\_TYPE} edge set is, of course, used to connect nodes with their types.

The \texttt{REF} edge set is also retained to link identifiers to the local variable they identify. It also connects \texttt{METHOD\_REF} nodes and \texttt{METHOD} nodes.

\paragraph{Mandatory Attributes}
All node sets also contain the \texttt{ID} and \texttt{LABEL} attributes, which are not mentioned in the documentation. There are more undocumented attributes, such as \texttt{CLOSURE\_BINDING\_ID} in the \texttt{LOCAL} node set. However, since none of them are used, they are not mentioned in the text. A~complete list of attributes for node sets in Graph D2A can be found in Table~\ref{tab:attributes1}. The \texttt{ID} attribute identifies a node within each Graph D2A sample, and the \texttt{LABEL} attribute contains the name of the node set. Both of these attributes are used, although the \texttt{ID} is more for implementation reasons, and the original \texttt{ID} is not present in the output TFRecords files, which instead use IDs from the TFGNN library (see Section~\ref{implementation-feature-engineering}). Each edge set (except \texttt{REACHING\_DEF}, which contains an additional attribute) contains the following three attributes:
\begin{itemize}
    \item \texttt{START\_ID} -- the source node of the directed edge.
    \item \texttt{END\_ID} -- the target node of the directed edge.
    \item \texttt{TYPE} -- the name of the edge set.
\end{itemize}


\paragraph{Control Structures in Raw ECPGs}
The generated ECPGs do not contain the node sets \texttt{CONTROL\_STRUCTURE}, \texttt{JUMP\_LABEL}, or \texttt{JUMP\_TARGET}, even though the original C source code uses them. The reason lies partly in the conversion to LLVM IR and partly in generating the CPGs. During the compilation to LLVM IR, all control structures (\texttt{if}, \texttt{for}, \texttt{while}, etc.) are simplified to jumps. Consider a~simple \texttt{if} statement in C in Listing~\ref{listing:if-c}. The same code in LLVM IR is shown in Listing~\ref{listing:if-llvm}, where label \texttt{6:} represents the true branch and label \texttt{7:} the false branch of the original code. The tools LLVM2CPG and Joern did not generate \texttt{JUMP\_LABEL} or \texttt{JUMP\_TARGET} even though they are present. However, program branching information (which is crucial) can still be extracted from the CPG, specifically using \texttt{CFG} and \texttt{CDG} edges.

Consider a partial ECPG (only \texttt{CFG} and \texttt{CDG} edges and without the \texttt{ret} statement) in Figure~\ref{figure:if}, representing the LLVM IR code from Listing~\ref{listing:if-llvm}. If a node branches \texttt{CFG} edges (node \texttt{30}, representing the value assignment to \texttt{\%5}), the program flow branches at that node. The possible \texttt{CFG} paths are branches in the original code. These branches are connected to the condition that determines the program flow branching via \texttt{CDG} edges (from node \texttt{30}). 

From the graph, one can distinguish the true branch (assigning \texttt{1} to \texttt{\%1}) from the false branch (assigning \texttt{2} to \texttt{\%1}) because the true branch has \textbf{lower} \texttt{ORDER} value for the last node (node \texttt{34}) before the paths merge (node \texttt{40}) than the node from the false branch (node \texttt{38}). This is because the true branch is always first due to the compilation. If the LLVM IR is manually modified and the branches are rearranged, it will no longer be possible to distinguish the true and false branches from the graph. This fact further demonstrates that utilizing node sets and edge sets constructing the CPG is necessary. It also shows the importance of \texttt{ORDER} attributes in AST nodes.



\begin{lstlisting}[
    language=none, 
    label={listing:if-c}, 
    float=t,
    caption={A simple \texttt{if} statement in the C language.}
]
 if (z)
     return 1;
 else
     return 2;
\end{lstlisting}

\begin{lstlisting}[
    language=none, 
    label={listing:if-llvm}, 
    float=t,
    caption={The code from Listing~\ref{listing:if-c}, but converted to LLVM IR.}
]
   %5 = icmp ne i32 %4, 0, !dbg !16
   br i1 %5, label %6, label %7, !dbg !18

 6:
   store i32 1, i32* %1, align 4, !dbg !19
   br label %8, !dbg !19

 7:
   store i32 2, i32* %1, align 4, !dbg !20
   br label %8, !dbg !20

 8:
   %9 = load i32, i32* %1, align 4, !dbg !21
   ret i32 %9, !dbg !21
\end{lstlisting}

\begin{figure}[t]
	\centering
	\includegraphics[width=1\textwidth]{figures/ecpg-if.pdf}
	\caption{A~partial (only \texttt{CFG} and \texttt{CDG} edges and without the \texttt{ret} statement) ECPG demonstrating branching in LLVM IR from Listing~\ref{listing:if-llvm}.}
	\label{figure:if}
\end{figure}
    
The node sets and their attributes present in Graph D2A are listed in Table~\ref{tab:attributes1}. Attributes of individual node sets that were considered useful and selected during feature selection are also marked in the table.

\subsubsection{Graph Structure Optimization}
Before using individual graphs as inputs to GNNs, it is necessary to remove as many unnecessary and redundant nodes, edges, and attributes from the graphs as possible. The previous text dealt with the removal of information at the level of entire node sets, edge sets, and attributes. However, even within a single node set, there are nodes that do not add any useful information and can be removed, effectively reducing the graph and easing the learning process of the GNNs. Furthermore, adjustments are required to ensure certain properties arising from the use of GNNs and the TFGNN library.

To propagate information correctly within the graph during GNN computation, the graph must consist of only one WCC (see Section~\ref{gnn}). For GNNs where message passing follows the direction of the edges, it is also necessary to ensure the correct orientation of the edges. Using TFGNN requires creating a so-called TFGNN schema~\cite{tfgnn-schema}, which describes the graph structure: node sets, edge sets, their attributes, and attribute data types. In the TFGNN schema, all nodes in a node set must have the same attributes (similarly for edges) -- this already applies to raw ECPGs. Furthermore, each edge set must have a fixed source node set and target node set -- unfortunately, this is not the case with raw ECPGs. For example, \texttt{AST} edges connect \texttt{BLOCK}, \texttt{CALL}, \texttt{LOCAL}, \texttt{IDENTIFIER}, and other types of nodes.

The following transformations are described in this section:
\begin{enumerate}
    \item removal of unnecessary and redundant information,
    \item creating graphs with only one WCC,
    \item ensuring fixed source and target node sets for all edge sets.
\end{enumerate}

The removal of unnecessary and redundant information and ensuring a single WCC are closely related because removing some unnecessary nodes also removes unnecessary separate WCCs. This step consists of the following sub-steps, which must be performed in the given order:
\begin{enumerate}
    \item removal of invalid nodes within the AST,
    \item removal of WCCs (only in the AST) consisting only of \texttt{BLOCK} nodes,
    \item removal of AST leaf \texttt{BLOCK} nodes,
    \item filtering out unnecessary \texttt{ARGUMENT} edges,
    \item removal of AST children of external functions,
    \item removal of unused functions,
    \item filtering out unnecessary \texttt{EVAL\_TYPE} edges,
    \item removal of the \texttt{TYPE\_DECL} node set,
    \item filtering out unused \texttt{TYPE} nodes.
\end{enumerate}

\paragraph{Removal of Invalid Nodes}
The removal of invalid nodes occurs only within the AST -- considering only \texttt{AST} edges and ignoring others. Invalid nodes are considered all those that were removed during feature selection and have an \texttt{AST} edge leading to or from them (e.g., a standalone \texttt{META\_DATA} node is not part of the AST). If these nodes were simply removed (along with their edges), the AST they belong to would be split. Therefore, they need to be removed based on their position in the AST as follows:
\begin{enumerate}
    \item Leaf -- the invalid node, along with its edges, can be simply removed.
    \item Root -- the invalid node is replaced with a valid \texttt{BLOCK} node to ensure that the AST has exactly one valid root.
    \item Inner node -- the AST children of the invalid node are connected to the AST parent of the invalid node.
\end{enumerate}

Since no node types inheriting from \texttt{CFG\_NODE} are removed during feature selection, it is not necessary to connect \texttt{CFG} edges in the case of inner nodes because there should not be any, according to the documentation. Similarly, \texttt{CDG} edges do not make sense for any invalid nodes. An exception is the removed \texttt{METHOD\_PARAMETER\_OUT} node set, which inherits from \texttt{CFG\_NODE} (see Figure~\ref{figure:node-set-hierarchy}), but no \texttt{CFG} or \texttt{PDG} edges leading to or from these nodes were found in Graph D2A. Therefore, only \texttt{AST} edges need to be reconnected. 

After removing the invalid nodes, the graph is composed of one or more ASTs -- one AST for each function.

\paragraph{Removal of \texttt{BLOCK} WCCs}
After removing the invalid nodes, it is necessary to remove WCCs entirely composed of \texttt{BLOCK} nodes. These are latent nodes, and WCCs entirely composed of latent nodes contain no useful information. Such WCCs can arise, for example, from ASTs entirely composed of invalid nodes, as the previous algorithm would convert the invalid root into a valid node and remove the other invalid nodes. This would result in an AST with only a \texttt{BLOCK} node. The case where a WCC consists only of \texttt{BLOCK} nodes is relatively rare.

\paragraph{Removal of Leaf \texttt{BLOCK} Nodes}
All leaf \texttt{BLOCK} nodes are also removed from all ASTs. The reason is that \texttt{BLOCK} nodes are used to cluster other nodes -- if they have no AST children, they are unnecessary.

\paragraph{Filtering of \texttt{ARGUMENT} Edges}
Next, the \texttt{ARGUMENT} edge set is filtered to keep only edges that have a \texttt{CALL} node as their source. In other words, the \texttt{ARGUMENT} edges between \texttt{RETURN} nodes and the expressions they return (nodes inheriting from \texttt{EXPRESSION}) will be removed. This adjustment is made to move the \texttt{ARGUMENT\_INDEX} into the \texttt{ARGUMENT} edges later. For \texttt{ARGUMENT} edges from \texttt{RETURN} nodes, it does not make sense to talk about the index of arguments (there is always only one for LLVM IR~\cite{LLVM-IR}), so these edges are removed.

\paragraph{Removal of \texttt{AST} Children of External Functions}
All AST children of \texttt{METHOD} nodes representing external functions (having the value \texttt{True} in the \texttt{IS\_EXTERNAL} attribute) are then removed. The removed nodes for each function are one \texttt{METHOD\_RETURN} and $N$ \texttt{METHOD\_PARAMETER\_IN}, where $N$ is the number of function parameters. For external functions, this information is not useful because the information about input and output parameters is already present when calling the function (\texttt{CALL} node and its AST children). \texttt{METHOD\_PARAMETER\_IN} serves only as a link between arguments from the call site and the use of parameters within the function body for non-external functions. Similarly, \texttt{METHOD\_RETURN} serves as an abstraction of all \texttt{RETURN} nodes (typically only a single \texttt{ret} statement in each function, see Listing~\ref{listing:if-llvm}) in the function body -- it is connected to them. 

Another reason for removing AST children of external functions is that for many operators, \texttt{EVAL\_TYPE} edges led from the \texttt{METHOD\_PARAMETER\_IN} and \texttt{METHOD\_RETURN} nodes to a \texttt{TYPE} node, whose type was \texttt{ANY} -- which does not provide any additional information. By removing these AST children, these \texttt{EVAL\_TYPE} edges are also removed, leading to the removal of the \texttt{TYPE} node with the \texttt{ANY} value. Thus, the data type \texttt{ANY} will not need to be considered.

\paragraph{Removal of Unused Functions}
If an unused function is found -- in other words, if there is a \texttt{METHOD} node in the graph without incoming \texttt{CALL} edges -- it is also removed. This can happen because each graph implicitly includes at least the function \texttt{llvm.dbg.declare}, which is part of the debug information~\cite{LLVM-IR}. There may also be some unused operators or other default global functions.

\paragraph{Filtering of \texttt{EVAL\_TYPE} Edges}
The next step is the removal of \texttt{EVAL\_TYPE} edges leading from the \texttt{METHOD}, \texttt{BLOCK}, and \texttt{METHOD\_REF} nodes. All these node sets contain information about the data type, which do not need to be retained. For \texttt{BLOCK} nodes, this represents the return type of the entire block -- in some languages (typically functional), this information is useful, but in C/LLVM IR, this value is irrelevant as it merely indicates the data type of the last statement in the given block. For \texttt{METHOD} and \texttt{METHOD\_REF}, it represents the signature, which (as previously mentioned) is expressed through the data types of the function's inputs and outputs.

\paragraph{Removal of the \texttt{TYPE\_DECL} Node Set}
In feature selection, it was mentioned that the \texttt{TYPE\_DECL} node set needs to be removed in a~specific way because it connects structures and their elements. If a \texttt{TYPE} node is a~structure for which its \texttt{MEMBER} nodes are known (if only a pointer to it is used, the elements may not be known), then this \texttt{TYPE} node is connected by a \texttt{REF} edge to a \texttt{TYPE\_DECL} node, from which \texttt{AST} edges lead to individual \texttt{MEMBER} nodes, as shown in Figure~\ref{figure:struct}. Each \texttt{TYPE\_DECL} is then removed such that if it has any AST \texttt{MEMBER} children, these \texttt{MEMBER} (target node) nodes are connected to the \texttt{TYPE} node (source node) using new \texttt{CONSISTS\_OF} edges, and the \texttt{TYPE\_DECL} is removed along with all edges leading to or from it. If the \texttt{TYPE\_DECL} has no \texttt{AST} \texttt{MEMBER} children, it can simply be removed.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.85\textwidth]{figures/ecpg-struct.pdf}
	\caption{The figure shows the connection of \texttt{TYPE\_DECL} nodes in ECPG when modeling structured data types.}
	\label{figure:struct}
\end{figure}

\paragraph{Filtering of Unused \texttt{TYPE} Nodes}
Unused \texttt{TYPE} nodes are then removed. An unused \texttt{TYPE} node has no incoming \texttt{EVAL\_TYPE} edges. This removal process is iterative -- it iterates until there are no \texttt{TYPE} nodes without incoming edges. This ensures that nested or recursive structures are also removed. One might ask what happens if a \texttt{TYPE} node has a self-loop (loop) -- a recursive structure (or two or more mutually recursive structures). The answer lies in the representation of such structures -- a recursive structure cannot contain itself, only a pointer to itself (similarly for mutual recursion). Self-loops and loops are not possible for \texttt{TYPE} nodes because \texttt{MyStruct} and \texttt{MyStruct*} are two different types, represented by two \texttt{TYPE} nodes. For this reason, the information about the individual elements of \texttt{MyStruct} does not need to be present in the graph if only its pointer \texttt{MyStruct*} is used and its elements are not accessed -- information about the elements would be redundant.

As a result of these modifications, the graph is composed of a single WCC. When using bi-directional GNNs, information can be propagated between all nodes. For directional GNNs , it is still necessary to correctly orient the edge sets, as described below.

\paragraph{The Edge Set Condition}
The condition set by the TFGNN schema that each edge set must have a fixed source node set and target node set is currently not met. Examples include the basic \texttt{AST} edges that start and end in different node sets. This condition can be met in two basic ways:
\begin{enumerate}
    \item Splitting all edge sets that do not meet the conditions into smaller edge sets to meet the condition. The problem with this solution is that the number of possible sub edge sets is up to $|N|^2$ where $N$ is the set of node sets that can appear on either side of any edge in the given edge set -- because it is necessary to cover each combination of node sets. Of course, some combinations are not possible, such as a \texttt{METHOD} node not having direct AST children of the type \texttt{CALL}, so practically, there are fewer combinations. Adding the fact that even \texttt{CFG} and \texttt{PDG} also connect a large number of node sets, like \texttt{AST}, this number increases significantly. This results in dealing with tens to hundreds of sub edge sets. The fact that it would be necessary to define them manually in the TFGNN schema, and the fact that the more edge sets there are, the more complex the GNN model, shows that this number of sub edge sets is unsustainable. The principle is demonstrated in Figure~\ref{figure:tfgnn-condition}.
    \item Some existing node sets can be merged so that all edge sets meet the required condition. This method simplifies the TFGNN schema (there will be fewer node sets) but requires that all node sets that will be merged into a super node set have the same attributes. For this reason, potentially useful attributes were discarded and some unnecessary attributes were retained during the feature selection phase. The principle is also demonstrated in Figure~\ref{figure:tfgnn-condition}.
\end{enumerate}


\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/tfgnn-condition.pdf}
	\caption{The figure demonstrates two basic ways to meet one of the conditions set by the TFGNN schema -- that each edge set has exactly one source and target node set.}
	\label{figure:tfgnn-condition}
\end{figure}

\paragraph{Merging of Node Sets}
The condition is thus ensured in this thesis by merging certain node sets. Although efforts were made in the feature selection phase to ensure that all node sets to be merged in the future have the same attributes, some attributes are too important to discard. The problem of different attribute sets when merging node sets can be solved in two extreme approaches:
\begin{enumerate}
    \item \textit{Sparse nodes} -- create a set of all attributes contained in the merged node sets. The output super node set will have all these attributes. If an attribute does not make sense for a particular node, it is simply replaced with an invalid value. This principle is simple but creates sparse nodes and effectively increases the graph's data size. The principle of merging node sets using sparse nodes is demonstrated in Figure~\ref{figure:sparse-nodes}.
    
    \item Latent nodes -- the exact opposite approach is extracting each node's attributes into a special \textit{data node} connected to the original node by a special edge set, according to the original node set. This again effectively unifies the node format. This principle is somewhat more complex because it requires the creation of new edge sets connecting latent nodes with their data nodes. The number of these new edge sets is $|N|$, where $N$ is the set of merged original node sets. However, the output is a graph that is smaller in data size but larger in the number of nodes. Another advantage is that data nodes will (when using oriented GNNs) constantly send information about the original data, as their values will not be overwritten during GNN computation because they have no incoming edges. The principle of merging node sets using latent nodes is demonstrated in Figure~\ref{figure:latent-nodes}.
\end{enumerate}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.93\textwidth]{figures/sparse-nodes.pdf}
	\caption{The figure demonstrates merging node sets using sparse nodes -- the super node set contains attributes of all original node sets. The original graph is from Figure~\ref{figure:tfgnn-condition} and is supplemented with node set attributes.}
	\label{figure:sparse-nodes}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.93\textwidth]{figures/latent-nodes.pdf}
	\caption{The figure demonstrates merging node sets using latent nodes -- the attributes of the original node sets are extracted into special data node sets. The original graph is from Figure~\ref{figure:tfgnn-condition} and is supplemented with node set attributes.}
	\label{figure:latent-nodes}
\end{figure}

Both approaches have their advantages and disadvantages. In this work, the \textit{mixed nodes} approach is used -- a combination of the best properties of both approaches. For attributes that are common to all/most original node sets (e.g., \texttt{ORDER} for all nodes inheriting from \texttt{AST\_NODE}), the sparse node approach is used. On the other hand, for attributes that are specific to certain node sets (e.g., \texttt{CODE} for \texttt{LITERAL}, containing the literal value), the latent node approach is used. Here, however, the latent node is not empty but contains previously defined sparse attributes. By combining these methods, the graphs are small in both node count and attribute count, while only requiring the definition of a few new edge and node sets to connect data nodes with specific attributes. Table~\ref{tab:merged-node-sets} shows all selected node sets and attributes in the feature selection phase and their combination into new node sets.

\paragraph{The New \texttt{AST\_NODE} Node Set}
The new node set \texttt{AST\_NODE} consists of original node sets that are connected by \texttt{AST} edges. All these node sets inherit from the template node set \texttt{AST\_NODE} (hence the same name), see Figure~\ref{figure:node-set-hierarchy}. These original node sets thus represent the code as such -- the AST. Thanks to the new \texttt{AST\_NODE} set, it is possible to keep the \texttt{AST}, \texttt{CFG}, \texttt{CDG}, \texttt{REF}, and \texttt{ARGUMENT} edge sets intact -- their target and source node sets will be the new \texttt{AST\_NODE} node set. The \texttt{AST\_NODE} node set contains sparse attributes:
\begin{itemize}
    \item \texttt{LABEL} -- indicates the original node set (e.g., \texttt{BLOCK}, \texttt{LOCAL}, \texttt{METHOD}, etc.); each node contains it, so there is no need to fill it with invalid values.
    \item \texttt{ORDER} -- is also present in all nodes.
    \item \texttt{ARGUMENT\_INDEX} -- for the original node sets \texttt{METHOD}, \texttt{LOCAL}, \texttt{METHOD\_RETURN}, and \texttt{METHOD\_PARAMETER\_IN}, this information needs to be filled with zeros, see Table~\ref{tab:merged-node-sets}. However, \texttt{ARGUMENT\_INDEX} will be completely removed later, as explained below.
\end{itemize}

\paragraph{New Data Node Sets}
From Table~\ref{tab:attributes1}, it is evident that the original node sets \texttt{METHOD} and \texttt{LITERAL} contain special attributes that require the creation of data nodes. For the \texttt{METHOD} node set, the attributes \texttt{FULL\_NAME} and \texttt{IS\_EXTERNAL} need to be separated. The new data node set for the \texttt{METHOD} node set is named \texttt{METHOD\_INFO} and is connected using the \texttt{METHOD\_INFO\_LINK} edge set, where the source is \texttt{METHOD\_INFO} and the target is the new \texttt{AST\_NODE} node set (original \texttt{METHOD} nodes). Similarly, for \texttt{LITERAL}, the \texttt{CODE} attribute needs to be separated into the \texttt{LITERAL\_VALUE} node set and connected using the \texttt{LITERAL\_VALUE\_LINK} edge set, with the source being \texttt{LITERAL\_VALUE} and the target being the new \texttt{AST\_NODE} (original \texttt{LITERAL} nodes).

\paragraph{Retained Node Sets}
Node sets \texttt{MEMBER} and \texttt{TYPE} are retained. However, due to the creation of the new \texttt{AST\_NODE} from which \texttt{EVAL\_TYPE} edges originate, and the retention of \texttt{MEMBER} -- which also has \texttt{EVAL\_TYPE} edges -- it is necessary to split the \texttt{EVAL\_TYPE} edge set. The name \texttt{EVAL\_TYPE} is retained for edges originating from \texttt{AST\_NODE} and ending in \texttt{TYPE}. The new edge set \texttt{EVAL\_MEMBER\_TYPE} represents the remaining edges from \texttt{MEMBER} to \texttt{TYPE}. The reason why \texttt{MEMBER} and \texttt{TYPE} are kept in separate node sets is that they do not represent the code itself (description of computation) but provide additional information about types -- thus, the node sets are logically separated.

\paragraph{The New \texttt{ARGUMENT\_INDEX} Edge Attribute}
The penultimate adjustment is the transfer of the \texttt{ARGUMENT\_INDEX} attribute from the new node set \texttt{AST\_NODE} to the \texttt{ARGUMENT} edge set. However, this is not done for all nodes that have \texttt{ARGUMENT\_INDEX}, but only for those that are the target node for some \texttt{ARGUMENT} edge. At this stage, only \texttt{ARGUMENT} edges originating from the original \texttt{CALL} node set remain. The TFGNN schema allows attributes for both nodes and edges, and therefore, this transformation saves a considerable amount of data and reduces the complexity of the \texttt{AST\_NODE} nodes.

\paragraph{Orientation of Edges}
The final step is to correctly orient the edges in case oriented GNNs are used. The orientation of edge sets at this stage is shown in Table~\ref{tab:edge-sets-orientation}, where the edge sets that need to be reversed are highlighted in red. The reasons are:
\begin{itemize}
    \item \texttt{ARGUMENT} -- information about arguments will flow towards the \texttt{CALL} node, from which it will then propagate to the respective function.
    \item \texttt{EVAL\_TYPE} -- information about types will propagate to the nodes where it will be used -- it makes no sense to propagate information from \texttt{AST\_NODE} to be concentrated in \texttt{TYPE} nodes.
    \item \texttt{EVAL\_MEMBER\_TYPE} -- analogous to \texttt{EVAL\_TYPE}.
    \item \texttt{CONSISTS\_OF} -- information about individual \texttt{MEMBER} nodes will flow into the structure so that the structure node contains information about its members.
    \item \texttt{REF} -- here, information will propagate from \texttt{LOCAL} nodes to identifiers so that they know it is the same variable (different \texttt{LOCAL} nodes in the same function can be distinguished using the \texttt{ORDER} attribute). For \texttt{METHOD\_REF}, information about the given method will flow into that node -- the function is not called here, but it is desirable to attach information to it (reversed compared to \texttt{CALL} edges).
\end{itemize}

\begin{table}
    \centering
    \caption{The table contains source and target node sets for each used edge set (after feature selection). Edge sets highlighted in red have an incorrect orientation and will be reversed.}
    \vskip6pt
	\begin{tabular}{
        !{\vrule width 1pt}l!{\vrule width 1pt}l|l!{\vrule width 1pt}}
        \noalign{\hrule height 1pt}
        Edge Set & Source Node Set & Target Node Set \\
        \noalign{\hrule height 1pt}

        METHOD\_INFO\_LINK & METHOD\_INFO & AST\_NODE \\ \hline
        \rowcolor{lightred}
        EVAL\_MEMBER\_TYPE & TYPE & MEMBER \\ \hline
        \rowcolor{lightred}
        CONSISTS\_OF & MEMBER & TYPE \\ \hline
        AST & AST\_NODE & AST\_NODE \\ \hline
        LITERAL\_VALUE\_LINK & LITERAL\_VALUE & AST\_NODE \\ \hline
        \rowcolor{lightred}
        ARGUMENT & AST\_NODE & AST\_NODE \\ \hline
        CALL & AST\_NODE & AST\_NODE \\ \hline
        CFG & AST\_NODE & AST\_NODE \\ \hline
        CDG & AST\_NODE & AST\_NODE \\ \hline
        \rowcolor{lightred}
        EVAL\_TYPE & TYPE & AST\_NODE \\ \hline
        \rowcolor{lightred}
        REF & AST\_NODE & AST\_NODE \\ \hline
        
        \noalign{\hrule height 1pt}
    \end{tabular}
    \label{tab:edge-sets-orientation}
\end{table}

The orientation of the other edge sets is preserved for the following reasons:
\begin{itemize}
    \item \texttt{METHOD\_INFO\_LINK} and \texttt{LITERAL\_VALUE\_LINK} -- they originate from data nodes to latent nodes, allowing data information to spread further into the graph.
    \item \texttt{CFG} -- their direction reflects the program's control flow and the chronological order of node traversal, where the node order plays a crucial role in the manifestation of errors.
    \item \texttt{AST} -- reversing the edges would mean that it is no longer a tree, but this is not a~problem. From a message-passing perspective, the tree has an interesting property: information is copied down the tree -- parents send the same message to their children, and children have only a single parent. If the \texttt{AST} edges were reversed, information would flow to the original root node, and information from the children would need to be combined in some way (the term "\textit{pooling}" is used, see Section~\ref{gnn-model}), leading to an irreversible loss of information. However, it would be possible to read the final state from the root, where information from the entire graph would accumulate -- but the graph would have to be shallow enough for information to travel from all leaf nodes to the root (because, as mentioned in Section~\ref{gnn-model}, the number of message passes is a~hyperparameter of the model). Pooling still occurs in GNNs because nodes in ECPGs can have multiple incoming and outgoing edges. Thus, it might be interesting trying the reverse direction of the \texttt{AST} edges in the future.
    \item \texttt{CALL} -- similar reason as \texttt{CFG} -- it is a natural control flow of the code.
    \item \texttt{CDG} -- edges originate from nodes representing conditions to nodes affected by the condition. Thus, it is again in the correct chronological order.
\end{itemize}


\subsubsection{Atribute Transformation}
Another property that graphs must satisfy according to the TFGNN schema is that an attribute (from now on, referred to as a feature) has a fixed type~\cite{tfgnn-schema}. However, this is not yet the case; for example, the feature \texttt{CODE} in the \texttt{LITERAL} node can contain values of all data types in LLVM IR. Although all values can be considered strings in CSV, and TFGNN supports features of type \texttt{DT\_STRING}~\cite{tfgnn-schema}, it would be more challenging for the model to extract useful information from such complex features. To facilitate training, some complex features are decomposed into multiple simpler ones~\cite{feature-splitting}.

\paragraph{Features of the \texttt{AST\_NODE} Node Set}
The node set \texttt{AST\_NODE} has a feature \texttt{ORDER}, which is a simple integer type. The feature \texttt{LABEL} is also a simple type, with categorical values representing the names of the original node sets. To prevent the model from having to take a string as input, the feature label values are mapped to a simple integer type as follows: \texttt{UNKNOWN: 0}, \texttt{METHOD: 1}, ... ,\texttt{RETURN: 11}. The model does not need the \texttt{LABEL} feature in string format; it just needs to distinguish between different types, and the simplest representation is an integer.

\paragraph{Features of the \texttt{MEMBER} Node Set}
The node set \texttt{MEMBER} has only one feature, \texttt{ORDER}, which does not require any modification.

\paragraph{Features of the \texttt{METHOD\_INFO} Node Set}
The node set \texttt{METHOD\_INFO} contains the flag feature \texttt{IS\_EXTERNAL}, which contains values \texttt{True} and \texttt{False} since it is a flag. These boolean values are converted to integer values \texttt{1} and \texttt{0} for simplicity. The second feature is \texttt{FULL\_NAME}, which contains the name of the function. Since LLVM IR operators are modeled as functions in ECPGs, \texttt{FULL\_NAME} can include the prefix \texttt{<operator>.} followed by the operator's name, such as \texttt{notEquals}, \texttt{xor}, etc. Since operators are used much more frequently than functions and are also limited in number, it makes sense to convert them into their own categorical feature, \texttt{OPERATOR}. This feature contains the numerical designation of the operator if detected from the \texttt{FULL\_NAME} feature. If it is not an operator, the entire function name should be remembered. Here, there are several options for modeling the remaining values:
\begin{itemize}
    \item Keep the name as a string -- the model will need to contain, for example, some kind of RNN layer.
    \item Use word2vec~\cite{mikolov2013efficient} or a similar model that can encode a word into a vector while preserving its meaning.
    \item Use trainable embedded tables~\cite{tfgnn-gnn-modeling}.
    \item Hash the name.
\end{itemize}

In this work, the simplest and fastest approach, hashing, is used. The function name is hashed into 24 bits (the reason for 24 bits is explained below). This approach discards all information about the original name but allows the model to remember the occurrence of specific functions -- if a function frequently appears in the true positive class, it is likely associated with it and can serve as part of a learned pattern. Hashing was chosen because, compared to other methods, it is by far the fastest, and generating Graph D2A (and applying feature engineering) is already computationally expensive, as described in Chapter~\ref{implementation}. However, future work should include experiments with other string encoding methods.

\paragraph{Features of the \texttt{TYPE} Node Set}
For the node set \texttt{TYPE}, it is again necessary to decompose the feature \texttt{FULL\_NAME} into simpler features that can be better processed by the model. \texttt{FULL\_NAME} contains the name of the represented data type. LLVM IR supports a~number of data types~\cite{LLVM-IR} (here are the formats as they appear in the \texttt{FULL\_NAME} feature):
\begin{itemize}
    \item Pointer -- the suffix contains one or more \texttt{*} characters, e.g., \texttt{i32*}, \texttt{FILE*}, etc.
    \item Array -- the format is \texttt{[ LEN x TYPE ]}, e.g., \texttt{[114 x i8]}, \texttt{[114 x [114 x i8]]}, etc.
    \item Integer -- the format is \texttt{iN}, where \texttt{N} is an integer > 1 indicating the size of the type in bits, e.g., \texttt{i1} (boolean), \texttt{i32}, \texttt{i128}, etc.
    \item Floating point -- one of \texttt{half}, \texttt{float}, \texttt{double}, or \texttt{fp128}.
    \item Structs -- the format is \texttt{\{ TYPE1, TYPE2, ... \}}, e.g., \texttt{\{ i32, i32 \}}, \texttt{\{ i1, float, \{ i32, i32 \} \}}, etc.
    \item Function signature -- the format is \texttt{TYPE (TYPE1, ...)}, e.g., \texttt{i1 (i1, i8*)}, etc.
    \item Void -- represents an empty value, denoted as \texttt{void}.
    \item Named type -- everything else, e.g., \texttt{ngx\_radix\_tree\_t}, \texttt{FILE}, etc.
\end{itemize}

In the previous section, it was described that \texttt{TYPE} nodes with signatures were removed, so they do not need to be considered. Similarly, \texttt{TYPE} nodes with the value \texttt{ANY} were removed -- this is not an LLVM IR type but a value inserted by the Joern tool. The other data type names need to be appropriately represented using simpler data types. Thus, the feature \texttt{FULL\_NAME} is decomposed into the following primitive features:
\begin{itemize}
    \item \texttt{PTR} -- if the type is a pointer, this stores the pointer depth -- the number of trailing \texttt{*}~symbols.
    \item \texttt{LEN} -- if the type is an array, this stores its length (only the outer-most array is considered).
    \item \texttt{INT} -- if the type is an integer, this stores \texttt{N}, the number of bits.
    \item \texttt{FP} -- if the type is a floating point, this stores a categorical numerical designation of the given type.
    \item \texttt{HASH} -- if the type is a struct or a named type, this stores the 24-bit hash of its name. In the case of an array, it stores the 24-bit hash of the element type name (if it is not a primitive type, see below).
\end{itemize}

The individual features are set as follows and \textbf{in exactly this order}:
\begin{enumerate}
    \item All features are initialized to 0.
    \item \texttt{PTR} is set (it can also be 0), the trailing \texttt{*} are removed, and \textbf{processing continues}.
    \item \texttt{LEN} is set (it can also be 0), and if \texttt{LEN} > 0, the outer-most array is removed, leaving only the type of the elements, and \textbf{processing continues}.
    \item If the type is an integer, \texttt{INT} is set, and processing ends.
    \item If the type is a floating point or void, \texttt{FP} is set (0 in the case of void), and processing ends.
    \item The remaining type is an array, struct, or named type -- \texttt{HASH} is set (from the remaining name), and processing ends.
\end{enumerate}

From this, it follows that for the void type, all values are equal to 0, which semantically makes sense because it indicates the absence of a value.

\paragraph{Features of the \texttt{LITERAL\_VALUE} Node Set}
For the node set \texttt{LITERAL\_VALUE}, the feature \texttt{CODE}, which contains the literal value, needs to be decomposed. It can take on all types described above, so it must be decomposed while preserving the highest possible accuracy. The primitive features will be:
\begin{enumerate}
    \item \texttt{INT} -- if the literal is an integer, this is its value.
    \item \texttt{FP\_MANTISSA} and \texttt{FP\_EXPONENT} -- if the literal is a floating point, this stores its mantissa and exponent, respectively.
    \item \texttt{INVALID\_POINTER} -- a flag, if the type is a pointer and contains the special value \texttt{nullptr}.
    \item \texttt{ZERO\_INITIALIZED} -- a flag, if the special value \texttt{zero initialized} is present.
    \item \texttt{UNDEF} -- a flag, if the special value \texttt{undef} is present.
    \item \texttt{HASH} -- the hashed value of arrays, structs, named types, and function pointer values (in this case, their code) into 24 bits.
\end{enumerate}

It is not necessary to store detailed information about the literal type here because the data node \texttt{LITERAL\_VALUE} is directly connected to the latent node \texttt{LITERAL} (\texttt{AST\_NODE}), which is connected to its \texttt{TYPE} node.

\paragraph{Normalization of Features}
All features are further normalized. Normalization is a~commonly used technique in machine learning that can accelerate learning and improve model performance~\cite{norm1, norm2}, especially for datasets where features have different ranges. In this thesis, simple \textit{MinMax normalization} to the interval $<0,1>$ is used. The advantage of MinMax is that it preserves the order and is very simple. Its disadvantage lies in outliers in the original data, which can cause common values to be compressed into a relatively small interval, making it challenging for the model to distinguish them. Additionally, it would be worthwhile to experiment with other normalization techniques.

Flag features \texttt{IS\_EXTERNAL}, \texttt{UNDEF}, \texttt{INVALID\_POINTER}, and \texttt{ZERO\_INITIALIZED}, do not need to be normalized because they only contain the values \texttt{0} and \texttt{1}.

The categorical feature \texttt{OPERATOR} is divided by the number of possible values since the value \texttt{0} is reserved for an empty feature. The number of possible values is determined from the training data.

The categorical feature \texttt{LABEL} is divided by \texttt{number of possible values - 1}, where there are 12 possible labels (original node sets). Here, \texttt{0} is not reserved because \texttt{LABEL} cannot have invalid values. Similarly, the categorical feature \texttt{FP} is divided by 4 because the possible values are \texttt{void}, \texttt{half}, \texttt{float}, \texttt{double}, and \texttt{fp128}, with \texttt{0} reserved for \texttt{void}~\cite{LLVM-IR}.

Numeric features such as \texttt{INT} (node set \texttt{TYPE}), \texttt{PTR}, \texttt{LEN}, \texttt{ORDER} (node set \texttt{AST\_NODE}), \texttt{ARGUMENT\_INDEX} (edge set \texttt{ARGUMENT}), and \texttt{ORDER} (node set \texttt{MEMBER}) are divided by the maximum values found among the training data (more info below).

The feature \texttt{HASH} (for all node sets) is normalized using the value $2^{24}-1$, where 24 is the hash length in bits. The reason for 24 bits is that the \texttt{float32} type has a mantissa of 24 bits (23 bits and 1 implicit bit), according to the IEEE 754 standard~\cite{ieee-floating-point}. It is thus possible to store a normalized number (though in the interval $<1, 2)$, see below) of 24 bits in \texttt{float32} without loss of information. The \texttt{float32} type must be used due to the reasons mentioned in the TFGNN schema description (see below).

For the feature \texttt{INT} (node set \texttt{LITERAL\_VALUE}), normalization is similar to \texttt{HASH}, except that accuracy of high values is sacrificed for better accuracy of lower values. The reason is that lower constant values are more likely to appear in control structures, such as loop counts, flags, etc., than higher values (as evidenced by checking many Infer reports in~\cite{bc}). Thus, the \texttt{INT} feature is essentially truncated to \texttt{int16}, converted to unsigned, and normalized using $2^{16}-1$ (\texttt{MAX\_UINT\_16}).

For the features \texttt{FP\_MANTISSA} and \texttt{FP\_EXPONENT}, simply dividing by the highest value is not possible due to differing magnitudes, which could result in a significant loss of information. The normalization used is based on the IEEE 754~\cite{ieee-floating-point} format for \texttt{float32} (which must be used, see below). The mantissa in this format is already normalized to the range $(-2, -1>$ for negative numbers and $<1, 2)$ for positive numbers. These intervals are only shifted to form a uniform interval $(0, 2)$ and then divided by 2 (here is a potentional loss of information). \texttt{FP\_EXPONENT} can take values from 0 to 255 for \texttt{float32} (or -127 to 128 due to the implicit offset), so dividing by 255 is sufficient to normalize it in unsigned format. However, since higher than \texttt{float32} values can also appear in the graph and must be encoded in \texttt{float32} and then normalized to $<0,1>$, information loss will undoubtedly occur, such that all larger floating point types are converted to \texttt{float32}. By splitting the original feature value in \texttt{float32} into two features, \texttt{FP\_MANTISSA} and \texttt{FP\_EXPONENT}, also in \texttt{float32}, the encoding and normalization process will not result in a significant loss of information (only in the form of inaccurate operations).

All normalization values that need to be obtained from the dataset must be derived from the training data. If they were obtained from the test or validation sets as well, information would be transferred to the training process. Thus, model evaluation would not be accurate -- model generalization would be affected to some extent. The extraction principle and sample values of the normalization coefficients are in Section~\ref{implementation-norm-coeffs}.


\subsubsection{TFGNN Schema}
As previously mentioned, when using the TFGNN library, it is necessary to define the TFGNN schema~\cite{tfgnn-schema}. It is an accurate and detailed description of the structure of heterogeneous multigraphs. The TFGNN schema designed in this thesis defines ECPGs (after feature engineering) just as Section~\ref{cpg} describes CPG graphs. However, the description here is stored in the form of Protocol Buffers~\cite{protobuf} (often referred to as Protobuf), which are language-neutral, platform-neutral, extensible mechanisms for serializing structured data. The TFGNN schema specifically uses a protocol named \texttt{tfgnn.GraphSchema}\footnote{Sources of \texttt{tfgnn.GraphSchema}: \url{https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/proto/graph_schema.proto}.}. The TFGNN schema contains information about individual \textit{graph pieces}:
\begin{itemize}
    \item \textbf{Context} -- a set of features that apply to the graph as a whole, such as the type of Infer error.
    \item \textbf{Node sets} -- disjoint sets of nodes where all nodes within a node set have the same set of features.
    \item \textbf{Edge sets} -- disjoint sets of edges where all edges within an edge set have the same set of features and also share the same source node set and target node set.
\end{itemize}

Each feature definition (for context, node set, and edge set) contains the following information~\cite{tfgnn-schema}:
\begin{itemize}
    \item \textbf{Name} -- must be unique within the graph piece.
    \item Description (optional).
    \item \textbf{Data type}:
    \begin{itemize}
        \item Integers \texttt{DT\_<INT|UINT><8|16|32|64>} or \texttt{DT\_BOOL} -- all stored as \texttt{int64}.
        \item Floating point \texttt{DT\_<FLOAT|DOUBLE|HALF|BFLOAT16>} -- all stored as \texttt{float32}.
        \item \texttt{DT\_STRING} -- stored as \texttt{bytes}.
    \end{itemize}
    \item \textbf{Shape} -- e.g., \texttt{[64]} for a vector of length 64, \texttt{-1} for ragged dimension~\cite{tf-ragged}, or it can be omitted, and then it is a scalar (all features in this thesis are scalars).
\end{itemize}

The schema allows specifying integer types as inputs for GNNs. However, since TFGNN model weights are in \texttt{float32}, all integer features are converted to \texttt{float32} after the first operation. These conversions are often done beforehand for type unification and better parallelization on the GPU\footnote{\textbf{Graphics Processing Unit (GPU)}.}. For this reason, all features (except \texttt{label}, see below) are defined as \texttt{float32}. This final conversion of all types to \texttt{float32} is why the feature transformations in the previous text revolved around \texttt{float32}, striving to preserve accuracy for \texttt{float32}.

The last step in creating the graphs is to include the \texttt{label} and \texttt{LINE} features in the TFGNN schema context. \texttt{label} indicates whether a graph belongs to the true positive or false positive class and has the type \texttt{DT\_INT32}. Including the graph label in the context is directly recommended by the TFGNN documentation~\cite{tfgnn-schema}. The feature \texttt{LINE} contains the line number on which the error was reported by Infer, normalized by the highest such value in the training data. According to~\cite{pujar2024analyzing}, this feature was the most important for Random Forest models and is thus included here as well. Again, adding other features to the context, which were used in~\cite{D2A-zheng2021d2a, pujar2024analyzing}, could also be beneficial.

This graph context and ECPGs together form the graphs described in the TFGNN schema. These complete graphs are stored in the TFRecords format, as advised by the TFGNN documentation~\cite{tfgnn-schema}, which can be easily read as input to GNNs and are also more space-efficient than raw ECPGs (Graph D2A) -- mainly due to feature engineering (see Section~\ref{implementation-feature-engineering}). The created TFGNN schema is included on the storage medium (see Appendix~\ref{appendix-media}) and is also available in the repository on GitHub\footnote{Created \textbf{TFGNN schema}: \url{https://github.com/TomasBeranek/but-masters-thesis/blob/thesis-submission/model/schemas/extended_cpg.pbtxt}.}.


\subsection{Graph Neural Networks Model}
\label{gnn-model}
The principle of GNNs (or GNN layers) was described in Section~\ref{gnn}. This section further describes, in more practical terms, the architecture used in this thesis (specific models with hyperparameters are provided in Chapter~\ref{experiments}), created in TFGNN to rank graphs (errors found by Infer) by the likelihood of being a real error. The trained model is the final output of the training pipeline, as seen in Figure~\ref{figure:training-pipeline}. The process of models training takes graphs (with labels inside) in the TFRecords format and a TFGNN schema as inputs. The process is then composed of the following steps:
\begin{enumerate}
    \item Load the graphs.
    \item Balance the class data.
    \item Create a preprocessing model.
    \item Create a model, consisting of the following main parts:
    \begin{enumerate}
        \item A~layer for initializing \textit{hidden states} (i.e., embedding vectors from Section~\ref{gnn}).
        \item GNN layers.
        \item \textit{GNN head}.
    \end{enumerate}
    \item Create a training loop.
    \item Save the model.
\end{enumerate}

Loading the graphs is very simple with the TFGNN support of the TFRecords format and is described in Section~\ref{implementation-training}. The loaded data is heavily unbalanced, as shown in Tables~\ref{tab:d2a-bug-types1} and~\ref{tab:d2a-bug-types2}. Even the most balanced project, libtiff, has a true positive:false positive ratio of only about 1:20, while the least balanced project, ffmpeg, has a ratio of almost 1:140. Balancing the data is crucial because it prevents models from favoring the majority class~\cite{balancing-data} and thus helps the model learn truly useful patterns. Balancing can be done in several ways:
\begin{itemize}
    \item Up-sampling the minority class -- replicate the minority elements (in this case, true positives) so that the ratio is approximately 1:1. The elements can be replicated in several ways:
    \begin{itemize}
        \item Duplication -- does not bring any new information.
        \item SMOTE\footnote{\textbf{Synthetic Minority Oversampling Technique (SMOTE)}.} -- create new synthetic samples from existing ones; this is a form of data augmentation~\cite{smote}.
    \end{itemize}
    \item Down-sampling the majority class -- randomly remove samples from the majority class until the data is balanced, but useful data are lost.
    \item Class weights -- add weights to classes that influence learning.
    \item K-fold cross-validation with a split of the majority class -- a type of K-validation~\cite{k-fold-validation}, where each split will contain the same set of all minority samples, but the majority data will be divided into as many equal parts as needed so that the ratio of each part is again approximately 1:1. For an original ratio of 1:20, this would be 20-fold cross-validation. One model is trained on each split -- 20 models in this case.
\end{itemize}

In this thesis, only up-sampling of the minority class is used. However, it would be beneficial to try other methods, especially SMOTE and K-fold cross-validation with subsequent voting by individual models.

The TFGNN documentation recommends creating a preprocessing model~\cite{tfgnn-input-pipeline}, which should adjust the data into its final form -- feature selection, feature splitting, normalization, etc. All these tasks have already been performed earlier (see Section~\ref{feature-engineering}), so the preprocessing model is used only to extract the label from the graph.

The main step is to create the actual model. The models used in this thesis have a very similar architecture and are heavily inspired by the TFGNN documentation~\cite{tfgnn-runner, tfgnn-gnn-modeling, tfgnn-mtalbis} and the examples in the TFGNN repository\footnote{\textbf{TFGNN}'s repository: \url{https://github.com/tensorflow/gnn}.}. A~slightly generalized architecture of the models used in this thesis is shown in Figure~\ref{figure:model-architecture}. The first part is a layer (or more) that initializes the hidden state of all nodes. The type of these layers is not strictly defined and depends on the format of the input nodes. This layer can be any differentiable function. In this thesis, the data in the nodes are a set of scalars, so only classic Dense layers (i.e., \textit{densely-connected neural network layers}) are used (one shared layer for each node set). If the features in the nodes were, for example, ragged, RNNs would probably be used. If the data were images, CNNs could be used, etc. Or this layer can be omitted, and the first GNN layer can be used to create hidden states.

After the layers initializing the hidden state, the GNN layers follow. In this thesis, \textit{MtAlbis layers}~\cite{tfgnn-mtalbis, tfgnn-gnn-models} are used, which are recommended for initial experiments by the TFGNN documentation. MtAlbis layers proved to be so effective in the experiments that they were retained, see Chapter~\ref{experiments}. These layers generalize VanillaMPNN\footnote{Vanilla Message Passing Neural Network (VanillaMPNN).} described in~\cite{ferludin2022tf}. MtAlbis work on heterogeneous graphs -- which are ECPGs. One cycle of updating all hidden node states, i.e., one round of the message-passing algorithm (see Section~\ref{gnn}), is a single MtAlbis layer~\cite{tfgnn-mtalbis}. Thus, the number of updates/layers is the depth of the model and one of the most important hyperparameters. The deeper the model, the further information from a~particular node can propagate, but the more complex the model -- it has more parameters, takes longer to learn, overfits more, etc.

\begin{figure}[t]
	\centering
	\includegraphics[width=1\textwidth]{figures/model-architecture.pdf}
	\caption{A~generalized architecture of the GNN models used in this thesis. The architecture is based on the MtAlbis GNN layers and is a binary classification model -- it ends with a Dense layer with a single output neuron.}
	\label{figure:model-architecture}
\end{figure}

The final part is the GNN head, which serves as the equivalent of the super node described in Section~\ref{gnn}. Information from all (or only some) node sets is input into a pooling layer, whose output is a combined hidden state (as it would be for a super node). This hidden state, together with features from the graph context, is input into additional fully connected layers (or a single layer), whose output is the model's output -- in this case, a single number. The model is supposed to distinguish between two classes, making it a binary classification. Thus, the last Dense layer must have a single neuron, as shown in Figure~\ref{figure:model-architecture}. The sigmoid function is applied to the output of this neuron (though it is not necessarily required), which converts the input number from the interval $(-\infty,+\infty)$ to $(0,1)$. Therefore, the model's output is a single number in this interval, which is higher the more confident the model is in class 1 (true positive). Models for binary classification are most often trained using \textit{Binary Cross Entropy}~\cite{cross-entropy}, as is the case with all models in this thesis.

Creating the training loop and saving models is again straightforward thanks to the use of TFGNN. Of course, it is also necessary to fine-tune hyperparameters such as learning rate, optimizer type, batch size, etc. More details can be found in Chapter~\ref{experiments}.

Models created using the training pipeline have interesting properties:
\begin{itemize}
    \item Thanks to the use of LLVM IR, they are \textbf{language-independent}.
    \item Since the output of Infer is only used for program slicing (and obtaining the type of error), it can \textbf{easily be adapted to another static analyzer}.
    \item It is also possible to use them \textbf{without a static analyzer}, if slicing criteria are created (possibly automatically) and the type of error is specified. This way, the models can completely replace static analysis and be used directly to find errors in the source code.
\end{itemize}


\section{Inference Pipeline}
\label{inference-pipeline}
The goal of the inference pipeline is to run Infer on a real C/C++ project, generate ECPGs for each Infer report, and then rank the reports based on their probability of being true positives using the created GNN models. As shown in Figure~\ref{figure:inference-pipeline}, the inference pipeline is fundamentally the same as the training pipeline. It differs only in the way LLVM bitcode is extracted and at the end, where models are not trained but are used solely for inference.

\begin{figure}[t]
	\centering
	\includegraphics[width=1\textwidth]{figures/inference-pipeline.png}
	\caption{The figure shows a schematic of the inference pipeline, which ranks Infer reports on real C/C++ projects based on the probability of being true positives. The dashed boxes indicate the intermediate products and generated data. The green outline indicates tools created as part of this thesis. Icons were taken from~\cite{icon-model, icon-tfrecords, icon-inference}.}
	\label{figure:inference-pipeline}
\end{figure}

\subsection{Capture Phase}
\label{capture-phase}
The goal of the capture phase (see Figure~\ref{figure:capture-phase}) is to connect to a running build process and capture the information required for the graph construction phase (described in Section~\ref{graph-construction-phase}). This necessary information includes the source files compiled to LLVM bitcode and the same source files captured in Infer's capture phase (see Section~\ref{infer}), which will then be analyzed by Infer. To obtain this information, compilation commands must be extracted from the build process. This can be done by:
\begin{enumerate}
    \item parsing the build scripts,
    \item capturing the commands using a compiler wrapper.
\end{enumerate}

\begin{figure}[t]
	\centering
	\includegraphics[width=1\textwidth]{figures/capture-phase.png}
	\caption{The figure shows a schematic of the capture phase, which generates LLVM bitcode and runs Infer analysis on real C/C++ software. The dashed boxes indicate the intermediate products and generated data. The green outline indicates tools created as a~part of this thesis. Icons were taken from~\cite{icon-compiler, infer-web}.}
	\label{figure:capture-phase}
\end{figure}

Parsing build scripts is very challenging because each build system uses a different syntax and different techniques. However, some build systems have built-in functionality for extracting compilation commands, such as CMake~\cite{cmake-doc}. Unfortunately, very few build systems have this feature. Another problem is that some software does not use any standard build systems. Instead, they use custom scripts (e.g., bash) for compilation, linking, etc. These scripts can have any structure and hierarchy of calling other scripts or tools, making it almost impossible to statically parse compilation commands from them. The use of such scripts is relatively common in SRPM\footnote{\textbf{Source Red Hat Package Manager (SRPM) package} -- provides the source code of software via the RPM package manager for operating systems like RHEL, Fedora, and CentOS.} packages, as found in author's previous work~\cite{bc}. This thesis aims for later deployment specifically on SRPM packages and must take this feature into account. For the reasons mentioned above, parsing build scripts of unknown software is generally impractical.

The second and practically applicable option is to create wrappers over C/C++ compilers and intercept the compilation commands during the build process. The design and implementation were addressed in the author's previous work~\cite{bc}, so the principles of the wrappers will be described here very briefly. Every time a compiler is invoked by the build system, the installed wrapper is called. For each such call, the wrapper captures its arguments and performs the following steps:

\begin{enumerate}
    \item It filters out options that are incompatible with Infer's internal Clang compiler, which is used to compile source files into SIL (see Section~\ref{infer}).
    \item It invokes Infer's capture phase, passing the modified compilation command. Infer then stores the captured source files in SIL representation in its database.
    \item It calls the original, unmodified command with the original compiler so that the build can proceed without issues.
\end{enumerate}

The wrapper is designed so that even if Infer's capture phase fails, the original command is still executed. Failure of Infer's capture phase will not crash the entire analysis/pipeline, but it may increase the likelihood of generating false positives/false negatives. This error recovery is possible due to Infer's properties: if it does not have the required implementations of the analyzed functions captured, it assumes they may return any value (limited by their return type, of course). This speculation introduces a certain degree of over-approximation and thus the potential for false positives. False negatives can occur because files not captured by the Infer capture phase are not analyzed. An important note is that these compiler wrappers can (and typically are) called in parallel. Therefore, it is necessary to be aware of possible critical sections, such as Infer's database. The description of how critical sections are protected in the wrapper can be found in~\cite{bc}.

For the inference pipeline, it is necessary to add additional functionality to the wrapper -- generating LLVM bitcode from each captured compilation command. The principle, including an example of how to generate LLVM bitcode using the compilation command, was already presented in Section~\ref{bitcode-generation}. Unlike the training pipeline, the inference pipeline must consider input compilation commands in all possible formats, so it is necessary to remove the \texttt{-o} option (and its value) and also ensure that it is truly a compilation command (it must contain the \texttt{-c} option).

The final task that the wrapper needs to accomplish is finding all the generated LLVM bitcode files. Again, there are several ways to obtain this list of files. For instance, one could analyze compilation commands and extract the names of the compiled files or insert \texttt{-o} options. However, the simplest method is currently used here: upon the wrapper's first invocation, a list of all existing \texttt{.bc} files in the filesystem is created. After the build is finished, this process is repeated. By comparing these two lists, it is possible to identify which \texttt{.bc} files were added during the build and thus contain the LLVM bitcode.

It may seem that running multiple builds concurrently could result in \texttt{.bc} files unrelated to the current project being compiled. This issue indeed occurs with all the methods mentioned, as it is not possible to distinguish which build the \texttt{.bc} files originated from. Similarly, Infer cannot distinguish between individual projects, so it is necessary to ensure that only one project's compilation is executed at any given time. However, running multiple projects concurrently will not cause errors but will merely lead to Infer reporting errors (and generating graphs) for all the projects being compiled.

After the build and before the graph construction phase begins, two additional steps must be taken. First, \textbf{all} generated LLVM bitcode files need to be merged into a~single file using the llvm-link tool. Then, the Infer analysis is performed on the captured files. After the analysis is complete, Infer generates a list of potential errors, from which slicing criteria are extracted for LLVM-Slicer during the graph construction phase. Unlike the training pipeline, where each error detected by Infer (or D2A sample in the training pipeline) generates its own LLVM bitcode file, here, a single file contains the entire source code. The LLVM bitcode files are differentiated only after slicing according to the criteria of individual reports.


\subsection{Inference Phase}
\label{inference-phase}
After the graph construction phase, feature engineering is applied to the raw ECPGs, just like in the training pipeline (see Section~\ref{feature-engineering}). The only difference is that normalization coefficients already generated from D2A are used.

Next comes the inference itself, as shown in Figure~\ref{figure:inference-pipeline}. The inference using the GNN model evaluates the input graphs -- representing individual errors found by Infer -- based on their probability of being true positives. The original output from Infer is then sorted in descending order according to the GNN model's score. Unsupported error types (see Section~\ref{bitcode-generation}) and errors for which a graph could not be generated (see Section~\ref{implementation}) are placed at the end of the list in their original relative order. Even in this sorted output, however, a typical trade-off is encountered between the number of true positives and the number of false positives. The more true positives that are sought, the worse the true positive vs. false positive ratio becomes (to ensure that all true positives are found, all reports must still be checked). However, the mere fact that it is possible to choose this threshold is a significant advantage of these sorted outputs compared to the unsorted ones.


\chapter{Implementation}
\label{implementation}
This chapter describes the implementation of training and inference pipelines, as designed in Chapter~\ref{design}. The training pipeline consists of a series of independent tools. Specifically, Section~\ref{implementation-d2a-filter} describes the D2A filter that removes unsupported error types. Section~\ref{implementation-bitcode-generator} discusses the implementation of a bitcode generator that creates LLVM bitcode for D2A samples. Section~\ref{implementation-slicing-info} describes the slicing criteria extractor. Section~\ref{implementation-graph-construction} details the generation of Graph D2A from LLVM bitcode. Section~\ref{implementation-norm-coeffs} explains the extraction of normalization coefficients for feature normalization. Section~\ref{implementation-feature-engineering} outlines the implementation of feature engineering, including graph and attribute transformations and normalization. Finally, Section~\ref{implementation-training} and Section~\ref{implementation-evaluation} cover model training and evaluation, respectively.

Unlike the training pipeline, the inference pipeline is fully automated. It comprises compiler wrappers, described in Section~\ref{implementation-compiler-wrapper}, and a script that automates graph creation, discussed in Section~\ref{implementation-inference-pipeline}.

All source files for both the training and inference pipelines are open-source and accessible on GitHub\footnote{All source files are available at \url{https://github.com/TomasBeranek/but-masters-thesis}.}. Data manipulation primarily utilized Python, particularly libraries such as Pandas\footnote{\textbf{Pandas}'s website: \url{https://pandas.pydata.org/}.}, NumPy\footnote{\textbf{NumPy}'s website: \url{https://numpy.org/}.}, NetworkX\footnote{\textbf{NetworkX}'s website: \url{https://networkx.org/}.} (nx), and TensorFlow\footnote{\textbf{TensorFlow}'s website: \url{https://www.tensorflow.org/}.}. Bash and make were used to automate the calling of scripts and other auxiliary tasks.

The following sections provide details on the computation times for various components. All measurements were conducted on Ubuntu 20.04 with the following hardware:
\begin{itemize}
    \item CPU\footnote{\textbf{Central Processing Unit (CPU).}} -- Intel(R) Core(TM) i5-10400F CPU @ 2.90GHz, 6x cores, 12x threads,
    \item GPU\footnote{\textbf{Graphics Processing Unit (GPU)}.} -- NVIDIA GeForce RTX 3060 Ti, 8GB,
    \item RAM\footnote{\textbf{Random Access Memory (RAM)}.} -- 16GB,
    \item Memory -- 500GB SSD\footnote{\textbf{Solid-State Drive (SSD)}.}.
\end{itemize}


\section{D2A Filter}
\label{implementation-d2a-filter}
The initial step in creating the Graph D2A involves filtering out unsupported data types, as discussed in Section~\ref{bitcode-generation}. This filtering is implemented through a script named \texttt{filter.py}, written for Python 3.8. The script takes a directory containing the D2A dataset as its input (specified using the \texttt{-d} or \texttt{-{}-dir} option), which can be downloaded from~\cite{D2A-webpage}. The files, named in the format \texttt{\{project\}\_labeler\_\{0|1\}.pickle.gz}, correspond to individual projects and labels. Although "after-fix" samples are available (see Chapter~\ref{chapter-d2a}), they are not used in this thesis and are ignored by \texttt{filter.py}. The second input parameter is the output directory (specified using the \texttt{-o} or \texttt{-{}-output-dir} option), where the results are stored (if the directory does not exist, it will be created).

Each input file in the input directory is first decompressed from the \texttt{.gz} archive using the \texttt{gzip} library. Then, using the \texttt{pickle} library, which is used for object serialization and deserialization, the samples are sequentially read. Samples belonging to the supported error types are saved in a file with the same name (including \texttt{.pickle.gz}) in the output directory. Unsupported samples are completely discarded.

From each saved sample, certain information that is no longer needed in the training pipeline is also removed. This significantly reduces the size of the samples, saving disk space and speeding up operations such as loading and saving. The removed information includes (explanations of each attribute can be found in~\cite{d2a-sample-description}) \texttt{label\_source}, \texttt{bug\_loc\_trace\_index}, \texttt{sample\_type}, \texttt{commit[changes]}, \texttt{functions}, and \texttt{zipped\_bug\_report}.

The command to run \texttt{filter.py} might look like this:
\begin{lstlisting}[language=bash, xleftmargin=2em]
python3.8 filter.py -d d2a/ -o d2a-filtered/
\end{lstlisting}

The script removes 20,732 samples with unsupported error types, which is \textasciitilde1.6~\% of the total number of samples. The number of removed samples for each project is shown in Table~\ref{tab:implementation-d2a}. Filtering the entire dataset takes \textasciitilde3 minutes, and the dataset size is reduced from \textasciitilde3.6GB to \textasciitilde288MB (calculated only with the \texttt{*\_labeler\_*} files).


\section{Bitcode Generator}
\label{implementation-bitcode-generator}
From the filtered D2A dataset, it is necessary to generate a single LLVM bitcode for each sample, as described in Section~\ref{bitcode-generation}. This is accomplished using the \texttt{generate\_bitcode.py} script for Python 3.8. The script must be applied individually to each \texttt{*\_labeler\_*} generated by \texttt{filter.py} (see Section~\ref{implementation-d2a-filter}), specified using the \texttt{-f} or \texttt{-{}-file} option. The script also requires the directory containing the \textbf{original} project repository (using the \texttt{-r} or \texttt{-{}-repository} option) and the project (using the \texttt{-{}-project} option) from which the LLVM bitcode will be generated, such as \texttt{httpd}\footnote{\textbf{httpd}'s repository: \url{https://github.com/apache/httpd}.}). Finally, the output directory must be specified (using \texttt{-o} or \texttt{-{}-output-dir}); if it does not exist, it will be created. Running \texttt{generate\_bitcode.py} might look as follows:
\begin{lstlisting}[language=bash, xleftmargin=2em]
python3.8 generate_bitcode.py -r httpd/ --project httpd \
-f d2a-filtered/httpd_labeler_1.pickle.gz -o d2a-bitcode/httpd_1/
\end{lstlisting}

The script first retrieves a chronological list of all commits from the repository using:
\begin{lstlisting}[language=bash, xleftmargin=2em]
git log --all --format=%H
\end{lstlisting}

Then, a set of commits for all samples from the input D2A file is obtained. From the complete list of commits, those that are not also in the set of commits from the D2A are removed -- in other words, commits on which no D2A samples exist. This modified list is reversed so that the first commit is the oldest and the last is the newest. The script then iterates through individual commits (represented by their hashes) and performs the following:

\begin{enumerate}
    \item The repository is switched to the given commit using \texttt{git reset -{}-hard HASH}.
    \item All files that are not part of the repository (especially products from previous runs) are deleted using \texttt{git clean -dfx}.
    \item A~project-specific set of actions required for a successful build is performed (see below).
    \item For each D2A sample at this commit:
    \begin{enumerate}
        \item A~list of files to be compiled for the given sample is obtained from D2A.
        \item Samples consisting only of \texttt{.h} files or containing files such as \texttt{.y} or \texttt{.l} are skipped, as they do not generate LLVM bitcode when compiled.
        \item The cache is checked to see if the LLVM bitcode for a sample with the same set of files has already been generated at the current commit. If so, a symlink\footnote{\textbf{Symlink} -- A~special type of file that points to another file in the filesystem.} \texttt{\{output\_dir\}/\{sample\_id\}.bc} is created, pointing to the already generated LLVM bitcode. This speeds up the process because recompilation is not required and reduces memory usage since the same sample does not need to be stored multiple times. Symlinks only occupy a few bytes.
        \item The repository is cleaned of residual files from previous compilations (at this commit) using project-specific criteria to avoid deleting essential configuration data generated when switching to this commit.
        \item A~set of already present \texttt{.bc} files in the repository is obtained; these are not LLVM bitcode files.
        \item For each record (compiled file) in the D2A attribute \texttt{compiler\_args}:
        \begin{enumerate}
            \item Adjust the D2A compiler arguments -- replace \texttt{<$repo$>} with the repository path and remove arguments starting with \texttt{<$sys$>}, which include external libraries specific to httpd (these libraries are included with their own paths, see below).
            \item Add missing project-specific include arguments (\texttt{-I}).
            \item Add arguments to generate LLVM bitcode (see Section~\ref{bitcode-generation}).
            \item Execute the generated compilation command.
        \end{enumerate}
        \item Using the previously located existing \texttt{.bc} files, obtain a list of newly added files -- generated LLVM bitcode files.
        \item Ensure that the same number of LLVM bitcode files have been generated as there are original \texttt{.c} files (\texttt{.h} files are included -- they do not generate separate LLVM bitcode).
        \item Use \texttt{llvm-link} to combine all LLVM bitcode files of the current sample into a~single LLVM bitcode file and save it to \texttt{\{output\_dir\}/\{sample\_id\}.bc}.
        \item Finally, note in the cache which files were used to generate this LLVM bitcode.
    \end{enumerate}
\end{enumerate}

After switching to the new version of the repository (new commit), a pre-compilation configuration is required. This typically involves generating platform-specific \texttt{.h} or \texttt{.c} files, generating configuration files, setting the correct paths to libraries, etc. This phase is different and quite extensive for each project, so only the procedure for httpd will be described here as an example. Configuration details of other projects can be found directly in \texttt{generate\_bitcode.py}.

The httpd project is the only one that requires downloading external libraries in advance. Specifically, \texttt{apr-1.7.4} and \texttt{apr-util-1.6.3}, which can be downloaded from the Apache website\footnote{\textbf{Apache}'s website: \url{https://apr.apache.org/download.cgi}.}, and the \texttt{pcre2-10.42} library, which is available in the \texttt{pcre2} repository\footnote{\textbf{pcre2}'s repository: \url{https://github.com/PCRE2Project/pcre2/releases/tag/pcre2-10.42}.}. These libraries must be renamed to \texttt{apr}, \texttt{apr-util}, and \texttt{pcre} and moved to the \texttt{httpd-dependencies/srclib} directory, which must be at the same level as the httpd repository. Furthermore, all libraries need to be configured according to their instructions (pre-configured libraries are included in the attached media; see Appendix~\ref{appendix-media}).

The \texttt{generate\_bitcode.py} script moves to the httpd repository and prepares for the httpd compilation as follows:
\begin{enumerate}
    \item Copies the external libraries \texttt{apr} and \texttt{apr-util} to the \texttt{srclib/} directory in the repository.
    \item In some versions of the repository that contain the \texttt{pcre} library, it is necessary to initiate configuration by first running \texttt{./buildconf} (still in the root directory of the repository), which creates \texttt{srclib/pcre/configure}. Then, switch to \texttt{srclib/pcre/} and run \texttt{./configure} to generate the necessary header files for the \texttt{pcre} library, such as \texttt{config.h}.
    \item If the \texttt{pcre} library is not present, the script copies the already configured one from \texttt{../httpd-dependencies/srclib/pcre/} into \texttt{srclib/}.
    \item The script then checks whether any of the tracked files have changed from the last version of the project:
    \begin{itemize}
        \item \texttt{include/ap\_config\_auto.h.in},
        \item \texttt{include/ap\_config\_layout.h.in},
        \item \texttt{modules/ssl/ssl\_policies.h.in},
        \item \texttt{buildconf}.
    \end{itemize}
    These are templates for the generated \texttt{.h} files and the configuration file. If none of these files have changed, the previously generated \texttt{.h} files can be reused -- copy them back from \texttt{/tmp/d2a\_pipeline/}. This saves a lot of time since generating them and running \texttt{./buildconf} is time-consuming across many commits.
    \item If any template has changed, \texttt{./buildconf} and \texttt{./configure} must be rerun. The configuration process takes about 20 seconds, but it has to be done for thousands of commits.
    \item The script checks another set of tracked files that are generated differently:
    \begin{itemize}
        \item \texttt{server/gen\_test\_char.c},
        \item \texttt{srclib/pcre/dftables.c}.
    \end{itemize}
    If they have not changed, again copy them from previous versions.
    \item If they have changed, they need to be generated as follows:
    \begin{enumerate}
        \item \texttt{include/test\_char.h} -- generated using \texttt{gcc -Isrclib/apr/include \newline -Isrclib/apr-util/include server/gen\_test\_char.c -o gen\_test\_char} 
        \newline followed by \texttt{./gen\_test\_char > include/test\_char.h}.
        \item \texttt{include/chartables.c} -- generated using \texttt{gcc srclib/pcre/dftables.c -o dftables} followed by \texttt{./dftables include/chartables.c}. In some newer versions, it is necessary to check if \texttt{include/chartables.c} was created, and if not, it must be generated using \texttt{./dftables > include/chartables.c} instead.
    \end{enumerate}
\end{enumerate}

For each version (commit) of the project, running the configuration multiple times should be avoided. To prevent losing the generated configuration files, a project-specific cleanup that preserves the contents of certain directories is used. For httpd, the command is:
\begin{lstlisting}[language=bash, xleftmargin=2em]
git clean -dfx --exclude=srclib/ --exclude=include/
\end{lstlisting}

When starting the compilation for individual files of each sample, arguments extracted from D2A are used. However, most are insufficient as they do not include necessary \texttt{-I} paths for various header files. This may be due to different methods of installing libraries when creating D2A, so these paths need to be added. For httpd, \texttt{-Iinclude}, \texttt{-Isrclib/apr/include}, and \texttt{-Isrclib/apr-util/include} are appended.

As previously indicated, some samples might be skipped, or their compilation may fail. Statistics for individual projects are presented in Table~\ref{tab:implementation-d2a}. For httpd\_1, the size of the filtered D2A is \textasciitilde47KB, compared to the unfiltered D2A, which is \textasciitilde629KB. The generated LLVM bitcode for httpd\_1 is \textasciitilde22MB. The script for \texttt{httpd\_1} runs for \textasciitilde330 seconds, which equates to \textasciitilde1.6 seconds per sample. Parallelization would speed this up, but since the project repository is a critical section accessed almost continuously, it would be necessary, for instance, to duplicate it. Thus, parallelization of this process is left for potential future improvements. Similarly, enhancements to the automated project configuration are also left for future improvements, as they could improve the success rate of bitcode generation. However, compilation issues must be resolved manually, which consumes an enormous amount of time.


\section{Slicing Criteria Extractor}
\label{implementation-slicing-info}
To enable slicing of the generated LLVM bitcode from Section~\ref{implementation-bitcode-generator}, it is first necessary to extract slicing criteria from the filtered D2A from Section~\ref{implementation-d2a-filter}. For this purpose, the Python 3.8 script \texttt{slicing\_criteria\_extraction.py} is provided. It takes as input a single file from the filtered D2A (specified using the \texttt{-{}-d2a} option). The script outputs the slicing criteria in the CSV format (without header) to \texttt{stdout}, with the following columns:

\begin{enumerate}
    \item \texttt{status} -- 0 means success, 1 indicates an internal error.
    \item \texttt{bug\_id} -- the \texttt{id} of the sample.
    \item \texttt{entry} -- name of the entry function (see Section~\ref{graph-construction-phase}).
    \item \texttt{file} -- the file where the error is located.
    \item \texttt{fun} -- the function where the error is located.
    \item \texttt{line} -- the line number where the error is located.
    \item \texttt{variable} -- the variable associated with the error.
\end{enumerate}

An example of running \texttt{slicing\_criteria\_extraction.py} might look like this:
\begin{lstlisting}[language=bash, xleftmargin=2em]
python3.8 slicing_criteria_extraction.py --d2a d2a-filtered/ \ 
httpd_labeler_1.pickle.gz > slicing-info/httpd_labeler_1.csv
\end{lstlisting}

The \texttt{slicing\_criteria\_extraction.py} script is used for both the training and inference pipelines. This is because both D2A and Infer's output are in the JSON format, and since D2A originates from Infer's output, they are quite similar. When the script is used on the D2A sample, it is converted to the same format as Infer's output via the simple \texttt{transform\_d2a\_sample} function, which essentially involves renaming and splitting some D2A attributes.

For each sample/report, a function \texttt{extract\_\{error\_type\_group\}} (extracting the slicing criteria) is invoked based on its type -- the 6 groups listed in Section~\ref{graph-construction-phase}. Retrieving \texttt{entry}, \texttt{file}, \texttt{fun}, and \texttt{line} is straightforward: the correct attributes are simply extracted from the JSON (see Section~\ref{graph-construction-phase}). If \texttt{variable} is extracted, it is obtained from the \texttt{qualifier} field. For \texttt{bug\_id}, \texttt{id} from D2A is used in the case of D2A. For Infer, the samples are labeled incrementally starting from \texttt{0}, and unsupported sample types are skipped in the numbering to preserve the original numbering in Infer's output.

The script skips unsupported error types. If an unknown format of a supported error is encountered, the script returns \texttt{status = 1} and tries to extract at least \texttt{entry}, \texttt{file}, \texttt{fun}, and \texttt{line} from the basic information to allow slicing based on the line number.

If the \texttt{file} is a header file (\texttt{.h}), the \texttt{file} field is left empty because of future slicing -- because slicing based on header files is not supported by LLVM-Slicer in the standard format. Instead, slicing should be done using only \texttt{fun} and \texttt{line}, excluding the \texttt{file} field. If the \texttt{file} contains a regular \texttt{.c} file, \texttt{fun} is omitted because \texttt{file} and \texttt{line} are sufficient to determine the slicing criteria unambiguously. Extracting slicing criteria from the filtered D2A completes for all files in under a minute.


\section{Graph Construction Script}
\label{implementation-graph-construction}
The bash script \texttt{construction\_phase\_d2a} is used for generating Graph D2A from LLVM bitcode (created in Section~\ref{implementation-bitcode-generator}) and slicing criteria in the CSV format (created in Section~\ref{implementation-slicing-info}). This script implements the remaining transformations described in Section~\ref{graph-construction-phase}. The script accepts the following position-dependent arguments:
\begin{enumerate}
    \item The output directory for storing raw ECPGs.
    \item The file containing slicing criteria.
    \item The directory containing LLVM bitcode.
    \item (optional) The sample number at which to end.
    \item (optional) The sample number from which to start.
\end{enumerate}

The \texttt{construction\_phase\_d2a} script can be executed with a command such as:
\begin{lstlisting}[language=bash, xleftmargin=2em]
./construction_phase_d2a graph-d2a/httpd_1 httpd_labeler_1.csv \
d2a-bitcode/httpd_1
\end{lstlisting}

The script \texttt{construction\_phase\_d2a} operates as follows:
\begin{enumerate}
    \item Records from the slicing information (from its copy) that already have a directory with raw ECPG are removed. This allows for the intermittent transformation of the dataset.
    \item The slicing information file is divided into smaller files of 100 lines each (the last file may be smaller) and stored in \texttt{/tmp/construction\_phase\_d2a/split\_files/}.
    \item Each file in \texttt{split\_files/} is then processed as follows:
    \begin{enumerate}
        \item The \texttt{create\_cpgbin} function is called in parallel for each line in \texttt{file} using the command:
\begin{lstlisting}[language=bash, xleftmargin=1em]
cat ${file} | parallel --colsep ',' create_cpgbin {1} {2} {3} \
{4} {5} {6} {7}
\end{lstlisting}
        This function generates a binary CPG for each line from LLVM2CPG (detailed description of this function is provided below).
        \item If \texttt{/tmp/construction\_phase\_d2a/cpg/\$\{bug\_id\}.cpg.bin.zip} was not generated for some samples, these samples are removed from \texttt{file}.
        \item A script for Joern is generated, containing commands to load and re-save all \texttt{.cpg.bin.zip} files, thereby expanding them into ECPGs. An example Joern script is provided in Listing~\ref{listing:joern-script}.
        \item Joern processes all (up to 100) ECPGs.
        \item The \texttt{cpgbin\_to\_csv} function is called in parallel for each line in \texttt{file}, but only the \texttt{bug\_id} column is used:
\begin{lstlisting}[language=bash, xleftmargin=1em]
cat ${file} | parallel --colsep ',' cpgbin_to_csv {2}
\end{lstlisting}
        This function converts binary ECPGs to CSV -- raw ECPGs, stored in the output directory. Each raw ECPG has its own directory (named after its \texttt{bug\_id}) containing CSV files.
        \item Finally, all temporary files created during the current iteration are cleaned up to prevent accumulation of logs and intermediate files, which could unnecessarily consume memory.
    \end{enumerate}
    \item Statistics on the number of successful/unsuccessful samples are then calculated and printed.
\end{enumerate}

\begin{lstlisting}[
    language=json, 
    label={listing:joern-script}, 
    float=t,
    caption={An example of an automatically generated Joern script for \texttt{httpd\_1}. The script only includes \texttt{importCpg} to load binary CPGs (up to 100) and concludes with the \texttt{save} command, which saves the graphs.}
]
 importCpg("/tmp/construction_phase_d2a/cpg/httpd_27 ... 17_1.cpg.bin.zip")
 importCpg("/tmp/construction_phase_d2a/cpg/httpd_04 ... 1e_1.cpg.bin.zip")
 // more imports
 importCpg("/tmp/construction_phase_d2a/cpg/httpd_3e ... 2c_1.cpg.bin.zip")
 importCpg("/tmp/construction_phase_d2a/cpg/httpd_1d ... 02_1.cpg.bin.zip")
 save
\end{lstlisting}

The aforementioned function \texttt{create\_cpgbin}, which takes a line with slicing criteria as input, works as follows:
\begin{enumerate}
\item First, it checks whether the input directory with LLVM bitcode contains the bitcode for the current sample. If it does, the function continues.
\item The LLVM-Slicer is called using:
\begin{lstlisting}[language=bash, xleftmargin=1em]
timeout 3s llvm-slicer --sc="${file}#${fun}#${line}#${variable}" \
--entry=${entry} -o=${bc_sliced} ${bc_combined}
\end{lstlisting}
The \texttt{timeout} command ensures that \texttt{llvm-slicer} completes its run. Experiments have shown that it can sometimes get stuck or run for several minutes, which is unacceptable given the large number of samples. By removing certain columns from the slicing criteria (see Section~\ref{implementation-slicing-info}) and leveraging the behavior of variables in bash, it is possible to call \texttt{llvm-slicer} uniformly for both \texttt{.c} and \texttt{.h} files.
\item If \texttt{llvm-slicer} is successful, a CPG in binary format is generated using \texttt{llvm2cpg} as follows:
\begin{lstlisting}[language=bash, xleftmargin=1em]
llvm2cpg ${bc_sliced} --output=${cpg_bin}
\end{lstlisting}
where \texttt{\$\{cpg\_bin\}=/tmp/construction\_phase\_d2a/cpg/\$\{bug\_id\}.cpg.bin.zip}.
\end{enumerate}

The \texttt{cpgbin\_to\_csv} function, which takes only \texttt{bug\_id} as input, simply calls \texttt{joern-export} as follows:

\begin{lstlisting}[language=bash, xleftmargin=2em]
joern-export --repr all --format neo4jcsv \
-o "${output_dir}/${bug_id}" ${joern_cpg_bin}
\end{lstlisting}

where \texttt{\$\{joern\_cpg\_bin\}} contains the path to the binary ECPG in the temporary directory \texttt{/tmp/construction\_phase\_d2a/workspace/\$\{bug\_id\}.cpg.bin.zip/cpg.bin}. At the end of the function, the success of \texttt{joern-export} is checked.

As mentioned earlier, the creation of binary CPGs and the conversion of binary ECPGs to CSV are parallelized using the \texttt{parallel} tool. However, the bottleneck here is Joern, which, despite running multiple instances, does not provide any speedup. Moreover, starting up Joern takes multiple seconds, so ideally, it is best to start and stop it as little as possible, hence it works in batches of 100. Larger batch sizes have been tested to further reduce the startup load of Joern, but the following issues were found:
\begin{itemize}
\item Batch > 5000 -- Joern crashes.
\item Batch > 3000 -- Joern may get stuck in an infinite loop.
\item Batch > 500 -- Joern non-deterministically generates incomplete graphs (missing edge sets like \texttt{CDF}, \texttt{CFG}, etc.).
\item Batch = 100 -- Joern works correctly.
\end{itemize}

The following results were measured on \texttt{httpd\_1}. The non-parallelized script generates approximately 500 graphs per hour. By parallelizing both of the phases mentioned above, the script reaches approximately 1100 graphs per hour. Moving Joern to batch mode allows the script to generate approximately 4000 graphs per hour. Other projects have been found to contain, on average, larger graphs than \texttt{httpd}, so the number of graphs per hour may be smaller for those projects. The output graphs for httpd\_1 are \textasciitilde240MB.

As hinted earlier, some samples may fail. The largest contributing factor is the timeout for LLVM-Slicer. It is possible to increase the timeout, but that would decrease the number of graphs per hour. The number of successfully generated samples can be seen in Table~\ref{tab:implementation-d2a}.


\section{Normalization Coefficients Extractor}
\label{implementation-norm-coeffs}
Before applying feature engineering (see Section~\ref{feature-engineering}), it is necessary to extract normalization coefficients from individual projects in Graph D2A (created in Section~\ref{implementation-graph-construction}). This is the task of the \texttt{find\_normalization\_coefficients.py} script for Python 3.8. The script is executed for each project separately with 6 position-dependent arguments:
\begin{enumerate}
    \item directory with false positives in Graph D2A of the specific project,
    \item directory with true positives in Graph D2A of the specific project,
    \item project name (httpd, nginx, or libtiff),
    \item \texttt{splits.csv} file providing the data split into train, val, and test sets, downloadable from~\cite{D2A-webpage},
    \item slicing criteria for false positives of the specific project,
    \item slicing criteria for true positives of the specific project.
\end{enumerate}
Running the \texttt{find\_normalization\_coefficients.py} script for httpd could look like this:
\begin{lstlisting}[language=bash, xleftmargin=2em]
python3.8 find_normalization_coefficients.py graph-d2a/httpd_0/ \
graph-d2a/httpd_1/ httpd d2a/splits.csv httpd_labeler_0.csv \
httpd_labeler_1.csv
\end{lstlisting}

The \texttt{find\_normalization\_coefficients.py} script works as follows:
\begin{enumerate}
    \item It processes \texttt{splits.csv} and selects the set of \texttt{id} samples belonging to the input project and the training set.
    \item It then iterates over all false positive samples and then all true positive samples (the order does not matter) as follows:
    \begin{enumerate}
        \item If the sample \texttt{id} is not in the training set, it is skipped -- obtaining information from the validation and test sets is avoided because it could affect the experiments.
        \item For each CSV header file (\texttt{*\_header.csv}) of the current sample:
        \begin{enumerate}
            \item If the current header file does not belong to the original node sets of the merged node set \texttt{AST\_NODE} (see Section~\ref{feature-engineering}), the node set \texttt{TYPE}, or the node set \texttt{MEMBER}, proceed to the next header file.
            \item The corresponding data file \texttt{*\_data.csv} is read.
            \item If the header is \texttt{nodes\_TYPE\_header.csv} (\texttt{TYPE} node set), \texttt{LEN} and \texttt{PTR} values are extracted (see Section~\ref{feature-engineering}) and if their maximum values are greater than the currently found ones, they are updated. Then, proceed to the next header file.
            \item For a header file from the merged \texttt{AST\_NODE} node set or \texttt{MEMBER} node set, the values \texttt{MEMBER\_ORDER} (for node set \texttt{MEMBER}) and \texttt{ORDER} (for all others) are updated (see Section~\ref{feature-engineering}).
            \item If the header file is for the node set \texttt{METHOD}, newly found operators (if any) are added to the \texttt{OPERATORS} set.
            \item If the header file has a column \texttt{ARGUMENT\_INDEX}, it is stored together with the column \texttt{ID}.
        \end{enumerate}
        \item From the file \texttt{edges\_ARGUMENT\_data.csv}, obtain the set of target nodes for \texttt{ARGUMENT} edges. From previously stored \texttt{ARGUMENT\_INDEX}, discard those that are not target nodes for \texttt{ARGUMENT} edges (using \texttt{ID}s). From the remaining ones, update the maximum value of \texttt{ARGUMENT\_INDEX}.
    \end{enumerate}
    \item Extract the maximum value of \texttt{LINE} (for graph context, see Section~\ref{gnn-model}) from the files with slicing criteria.
    \item Finally, print all normalization coefficients.
\end{enumerate}

The \texttt{find\_normalization\_coefficients.py} script outputs its results to \texttt{stdout} in the format shown in Listing~\ref{listing:norm-coeffs}. Normalization coefficients \texttt{ARGUMENT\_INDEX}, \texttt{LEN}, \texttt{LINE}, \texttt{ORDER}, \texttt{MEMBER\_ORDER} and \texttt{PTR} are the maxima of all found attributes. \texttt{OPERATORS} is the set of all found operators. And \texttt{BUG\_TYPES} is the set of all supported error types (see Section~\ref{bitcode-generation}). Extraction for the httpd project takes about \textasciitilde260s.


\begin{lstlisting}[
    language=json, 
    label={listing:norm-coeffs}, 
    float=t,
    caption={An example of the normalization coefficients for the httpd project, generated by the \texttt{find\_normalization\_coefficients.py} script.}
]
 {'ARGUMENT_INDEX': 14,
  'BUG_TYPES': ['NULL_DEREFERENCE',
                // more error types
                'UNINITIALIZED_VALUE'],
  'LEN': 65536,
  'LINE': 9162,
  'MEMBER_ORDER': 75,
  'OPERATORS': {'<operator>.addition',
                '<operator>.addressOf',
                // more operators
                '<operator>.subtraction',
                '<operator>.xor'},
  'ORDER': 1471,
  'PTR': 4}
\end{lstlisting}


\section{Feature Engineering Script}
\label{implementation-feature-engineering}
After extracting the normalization coefficients (described in Section~\ref{implementation-norm-coeffs}), feature engineering (designed in Section~\ref{feature-engineering}) can be applied to Graph D2A (created in Section~\ref{implementation-graph-construction}) to produce a dataset in the TFRecords format. All feature selection, graph transformations, and attribute transformations are implemented using the \texttt{feature\_engineering.py} script for Python 3.8. The script is called separately for each project and label with 8 position-dependent arguments:
\begin{enumerate}
    \item TFGNN schema file (designed in Section~\ref{feature-engineering}),
    \item output file name,
    \item project name (httpd, libtiff, nginx, ...),
    \item label (\texttt{0} or \texttt{1}),
    \item \texttt{splits.csv} file,
    \item filtered D2A file (\texttt{*\_labeler\_*}),
    \item file with slicing criteria,
    \item (optional) Which of the normalization coefficients to use (\texttt{httpd}, \texttt{libtiff}, \texttt{nginx}, or \texttt{nginx+libtiff+httpd}). If the argument is missing, the project value is used.
\end{enumerate}

The script reads directories with individual samples from its \texttt{stdin} (one directory per line). Running \texttt{feature\_engineering.py} for \texttt{httpd\_1} might look like this:
\begin{lstlisting}[language=bash, xleftmargin=2em]
find graph-d2a/httpd_1 -mindepth 1 -type d | python3.8 \
feature_engineering.py extended_cpg.pbtxt \
tfrecords/httpd_1.tfrecords httpd 1 d2a/splits.csv \
d2a-filtered/httpd_labeler_1.pickle.gz httpd_labeler_1.csv
\end{lstlisting}

The \texttt{feature\_engineering.py} script does the following for each input Graph D2A sample:
\begin{enumerate}
    \item Loads only those node/edge set files that are not to be removed, with the exception of the \texttt{TYPE\_DECL} node set, which is removed later (see Section~\ref{feature-engineering}). If any used edges were connected to a node that was not loaded, it becomes an invalid node that needs to be removed appropriately. During loading, a merged \texttt{AST\_NODE} is also created, and the original node set name is stored in the \texttt{LABEL} attribute of each \texttt{AST\_NODE} node.
    \item Discards unused node set attributes.
    \item From the loaded nodes and the \texttt{AST} node set, a \texttt{MultiDiGraph} representation is created using the \texttt{nx} library optimized for graph processing.
    \item Using the simple algorithm described in Section~\ref{feature-engineering}, all invalid nodes are removed from the graph (for now, it is just a set of ASTs).
    \item Using \texttt{nx.weakly\_connected\_components(G)}, all WCCs are obtained, and those consisting only of \texttt{BLOCK} nodes are removed.
    \item All leaf \texttt{BLOCK} nodes are also removed. At this stage, all currently present nodes are considered valid (although some will still be removed later).
    \item Other edge sets are added to the graph, with the \texttt{ARGUMENT} edges only added if they originate from a \texttt{CALL}, meaning:
\begin{lstlisting}[language=bash, xleftmargin=2em]
G.nodes[edge['start']]['type'] == 'CALL'
\end{lstlisting}
    \item Newly added edges may again create invalid nodes, which can now be easily removed along with their edges, as removing them will not disconnect the ASTs.
    \item The graph optimizations described in Section~\ref{feature-engineering} are then performed:
    \begin{enumerate}
        \item removing AST children of external methods,
        \item removing unnecessary \texttt{EVAL\_TYPE} edges,
        \item removing all \texttt{TYPE\_DECL} nodes,
        \item removing unused \texttt{TYPE} nodes.
    \end{enumerate}
    \item At this stage, it is verified that the graph forms a single WCC because no further edge or node removals will be performed that could split the graph into multiple WCCs.
    \item All \texttt{METHOD} and \texttt{LITERAL} nodes are split into data and latent nodes (see Section~\ref{feature-engineering}), which adds the node sets \texttt{METHOD\_INFO}, \texttt{LITERAL\_VALUE}, and also the edge sets \texttt{METHOD\_INFO\_LINK} and \texttt{LITERAL\_VALUE\_LINK}.
    \item At this stage, all node/edge sets are converted to separate \texttt{DataFrame} tables using the \texttt{Pandas} library, which is optimized for tabular operations. From this point onward, attributes of individual edge/node sets are processed in groups, not the graph structure itself.
    \item All attributes are split as needed and normalized using the extracted normalization coefficients (see Section~\ref{feature-engineering}).
    \item Now, it is necessary to convert \texttt{SOURCE} and \texttt{TARGET}, which contain the node IDs in all edges. Currently, nodes have IDs in ascending order starting from 1. However, TFGNN identifies nodes differently. They are numbered in ascending order starting from 0, but within node sets -- meaning there can be two or more nodes with the same ID if each is in a different node set. Since edge sets must define source and target node sets (see Section~\ref{feature-engineering}), there will be no collisions.
    \item Finally, the orientation of some edge sets is reversed (see Section~\ref{feature-engineering}).
    \item A~TFGNN \texttt{GraphTensor} is created using the \texttt{from\_pieces} and \texttt{from\_fields} methods~\cite{tfgnn-graphtensor},
    \item Serializes the \texttt{GraphTensor} objects into \texttt{tfrecords} files according to whether the sample belongs to the train, val, or test set.
\end{enumerate}

The outputs of the \texttt{feature\_engineering.py} script are files with the \texttt{*.train}, \texttt{*.val}, and \texttt{*.test} extensions in the TFRecords format. Some samples may be faulty -- for instance, no \texttt{AST} edge set was generated for them (by Joern), which must always be present in a valid sample. The number of successfully generated samples is shown in Table~\ref{tab:implementation-d2a}. The script runs on \texttt{httpd\_1} for approximately \textasciitilde80 seconds. The script was parallelized at the level of individual samples, but parallelization did not bring any significant speed improvement (likely because the libraries used are already internally parallelized), and some calls to the TensorFlow library (e.g., writing to TFRecords) did not work and would need to be locked into critical sections. Therefore, the parallelization was removed. The output TFRecords files for httpd\_1 are \textasciitilde15MB.

\begin{lstlisting}[
    language=python, 
    label={listing:convert-id}, 
    float=t,
    caption={An example of Python code that converts Joern node IDs into TFGNN IDs.}
]
 for edge_set_name, val in edgeset_info.items():
     source_nodeset=val['SOURCE']
     target_nodeset=val['TARGET']

     get_source_node_loc = lambda id: graph_in_dfs[source_nodeset].index.get_loc(id)
     get_target_node_loc = lambda id: graph_in_dfs[target_nodeset].index.get_loc(id)

     graph_in_dfs[edge_set_name]['source']=graph_in_dfs[edge_set_name] \      
       ['source'].apply(get_source_node_loc)
     graph_in_dfs[edge_set_name]['target']=graph_in_dfs[edge_set_name] \
       ['target'].apply(get_target_node_loc)
\end{lstlisting}

\begin{table}
    \centering
    \caption{The table shows the number of samples after each phase of the training pipeline. For values marked with *, the loss is not final as they were not transformed into TFRecords. However, the table indicates that this final transformation is almost lossless.}
    \vskip6pt
    \begin{tabular}{!{\vrule width 1pt}l!{\vrule width 1pt}r|r|r|r|r!{\vrule width 1pt}r!{\vrule width 1pt}}
        \noalign{\hrule height 1pt}
        {\small \textbf{Project}} & {\small \textbf{D2A}} & {\small \textbf{Filtered D2A}} & {\small \textbf{Bitcode}} & {\small \textbf{Graph D2A}} & {\small \textbf{TFRecords}} & {\small \textbf{Loss}} \\
        \noalign{\hrule height 1pt}
        httpd\_0 & 12475 & 11974 & 11818 & 9705 & 9705 & 22 \% \\ \hline
        httpd\_1 & 217 & 210 & 210 & 193 & 193 & 11 \% \\ \hline
        nginx\_0 & 17945 & 17209 & 17172 & 16741 & 16741 & 7 \% \\ \hline
        nginx\_1 & 421 & 418 & 417 & 407 & 407 & 3 \% \\ \hline
        libav\_0 & 236415 & 234062 & 226213 & 186614 & 186595 & 21 \% \\ \hline
        libav\_1 & 4614 & 4575 & 4398 & 3331 & 3331 & 28 \% \\ \hline
        libtiff\_0 & 12096 & 11385 & 11377 & 9276 & 9276 & 23 \% \\ \hline
        libtiff\_1 & 553 & 534 & 534 & 459 & 459 & 17 \% \\ \hline
        openssl\_0 & 343148 & 332584 & 301934 &  278292 & - & 20 \%* \\ \hline
        openssl\_1 & 8022 & 7913 & 7581 & 6918 & - & 14 \%* \\ \hline
        ffmpeg\_0 & 654891 & 649255 & 633997 & 500791 & - & 24 \%* \\ \hline
        ffmpeg\_1 & 4826 & 4772 & 4621 & 3938 & - & 18 \%* \\ \hline
        \noalign{\hrule height 1pt}
    \end{tabular}
    \label{tab:implementation-d2a}
\end{table}


\section{Model Training Script}
\label{implementation-training}
After creating TFRecords files, training of GNN models can be done using the script \texttt{mixed\_nodes\_model.py} for Python 3.8. This script takes 4 position-dependent arguments:
\begin{enumerate}
    \item TFGNN schema,
    \item directory with TFRecords,
    \item output directory for saving models,
    \item (optional) the value \texttt{combined} to train a single model across multiple projects (see Section~\ref{hyperparameter-tuning}); if omitted, a separate model is trained for each project.
\end{enumerate}

The script \texttt{mixed\_nodes\_model.py} trains models on training data (and validates on validation data) of the projects httpd, libtiff, and nginx. It expects files named according to the pattern:
\begin{lstlisting}[language=bash, xleftmargin=2em]
{TFRecords_dir}/{httpd|libtiff|nginx}_{0|1}.tfrecords.{train|val}
\end{lstlisting}
The script then operates as follows:
\begin{enumerate}
    \item Data are loaded using \texttt{tf.data.TFRecordDataset} -- positive and negative samples separately (validation data are loaded all at once, as shuffling is not necessary).
    \item Up-sampling is applied to the minority class.
    \item Positive and negative samples are interleaved.
    \item All samples are shuffled to mix the samples from the individual projects.
    \item Datasets are batched.
    \item A \texttt{preprocessing} model is applied, which extracts the labels (see Section~\ref{gnn-model}).
    \item The function \texttt{train\_model} is then called, performing:
    \begin{enumerate}
        \item First, a model is constructed using the \texttt{build\_model} function, which utilizes the Keras API\footnote{\textbf{Keras API}'s documentation: \url{https://www.tensorflow.org/guide/keras}.} and operates as follows:
        \begin{enumerate}
            \item An \texttt{Input} layer is created, taking graphs defined by the TFGNN schema as input.
            \item A \texttt{Dense} layer initializing hidden states for each node set (and optionally edge sets) is added, using \texttt{MapFeatures}.
            \item GNN layers \texttt{mt\_albis.MtAlbisGraphUpdate} are added.
            \item A \texttt{Pool} layer is added.
            \item \texttt{Dense} layers in the GNN head are added, combining context features and the output of the \texttt{Pool} layer.
            \item Finally, a \texttt{Dense} layer with a single output and sigmoid activation function is added.
        \end{enumerate}
        \item The loss function, metrics, and optimizer are set, and the model is compiled using \texttt{model.compile}.
        \item An \texttt{EarlyStopping} callback monitoring Area Under the Receiver Operating Characteristic Curve (AUROCC) (see Section~\ref{base-model}) on validation data is set.
        \item Finally, the training loop is initiated using \texttt{model.fit}.
    \end{enumerate}
    \item Thanks to the \texttt{EarlyStopping} callback, the output of the training is the model with the highest validation AUROCC found. This model (or models) is then saved to the output directory. Directories with models are automatically saved with the prefix \texttt{\{ID\}\_}, where \texttt{ID} is a unique number -- the largest found in the directory, increased by one. The directory name might look like \texttt{8\_AUC\_0.818}, where the AUC value specifically refers to the validation AUROCC (or their average in case of multiple models). The values of hyperparameters set in the dictionary \texttt{hyperparameters} are stored in the output folder in the file \texttt{hyperparameters.json}.

\end{enumerate}

The entire model architecture is defined in the \texttt{build\_model} function. Older versions of this script for earlier models can be found in the repository under commits named \texttt{Model \{ID\} - AUC 0.XYZ}. These historical versions, though executable, do not represent the final form of the training script and should only be used for insight into the architecture definition.

\section{Model Evaluation Script}
\label{implementation-evaluation}
Trained models can be evaluated using the \texttt{evaluate\_model.py} script for Python 3.8. The models are evaluated based on two metrics -- AUROCC and Top N~\% Precision (see Section~\ref{model-comparison}). Examples of these for the libtiff project and top-performing models are shown in Figure~\ref{figure:auc-libtiff} and Figure~\ref{figure:topn-libtiff}, respectively. The script accepts 5 position-dependent arguments:
\begin{enumerate}
    \item TFGNN schema,
    \item directory with TFRecords files,
    \item directory with saved models,
    \item model \texttt{ID},
    \item dataset type -- \texttt{test}, \texttt{val}, or \texttt{train}.
\end{enumerate}

The script initially loads the data in the TFRecords format from the same location and with the same naming convention as used by \texttt{mixed\_nodes\_model.py} (see Section~\ref{implementation-training}), with additional \texttt{*.test} files. No data shuffling or modifications are required for the evaluation. A~preprocessing model is applied to extract labels from the graphs. Then, the model (or models) is loaded using \texttt{tf.keras.models.load\_model}, and inference is performed on the data using \texttt{model.predict}. The results are provided to the \texttt{plot\_top\_N\_precision} function, which plots the precision dependency on the number of top-selected samples. Additionally, the \texttt{plot\_ROC\_curve} function is called to create ROC curves. Both graphs are displayed and also saved in the current directory under the names \texttt{ROC\_curves.svg} and \texttt{Top\_N\_precisions.svg}.

The script can be run in a special mode that creates graphs for predefined scenarios (all graphs in Chapter~\ref{experiments} were created using these scenarios), by passing the following special model \texttt{ID} values (4th argument):
\begin{enumerate}
    \item \texttt{combined} -- testing top performing models from Section~\ref{model-comparison} on combined data from the httpd, libtiff, and nginx projects.
    \item \texttt{httpd} -- testing top performing models on httpd.
    \item \texttt{libtiff} -- testing top performing models on libtiff.
    \item \texttt{nginx} -- testing top performing models on nginx.
    \item \texttt{libav} -- testing top performing models on libav -- this involves cross-analysis. Files \texttt{libav\_\{0|1\}.tfrecords.\{train|test|val\}} are required.
    \item \texttt{chatgpt} -- comparing the top performing model with ChatGPT4 (see Section~\ref{comparison-chatgpt}). The file \texttt{libtiff-chatgpt.tfrecords.test}, containing selected samples, is needed.
\end{enumerate}



\section{Compiler Wrapper}
\label{implementation-compiler-wrapper}
The compiler wrapper originates from the author's previous work~\cite{bc}, where its implementation is also described. Here, only a brief overview will be provided, focusing mainly on its inputs and outputs for integration with other parts of the inference pipeline.

The compiler wrapper is a bash script that replaces C/C++ compilers, and the original compiler binaries are renamed to \texttt{\{compiler\}-original} (e.g., \texttt{gcc} becomes \texttt{gcc-original}). The repository includes a \texttt{Makefile}\footnote{\textbf{Makefile for installing wrappers}: \url{https://github.com/TomasBeranek/but-masters-thesis/blob/thesis-submission/inference-pipeline/Makefile}.} for installing wrappers for many commonly used C/C++ compilers.

The wrapper works by intercepting all commands that would go to the original compilers, and:
\begin{enumerate}
    \item passes them to Infer for analysis,
    \item generates LLVM bitcode,
    \item and finally forwards them to the original compilers.
\end{enumerate}

The wrapper stores information in the \texttt{/tmp/infer-out} directory, which is generated directly by Infer. It contains the results of Infer's analysis and also a list of \texttt{.bc} files found before the first generation of LLVM bitcode. These existing \texttt{.bc} files are not generated by the wrapper (or are outdated). Their list is stored in the \texttt{/tmp/infer-out/old\_bc\_files.txt} file. This step needs to be optimized in future versions because it can be slow on large filesystems.



\section{Inference Pipeline}
\label{implementation-inference-pipeline}
Unlike the training pipeline, which is implemented as a series of standalone tools, the inference pipeline is fully automated. The inference pipeline uses the existing scripts \texttt{construction\_phase\_d2a} (see Section~\ref{implementation-graph-construction}), \texttt{feature\_engineering.py} (see Section~\ref{implementation-feature-engineering}), and compiler wrappers (see Section~\ref{implementation-compiler-wrapper}). The inference pipeline is a bash script named \texttt{inference\_pipeline}, which combines the previously mentioned scripts and provides additional functionality, particularly data conversion into formats expected by the already created scripts. The \texttt{inference\_pipeline} script should be called in the following context:
\begin{enumerate}
    \item First, the compiler wrappers need to be installed.
    \item Then, the analyzed project needs to be compiled (anywhere in the filesystem).
    \item After the compilation is complete, \texttt{inference\_pipeline} is called with a single parameter that specifies the output directory, for example:
\begin{lstlisting}[language=bash, xleftmargin=2em]
./inference_pipeline ./
\end{lstlisting} 
    \item Finally, it is advisable to uninstall the compiler wrappers.
\end{enumerate}

The \texttt{inference\_pipeline} script itself works as follows:
\begin{enumerate}
    \item First, Infer analysis is run on the \texttt{/tmp/infer-out} directory created by the compiler wrapper (see Section~\ref{implementation-compiler-wrapper}).
    \item All \texttt{.bc} files are found in the filesystem, and those that have been added compared to the \texttt{/tmp/infer-out/old\_bc\_files.txt} list created by the compiler wrapper at the start of the compilation are identified.
    \item Using \texttt{llvm-link}, all new LLVM bitcode files are merged into a single file named \texttt{/tmp/infer-out/combined.bc}.
    \item Next, the \texttt{slicing\_criteria\_extraction.py} script (see Section~\ref{implementation-slicing-info}) is executed to extract slicing criteria from the Infer output -- the \texttt{/tmp/infer-out/report.json} file.
    \item The \texttt{/tmp/infer-out/bitcode} directory with the LLVM bitcode must then be prepared as expected by the \texttt{construction\_phase\_d2a} script. Since there is only a~single \texttt{combined.bc} file for the entire project, an artificial directory is created and populated with symlinks that all point to the \texttt{combined.bc}. The symlinks are named according to the IDs of individual Infer reports (see Section~\ref{implementation-slicing-info}).
    \item Now, \texttt{construction\_phase\_d2a} can be executed as follows:
\begin{lstlisting}[language=bash, xleftmargin=2em]
../dataset/construction_phase_d2a /tmp/infer-out/raw-ecpg \
/tmp/infer-out/slicing_info.csv /tmp/infer-out/bitcode
\end{lstlisting}
    \item The generated raw ECPGs are then processed by \texttt{feature\_engineering.py} in inference mode -- the input consists of exactly 4 position-dependent arguments: specifically, the TFGNN Schema, the name of the output \texttt{.tfrecords} file, Infer analysis results in \texttt{report.json}, and slicing criteria in CSV. The script is called as follows:
\begin{lstlisting}[language=bash, xleftmargin=2em]
find /tmp/infer-out/raw-ecpg -mindepth 1 -type d | python3.8 \
../model/schemas/feature_engineering.py \
../model/schemas/mixed_nodes/extended_cpg.pbtxt \
/tmp/infer-out/graphs.tfrecords /tmp/infer-out/report.json \
/tmp/infer-out/slicing_info.csv
\end{lstlisting}
    \item The final step is to call the \texttt{model\_inference.py} script (described below) as follows:
\begin{lstlisting}[language=bash, xleftmargin=2em]
python3.8 model_inference.py \
../model/schemas/mixed_nodes/extended_cpg.pbtxt /tmp/infer-out/ \
graphs.tfrecords \
../model/saved_models/8_AUC_0.818/combined_AUC_0.818 \
/tmp/infer-out/report.json /tmp/infer-out/ranked_report.json
\end{lstlisting}
    The output of this script is the \texttt{/tmp/infer-out/ranked\_report.json} file, which contains the sorted reports from \texttt{report.json} according to the score from the GNN model.
\end{enumerate}

The \texttt{model\_inference.py} script applies the GNN model to the graphs, saved in the \texttt{graphs.tfrecords} file, and ranks individual reports from \texttt{report.json} according to the obtained scores. The script is a modified version of the \texttt{evaluate\_model.py} script (see Section~\ref{implementation-evaluation}). Its input consists of 5 positional arguments:
\begin{enumerate}
    \item TFGNN schema,
    \item graphs in \texttt{.tfrecords} format,
    \item directory containing the GNN model,
    \item Infer output in \texttt{report.json},
    \item result file name.
\end{enumerate}

Since the current models have not yet achieved significant results in the area of cross-analysis, the inference pipeline remains unused for now. Therefore, it has not been tested on real projects. However, the only project-specific part is the compiler wrapper, which was thoroughly tested on a range of real software in the author's bachelor's thesis~\cite{bc}, and before its incorporation into the \texttt{csmock} tool~\cite{CsmockAVM, CsmockFLOCK}, the functionality of the wrapper was tested on 55 randomly selected SRPM packages in the C language. The runtime of the inference pipeline depends primarily on the time taken for Infer analysis and the individual parts of the pipeline, which were described in previous chapters.


\chapter{Experimental Evaluation}
\label{experiments}bud
This chapter describes the experimental evaluation of the developed GNN models for ranking reports from Meta Infer based on the probability of being a true positive. Specifically, Section~\ref{base-model} provides a detailed description of the architecture and hyperparameters of the base model, from which other models are derived. Section~\ref{hyperparameter-tuning} discusses and evaluates the modifications of the base model on validation data. Section~\ref{model-comparison} compares the best developed models with existing models on test data. Section~\ref{comparison-chatgpt} compares the best developed GNN model with the \textit{large language model} ChatGPT. Section~\ref{cross-analysis} evaluates the developed models on cross-analysis. Finally, Section~\ref{summary} summarizes and discusses the achieved results, and also describes possible future improvements.


\section{Base Model}
\label{base-model}
The general architecture and its main components used in the following models were already described in Section~\ref{gnn-model}. Therefore, only supplementary information will be provided here, describing the specific architecture and hyperparameters of the base model -- the model from which all other models mentioned in Section~\ref{hyperparameter-tuning} are derived. The descriptions of basic machine learning concepts throughout this chapter, such as \textit{loss function}, \textit{dropout}, \textit{binary cross entropy}, etc., are taken from~\cite{book-deep-learning}, where they are discussed in detail and are only briefly mentioned here, as they are used in their conventional forms.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/base-model.png}
    \caption{The figure shows the architecture of the base model (\texttt{Model 1}), which forms the basis for all other models developed in this thesis.}
    \label{figure:base-model}
\end{figure}

The architecture of the base model is shown in Figure~\ref{figure:base-model}. The layer initializing hidden states is of type \texttt{Dense(16)} (i.e., a densely-connected neural network layer with 16 outputs) with an activation function \texttt{relu}, for each node set. This is followed by 8 GNN layers of the type \texttt{MtAlbis}. Initial parameters were chosen primarily based on~\cite{tfgnn-mtalbis} and examples in the TFGNN repository\footnote{\textbf{TFGNN}'s repository: \url{https://github.com/tensorflow/gnn}.}. Parameters such as \texttt{units} and \texttt{message\_dim} were selected considering the batch size and GPU memory size. Some initial parameters were chosen randomly and for parameters not mentioned here, default values were retained. All (except for the last) \texttt{MtAlbis} layers share the same parameters, which are:
\begin{itemize}
    \item \texttt{units=16} -- size of the hidden states.
    \item \texttt{message\_dim=16} -- size of the messages on the edges.
    \item \texttt{receiver\_tag=tfgnn.TARGET} -- specifies the direction of message passing, here it is along the direction of the edges (\texttt{tfgnn.SOURCE} would be in the opposite direction).
    \item \texttt{node\_set\_names=None} -- updates nodes of all node set types. For the last layer, the value is set to \texttt{AST\_NODE} in order to modify only nodes from the \texttt{AST\_NODE} node set from which the following pooling layer reads. Hidden states of other node sets are discarded by the pooling layer, making it unnecessary to update their values in the last round of message passing.
    \item \texttt{state\_dropout\_rate=0.1} -- the dropout rate applied to the pooled and combined messages from all edges.
    \item \texttt{simple\_conv\_reduce\_type='mean|sum'} -- the type of message aggregation.
    \item \texttt{next\_state\_type=residual} -- can be set to \texttt{dense} or \texttt{residual}, where \texttt{residual} adds a residual connection from the old to the new node state.
\end{itemize}

Following the \texttt{MtAlbis} layers is a \texttt{Pool} layer of type \texttt{max}, which reads hidden states only from nodes of the \texttt{AST\_NODE} node set. Here, for example, it could read from the root of the AST tree, where information would accumulate when changing the orientation of the \texttt{AST} edges (as mentioned in Section~\ref{feature-engineering}), but this would require the tree depth to be equal to or less than the number of GNN layers so that information from leaf nodes could reach the root. Since the depth varies and ECPGs are not just trees, it utilizes all \texttt{AST\_NODE} nodes whose information is eventually aggregated using the \texttt{Pool} layer.

The context features, along with the output of the \texttt{Pool} layer, are inputs to the \texttt{Dense(1)} layer with the \textit{sigmoid activation function}, which transforms the values into the range $(0,1)$. The base model uses the \texttt{Adam} \textit{optimizer} with a learning rate of \texttt{0.000002}. The loss function used is \texttt{BinaryCrossentropy}. The model is trained in batches of size \texttt{11} (limited due to GPU memory) over \texttt{300} epochs, where the number of steps per epoch is the dataset size divided by batch size. The model employs \texttt{EarlyStopping} with \texttt{patience} set at \texttt{20} (thus, the models are not trained for 300 epochs but only for tens of epochs, see Section~\ref{hyperparameter-tuning}). The base model is trained separately on each project from D2A -- this represents a form of \textit{3-fold validation}.

Although the architecture is trained for binary classification, the goal of the models is ranking, not classification. Therefore, it does not make sense to monitor separate metrics such as \textit{precision}, \textit{recall}, or \textit{accuracy} in this case, since these are designed specifically for classification. These metrics are also not suitable for unbalanced data. The metric that appropriately reflects ranking and can be used for unbalanced data is the Area Under the Receiver Operating Characteristic Curve (AUROCC). The ROCC~\cite{aurocc} plots the \textit{True Positive Rate} (i.e., recall) on the Y-axis and the \textit{False Positive Rate} on the X-axis for each classification threshold (previously mentioned metrics use only a single threshold), thereby clearly describing the ranking ability of the model. The True Positive Rate is defined as (here true positives and false positives relate to the model, not to the results of Infer):
\begin{equation*}
True\:Positive\:Rate\:(TPR) = \frac{True\:Positives\:(TP)}{True\:Positives\:(TP) + False\:Negatives\:(FN)}
\end{equation*}
The False Positive Rate is then defined as:
\begin{equation*}
False\:Positive\:Rate\:(FPR) = \frac{False\:Positives\:(FP)}{False\:Positives\:(FP) + True\:Negatives\:(TN)}
\end{equation*}

AUROCC for a random model is 0.5 (indicated by a dashed line in all subsequent figures, see Figure~\ref{figure:auc-combined}) and for a perfect model is 1 (a model that can perfectly separate the classes). For the reasons mentioned above, AUROCC on validation data is monitored for early stopping in all trained models -- the models are thus trained to achieve the highest possible AUROCC.

\section{Hyperparameters Tuning}
\label{hyperparameter-tuning}
After the base model is created, it is necessary to tune its hyperparameters to adapt it to the specific task -- in this case, the ranking of reports from Infer. If, even after tuning the hyperparameters, the architecture does not yield satisfactory results, a different architecture is typically tried. As will be evident from the results below, the architecture of the base model achieved very good results during the hyperparameter tuning process. Therefore, there was an effort to optimally tune this architecture. Due to limited computing resources, it was not possible to use automated hyperparameter tuning, which typically involves training many models with different settings. Thus, a manual approach had to be used, which requires fewer computing resources compared to automatic tuning but relies on experience and knowledge in the field.

A total of 14 models were trained, the results on validation data and their number of parameters are in Table~\ref{tab:tuning}. The models are trained and tested, due to limited computing resources, only on the smallest projects -- httpd, libtiff, and nginx. The AUROCC of each model is either the average validation AUROCC across the individual projects or the validation AUROCC on a set of validation data composed of all the tested projects, as detailed below.

The following description includes a list of changes for each model compared to the previous model:
\begin{enumerate}
    \item \texttt{Model 1} -- base model, described in Section~\ref{base-model}.
    \item \texttt{Model 2} -- increased network complexity -- hidden state size increased to 18, an additional \texttt{MtAlbis} layer added, a \texttt{Dense(4)} added for context features, and a \texttt{Dense(8)} layer added before the final \texttt{Dense(1)} layer. The increase in complexity required reducing the batch size to 6. Also, the state dropout was increased to 0.15.
    \item \texttt{Model 3} -- reduced network complexity -- decreased the size of hidden states to 12 but increased the batch size to 10.
    \item \texttt{Model 4} -- reducing complexity led to much worse results, so complexity was further increased -- hidden state size increased to 20 at the cost of removing one \texttt{MtAlbis} layer and reducing the batch size to 6. A \texttt{Dropout(0.15)} layer was also added right after the \texttt{Pool} layer. The learning rate was decreased to 0.000001.
    \item \texttt{Model 5} -- added one \texttt{MtAlbis} layer (total of 9) at the cost of reducing the size of hidden states to 18.
    \item \texttt{Model 6} -- instead of training 3 models for each project, \texttt{Model 6} (and all subsequent models) is trained on all 3 projects at once (combining their training and validation sets). The learning rate was substantially increased to \texttt{0.0001}, and the \texttt{Dropout} layer was removed from the GNN head.
    \item \texttt{Model 7} -- increased state dropout to 0.2, tried only \texttt{mean} for \texttt{simple\_conv\_reduce}, and switched to \texttt{dense} for \texttt{next\_state\_type}.
    \item \texttt{Model 8} -- since \texttt{Model 7} experienced a significant drop in AUROCC, \texttt{Model 6} was restored. Only the state dropout was kept at 0.2 and an L2 regularization was added with a value of \texttt{0.00001} since the training AUROCC for \texttt{Model 6} was nearly 0.95 -- the model manages to learn on training data, now it needs to better generalize.
    \item \texttt{Model 9} -- set edge dropout (in \texttt{MtAlbis} layers) to 0.2.
    \item \texttt{Model 10} -- the edge dropout led to a significant deterioration, so it was set back to 0. However, the state dropout was increased to 0.25, and the learning rate was decreased to 0.00005.
    \item \texttt{Model 11} -- again tried the so-far best \texttt{Model 8} but in a bi-directional mode -- the direction of message passing in the \texttt{MtAlbis} layers is alternated using the \texttt{receiver\_tag} parameter (see Section~\ref{base-model}).
    \item \texttt{Model 12} -- again tried \texttt{Model 8} and utilized edge features -- \texttt{ARGUMENT\_INDEX} (until now all edge features were ignored).
    \item \texttt{Model 13} -- again tried \texttt{Model 8} but with \textit{attention} -- trainable message aggregation in \texttt{MtAlbis} layers. Used type \texttt{gat\_v2} with 3 attention heads (4 are default, but the size of the hidden state -- 18 -- must be divisible by the number of attention heads).
    \item \texttt{Model 14} -- slightly increased the state dropout to 0.22.
\end{enumerate}

The models have only been briefly described. Their source files can be found in the GitHub repository in commits labeled as, for example, \texttt{Model 8 - AUC 0.818}\footnote{Source code of \texttt{Model 8}: \url{https://github.com/TomasBeranek/but-masters-thesis/blob/fdcaa8e5f896d50c9b55a616cea84d56a058d45f/model/src/mixed_nodes_model.py}.} (the best-performing model). However, these historical versions of the training script (described in Section~\ref{implementation-training}) were in the development stage and should only serve as a reference for the definition of the model architectures.

From Table~\ref{tab:tuning}, it is evident that the best performing models are \texttt{Model 8}, \texttt{Model 13}, and \texttt{Model 10}, respectively. All these models are very small -- with less than 140 thousand parameters (which is about \textasciitilde500KB on disk) -- yet they achieve very good results. These models were trained (hardware specifications used are in Chapter~\ref{implementation}) for 69 epochs (\textasciitilde460s per epoch), 7 epochs (\textasciitilde1,350s per epoch), and 73 epochs (\textasciitilde450s per epoch), respectively.

\begin{table}
    \centering
    \caption{The table shows the results of hyperparameter tuning and the size of each model. Validation data from httpd, libtiff, and nginx projects were used for the evaluation.}
    \vskip6pt
	\begin{tabular}{
        !{\vrule width 1pt}l!{\vrule width 1pt}c|c!{\vrule width 1pt}}
        \noalign{\hrule height 1pt}
        Model & Parameters & AUROCC \\
        \noalign{\hrule height 1pt}
        Model 1& 96,515  & 0.630 \\
        Model 2& 137,499  & 0.668 \\
        Model 3& 61,941 & 0.557 \\
        Model 4& 150,093 & 0.607 \\
        Model 5& 137,499 & 0.598 \\
        Model 6& 137,499  & 0.787 \\
        Model 7& 106,071  & 0.632 \\
        \rowcolor{green1} Model 8& 137,499  & 0.818 \\
        Model 9& 137,499  & 0.775 \\
        \rowcolor{green3} Model 10& 137,499 & 0.793 \\
        Model 11& 140,523  & 0.786 \\
        Model 12& 140,451  & 0.788 \\
        \rowcolor{green2} Model 13& 109,563  & 0.816 \\
        Model 14& 109,563  & 0.746 \\
        \noalign{\hrule height 1pt}
    \end{tabular}
    \label{tab:tuning}
\end{table}


\section{Models Comparison}
\label{model-comparison}
As previously mentioned, this thesis compares with the models developed in~\cite{D2A-zheng2021d2a, pujar2024analyzing} which also focus on reducing false positives reported by Infer. For this comparison, the three best-performing models on the validation data, specifically \texttt{Model 8}, \texttt{Model 10}, and \texttt{Model 13}, were selected based on Table~\ref{tab:tuning}. Additionally, a \texttt{3-soft-vote} model was created, which ranks based on a \textit{soft score} -- the sum of the scores from the three top-performing models. Moreover, a \texttt{6-soft-vote} model comprising the six top-performing models (\texttt{Model 6}, \texttt{8}, \texttt{10}, \texttt{11}, \texttt{12}, and \texttt{13}) was also created. It is important to note that the models are not compared on identical test sets as the Graph D2A contains fewer samples than the original D2A dataset due to:
\begin{enumerate}
    \item Support for only certain error types (see Section~\ref{bitcode-generation}). The number of unsupported samples is \textasciitilde1.6~\% of the total D2A samples, thus minimally influencing the results.
    \item The inability to generate ECPG from some D2A samples. These cases are significantly more frequent and could more substantially impact the results. Their quantity varies depending on the project, ranging from 3~\% to 23~\% for tested projects (see Table~\ref{tab:implementation-d2a}).
\end{enumerate}

Table~\ref{tab:comparison} presents a comparison of the \texttt{Model 8}, \texttt{Model 10}, \texttt{Model 13}, \texttt{3-soft-vote}, and \texttt{6-soft-vote} developed in this thesis with the existing \texttt{vote}, \texttt{c-bert}, and \texttt{vote-new} models from~\cite{D2A-zheng2021d2a, pujar2024analyzing}. The models are compared based on AUROCC on the test data, which is the only common metric across all models. The comparison is only shown for the projects httpd, libtiff, and nginx due to limited computational resources. The \texttt{vote}, \texttt{c-bert}, and \texttt{vote-new} models are trained on training and validation data, whereas the models developed in this thesis are trained only on training data with validation data used for early stopping.

\begin{table}
    \centering
    \caption{A comparison of the existing models \texttt{vote}, \texttt{c-bert}, and \texttt{vote-new} with the models developed in this thesis. The comparison criterion is AUROCC on test data.}
    \vskip6pt
	\begin{tabular}{
        !{\vrule width 1pt}l!{\vrule width 1pt}c|c|c!{\vrule width 1pt}}
        \noalign{\hrule height 1pt}
        \textbf{Model} & \textbf{httpd} & \textbf{libtiff} & \textbf{nginx} \\
        \noalign{\hrule height 1pt}
        vote & 0.77 & 0.89 & 0.77 \\ \hline
        c-bert & \cellcolor{green3} 0.82 & 0.94 & 0.89 \\ \hline
        vote-new & \cellcolor{green1} 0.90 & \cellcolor{green1} 0.98 & \cellcolor{green3}0.93 \\ \hline
        \noalign{\hrule height 1pt}
        Model 8 & 0.80 &  \cellcolor{green3} 0.95 & \cellcolor{green2}0.94 \\ \hline
        Model 10 & 0.79 & 0.91  & 0.91 \\ \hline
        Model 13 & 0.74 &  0.87 & 0.83 \\ \hline
        3-soft-vote & 0.80 &  \cellcolor{green2} 0.96 & \cellcolor{green1}0.95 \\ \hline
        6-soft-vote & \cellcolor{green2} 0.83 & \cellcolor{green2} 0.96 \cellcolor{green2} & \cellcolor{green2} 0.94 \\ \hline
        \noalign{\hrule height 1pt}
    \end{tabular}
    \label{tab:comparison}
\end{table}

From Table~\ref{tab:comparison}, it is evident that the developed GNN models can match or even surpass the state-of-the-art models, especially for nginx. However, the results for httpd are lower, likely due to a lack of data. As indicated in Table~\ref{tab:tuning}, models using a combined training set (\texttt{Model 6} and above) achieve significantly better results. It is possible that compared to existing models, these GNNs require more training data. The httpd project has the fewest samples in the original D2A dataset, and an additional \textasciitilde22~\% of samples were removed when generating Graph D2A from httpd, which greatly complicates learning.

However, models can also be compared from other perspectives, such as their size, which relates to the inference speed. All existing solutions are closed source, making it impossible to determine their sizes. Similarly, it is not possible to verify their results, experiment with the models, or use them. Hence, the models developed in this thesis are a promising open source alternative.

Specific ROC curves for the developed models on the combined test sets can be seen in Figure~\ref{figure:auc-combined}. ROC curves for individual projects are provided in the appendices in Figure~\ref{figure:auc-httpd} (httpd), Figure~\ref{figure:auc-libtiff} (libtiff), and Figure~\ref{figure:auc-nginx} (nginx).

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.75\textwidth]{figures/auc-combined.png}
	\caption{The figure shows the ROC curves for the top-performing models developed in this thesis, evaluated on a combined test set from the httpd, libtiff, and nginx projects.}
	\label{figure:auc-combined}
\end{figure}

The intended use case for these models is to rank Infer reports by likelihood of being a~real error. Developers would then typically check only the most promising reports -- for example, the top 5~\% (the same value was chosen in~\cite{pujar2024analyzing}). Consider now the best-performing model (on average), \texttt{6-soft-vote}, which is deployed on the test data of the libtiff project. The percentage of real errors (equivalent to $precision * 100$) in the libtiff project test data in Graph D2A is \textasciitilde4.7~\% (see Table~\ref{tab:implementation-d2a}). This number remains unchanged (on average) if a~random 5~\% of samples are checked -- equivalent to ranking by a random model. However, if the top 5~\% of samples according to the \texttt{6-soft-vote} model are selected, the amount of true positives increases to \textasciitilde57.1~\%. In terms of the number of samples -- in unsorted reports, there will be on average 2.3 real errors for every 49 checked reports. In the sorted reports, 28 real errors will be found for the same number of checked reports, which is more than 13 times as many.

Graphs showing precision values for different percentages of top samples and for top-performing models are presented in Figure~\ref{figure:topn-combined} (combined data from all projects), Figure~\ref{figure:topn-httpd} (httpd), Figure~\ref{figure:topn-libtiff} (libtiff), and Figure~\ref{figure:topn-nginx} (nginx). This metric becomes more sensitive as fewer top samples are considered.


\section{Comparison with ChatGPT}
\label{comparison-chatgpt}
In recent years, Large Language Models (LLMs) such as ChatGPT have become increasingly recognized by both professionals and the general public. ChatGPT can respond to textual inputs (and in version 4, even, e.g., image inputs) with textual outputs (again in version 4, even, e.g.,  image outputs). The introduction of ChatGPT~\cite{chatgpt} demonstrates the model's capabilities to search for and correct errors in code. The ability of ChatGPT (especially version 4) to handle programming tasks compared to other LLMs is discussed in~\cite{coello2024effectiveness}, where ChatGPT4 is shown to be particularly effective. These results raise the question of how ChatGPT might perform in reducing false reports.

For the experiment, the \texttt{6-soft-model}, which on average achieved the best results in Section~\ref{model-comparison}, was used. Ten true positives and ten false positives were randomly selected from the test set of the libtiff project. These samples, in their original JSON format (without class information), were submitted to a modified version of ChatGPT4 that could interpret code and search the internet, with the following instructions:
\begin{lstlisting}[language=bash, xleftmargin=2em]
Behave like a binary classification model. You will receive a sample 
from the D2A dataset, containing reports from Meta Infer static 
analyzer. Your goal is to output a number in the range <0,1>. The 
higher the number, the more certain you are that the report from Infer
is true. Individual samples contain the report itself, the location of 
the error, codes of functions related to the error, and other useful 
information.
\end{lstlisting}

From Figure~\ref{figure:auc-chatgpt}, which displays the ROC curves for the scores from \texttt{6-soft-vote} and from ChatGPT version 4, it is apparent that ChatGPT4 exhibits random behavior in terms of ranking these selected samples. In contrast, \texttt{6-soft-vote} achieves a perfect score -- distinguishing between the classes perfectly. When comparing the models in terms of their size, \texttt{6-soft-vote} again prevails with only 800 thousand parameters (comprising 6 sub-models), while ChatGPT4 has approximately 1.76 trillion parameters~\cite{chatgpt-params}. However, it is important to note that ChatGPT is a general-purpose model and not a classifier specifically designed for ranking reports from static analysis.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.75\textwidth]{figures/auc-chatgpt.png}
	\caption{The figure shows ROC curves comparing \texttt{6-soft-vote} and ChatGPT4, on a randomly selected (with balanced classes) 20 samples from the test data of the libtiff project.}
	\label{figure:auc-chatgpt}
\end{figure}

\section{Cross-analysis}
\label{cross-analysis}
The ultimate goal of all models in the field of static analysis report filtering/ranking, based on the likelihood of being true positive, is to function on cross-analysis. Existing models \texttt{vote}, \texttt{c-bert}, and \texttt{vote-new} from~\cite{D2A-zheng2021d2a, pujar2024analyzing} are designed only for self-analysis -- the model is trained and tested on the same project. However, the self-analysis is also useful in practice, especially for large projects with extensive git histories that can be used to train models. 

A primary objective of feature engineering in Section~\ref{feature-engineering} was to eliminate information that could lead the models to overfit to a specific project. To test cross-analysis capabilities, the top performing models were tested on the test data of the libav project. The results in Figure~\ref{figure:auc-libav} indicate that the models, on average, exhibit random behavior. The developed models, like all existing ones, thus fail to function on cross-analysis, which represents a~significant challenge in this research field.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.75\textwidth]{figures/auc-libav.png}
	\caption{The figure shows ROC curves for the top performing models developed in this thesis in cross-analysis mode. The models were trained on the httpd, libtiff, and nginx projects and tested on the test data of the libav project.}
	\label{figure:auc-libav}
\end{figure}

\section{Summary and Future Work}
\label{summary}
From the experiments in this chapter, it is evident that GNNs, and specifically the models developed in this thesis, are suitable for ranking reports from the Meta Infer static analyzer. The created models were able to match best existing solutions in this area that we are aware of, which were developed by strong industrial team from IBM. In the case of the nginx project, the existing models were even surpassed. However, results on the httpd project were weaker, which may be due to a lack of data for the httpd project, which is not only the smallest project in terms of the number of samples in the D2A dataset but also experienced high sample losses during the generation of Graph D2A. Nonetheless, we believe that these results demonstrate that the developed models are a promising open-source alternative to existing solutions, which are unfortunately all closed source.

The best-performing model, \texttt{6-soft-vote}, was also compared with the LLM model ChatGPT version 4. The model developed in this thesis proved superior with a perfect score, in contrast to ChatGPT4, which was unable to differentiate between false and real reports.

The models were also tested on a cross-analysis. None of the models were able to correctly distinguish false reports from real ones. Cross-analysis thus emerges as an unexpectedly challenging problem in this research area as no existing model that we are aware of functions effectively on cross-analysis either.

Future work should focus on testing self-analysis on all projects in the D2A dataset, which, however, requires training on a large number of samples and thus needs significant computational resources. Considering the results of experiments on the httpd project, it would be necessary to focus on improving the training pipeline to avoid such high sample losses. Specifically, it would be beneficial to increase the timeout for the LLVM Slicer tool and to focus on improving the success rate of LLVM bitcode extraction. Particularly, it should focus on the minority class -- true positives (not just for httpd), which are naturally very scarce.

Various ways of future improvements have already been mentioned in previous chapters. Considering the results of the models on cross-analysis, it would be necessary to focus especially on adjusting feature engineering -- more information that allows models to overfit on individual projects should be discarded. Additionally, refining the extraction of slicing criteria could help future graphs to contain less \textit{noise} (i.e., redundant information).

There is also a plan to deploy the developed models as part of the csmock tool~\cite{CsmockAVM,CsmockFLOCK}, which allows the automatic running of various analyzers on SRPM packages. A plugin for the csmock tool that adds support for static analysis by Meta Infer was created in the author's bachelor's thesis~\cite{bc}. It would simply involve supplementing this plugin with the developed models, which would rank the results of Infer.


\chapter{Conclusion}
\label{conclusion}
This thesis aimed to develop a machine learning-based system for ranking reports from the Meta Infer static analyzer based on their likelihood of being real error. Graph Neural Networks (GNNs) were selected due to their suitability for modeling various source code properties. The D2A dataset from IBM, which contains labeled Infer reports, was used for training. This dataset required conversion from a textual to a graphical format. To achieve this, a training pipeline was developed to produce Graph D2A -- a graphical representation of D2A. This pipeline improves existing graph generation techniques by considering conditional compilation. The raw format of graphs in Graph D2A necessitated the design of a feature engineering process that optimizes and transforms these graphs into Extended Code Property Graphs (ECPGs), which enrich commonly used Code Property Graphs by including Call Graphs, data types, and other information.

Experimental results with GNN models trained on projects httpd (AUROCC 0.83), libtiff (AUROCC 0.96), and nginx (AUROCC 0.94) show that the developed models are competitive with existing state-of-the-art solutions created by strong industrial teams. The models even reached state-of-the-art results on the nginx project although they performed less well on the httpd project, likely due to a low number of samples. Nonetheless, these experiments show that the developed models are a promising open-source alternative since all existing solutions are closed-source. The models were also tested using cross-analysis, which unfortunately did not yield useful results. Cross-analysis remains a significant challenge as none of the existing models compared in this thesis function effectively in this mode either.

In this thesis, an inference pipeline was also developed for the automatic Infer analysis, construction of ECPGs, and model inference on real-world C (and subset of C++) software. Even if cross-analysis does not work, the inference pipeline could be utilized in the future for inference on the projects on which the models were trained.

Future work should focus on evaluating and fine-tuning the developed models on larger projects from the D2A dataset. Based on the experiment results from the httpd project, the training pipeline should be improved to minimize the loss of samples during transformation. Specifically, increasing the timeout for the LLVM Slicer tool and focusing on generating LLVM bitcode, especially for the minority class -- real errors. There are also plans to deploy the developed models in the csmock tool, which automates analyses on SRPM packages.

Preliminary results of this thesis were presented at the Excel@FIT'24 conference, where it received an award from the expert panel.
\vspace{-1cm}